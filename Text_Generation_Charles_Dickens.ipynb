{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation - Charles Dickens\n",
    "\n",
    "In this notebook, I'll build a character-wise RNN trained on five books from Charles Dickens: 'A Tale of Two Cities,' 'Great Expectations,' 'Oliver Twist,' 'David Copperfield,' and 'A Christmas Carol.' These books were chosen because they were available on [www.gutenberg.org](http://www.gutenberg.org), they all come from the same, famous author, and together they add up to a significant sum of text, 4.7 MB. Although any one of these books could suffice for this simple project, by having more text it should help the model to avoid overfitting. \n",
    "\n",
    "Much of this code is from a lesson on recurrent neural networks, as part of my Deep Learning Nanodegree Foundation from Udacity. Here is the lesson on [Github](https://github.com/udacity/deep-learning/tree/master/intro-to-rnns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load the text file and convert it into integers for our network to use. Here I'm creating a couple dictionaries to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('Dickens_Books.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the first sentence of our text, which is from Great Expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Great Expectations\\n\\n[1867 Edition]\\n\\nby Charles Dickens\\n\\n\\nChapter I\\n\\nMy fatherâ€™s family name being Pirrip, and my Christian name Philip, my\\ninfant tongue could make of both names nothing longer or more explicit\\nthan Pip'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:218]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see the characters encoded as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([65,  5, 75, 12, 41,  7, 57,  9, 79, 75, 27, 41, 12, 41, 43, 56, 48,\n",
       "       51, 71, 71,  3, 31, 76, 13, 82,  7, 57, 74, 43, 41, 43, 56, 48, 73,\n",
       "       71, 71, 67,  4,  7, 66, 18, 12,  5, 37, 75, 51,  7, 85, 43, 27, 33,\n",
       "       75, 48, 51, 71, 71, 71, 66, 18, 12, 79, 41, 75,  5,  7, 17, 71, 71,\n",
       "       14,  4,  7, 63, 12, 41, 18, 75,  5, 46, 51,  7, 63, 12, 53, 43, 37,\n",
       "        4,  7, 48, 12, 53, 75,  7, 67, 75, 43, 48, 40,  7, 45, 43,  5,  5,\n",
       "       43, 79, 15,  7, 12, 48, 74,  7, 53,  4,  7, 66, 18,  5, 43, 51, 41,\n",
       "       43, 12, 48,  7, 48, 12, 53, 75,  7, 45, 18, 43, 37, 43, 79, 15,  7,\n",
       "       53,  4, 71, 43, 48, 63, 12, 48, 41,  7, 41, 56, 48, 40, 10, 75,  7,\n",
       "       27, 56, 10, 37, 74,  7, 53, 12, 33, 75,  7, 56, 63,  7, 67, 56, 41,\n",
       "       18,  7, 48, 12, 53, 75, 51,  7, 48, 56, 41, 18, 43, 48, 40,  7, 37,\n",
       "       56, 48, 40, 75,  5,  7, 56,  5,  7, 53, 56,  5, 75,  7, 75,  9, 79,\n",
       "       37, 43, 27, 43, 41, 71, 41, 18, 12, 48,  7, 45, 43, 79], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:218]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making training and validation batches\n",
    "\n",
    "Now I need to split up the data into batches, and into training and validation sets. I should be making a test set here, but I'm not going to worry about that. My test will be if the network can generate new text.\n",
    "\n",
    "Here I'll make both input and target arrays. The targets are the same as the inputs, except shifted one character over. I'll also drop the last bit of data so that I'll only have completely full batches.\n",
    "\n",
    "The idea here is to make a 2D matrix where the number of rows is equal to the batch size. Each row will be one long concatenated string from the character data. We'll split this data into a training set and validation set using the `split_frac` keyword. This will keep 90% of the batches in the training set, the other 10% in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    \n",
    "    slice_size = batch_size * num_steps\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the first split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll make my data sets and we can check out what's going on here. Here I'm going to use a batch size of 10 and 50 sequence steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 425550)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the size of this array, we see that we have rows equal to the batch size. When we want to get a batch out of here, we can grab a subset of this array that contains all the rows but has a width equal to the number of steps in the sequence. The first batch looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[65,  5, 75, 12, 41,  7, 57,  9, 79, 75, 27, 41, 12, 41, 43, 56, 48,\n",
       "        51, 71, 71,  3, 31, 76, 13, 82,  7, 57, 74, 43, 41, 43, 56, 48, 73,\n",
       "        71, 71, 67,  4,  7, 66, 18, 12,  5, 37, 75, 51,  7, 85, 43, 27],\n",
       "       [ 7, 53,  4, 51, 41, 75,  5,  4, 15,  7, 56, 63,  7, 68, 18, 43, 27,\n",
       "        18,  7, 17,  7, 68, 12, 51,  7, 41, 18, 75,  7, 18, 75,  5, 56, 35,\n",
       "         7, 57, 51, 41, 75, 37, 37, 12,  7, 68, 12, 51,  7, 41, 18, 75],\n",
       "       [75, 75, 48,  7, 74, 56, 68, 48,  7, 43, 48,  7, 65, 12,  5, 74, 75,\n",
       "        48,  7, 66, 56, 10,  5, 41,  7, 43, 48,  7, 41, 18, 75,  7, 74, 75,\n",
       "        12, 74,  7, 56, 63,  7, 41, 18, 75,  7, 48, 43, 40, 18, 41, 15],\n",
       "       [33, 51,  7, 56, 10, 41,  7, 56, 63,  7, 41, 18, 75, 43,  5,  7, 79,\n",
       "        37, 12, 27, 75, 51,  7, 43, 48,  7, 68, 12, 37, 37, 51, 35,  7, 57,\n",
       "        29, 75,  5,  4,  7, 79, 10, 37, 51, 75,  7, 12, 48, 74, 71, 18],\n",
       "       [37, 43, 29, 75,  5, 35, 71, 71, 69, 60, 10, 51, 18, 72, 69,  7,  5,\n",
       "        75, 79, 37, 43, 75, 74,  7, 41, 18, 75,  7, 85, 56, 74, 40, 75,  5,\n",
       "        35,  7, 69, 85, 56,  7,  4, 56, 10,  7, 51, 75, 75,  7, 41, 18],\n",
       "       [10,  5, 51, 75, 15, 71, 74, 75, 51, 27, 75, 48, 74, 75, 74,  7, 43,\n",
       "        48,  7, 51, 43, 37, 75, 48, 27, 75, 15,  7, 63, 56, 37, 37, 56, 68,\n",
       "        75, 74,  7, 67,  4,  7, 18, 43, 51,  7, 68, 43, 63, 75, 35,  7],\n",
       "       [53,  4,  7, 18, 75, 12, 74,  7, 43, 51,  7, 12, 51,  7, 18, 75, 12,\n",
       "        29,  4,  7, 12, 51,  7, 51, 56,  7, 53, 10, 27, 18,  7, 37, 75, 12,\n",
       "        74, 35,  7, 17,  7, 68, 56, 10, 37, 74,  7, 40, 43, 29, 75,  7],\n",
       "       [37,  7, 40, 75, 41,  7, 37, 56, 48, 40,  7, 48, 75,  9, 41, 15,  7,\n",
       "        53,  4,  7, 53, 75, 53, 56,  5,  4, 69, 51,  7, 40, 75, 41, 41, 43,\n",
       "        48, 40,  7, 51, 56,  7, 53, 10, 27, 18, 71, 51, 56, 15, 69,  7],\n",
       "       [69, 71, 71, 71, 71, 66, 60, 38, 45, 86, 57, 54,  7, 24, 13, 35,  7,\n",
       "        57, 47, 86, 60, 28, 77, 17, 38, 77, 14, 71, 71, 17,  7, 67, 75, 40,\n",
       "        12, 48,  7, 41, 18, 75,  7, 48, 75,  9, 41,  7, 74, 12,  4,  7],\n",
       "       [56, 75, 51,  7, 12,  7, 63, 43, 48, 75,  7, 67, 10, 51, 43, 48, 75,\n",
       "        51, 51, 35,  7, 57,  9, 81, 27, 75, 37, 37, 75, 48, 41,  7, 67, 10,\n",
       "        51, 43, 48, 75, 51, 51, 72, 69, 71, 71, 69, 17,  7, 12, 53,  7]], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:,:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll write another function to grab batches out of the arrays made by `split_data`. Here each batch will be a sliding window on these arrays with size `batch_size X num_steps`. For example, if we want our network to train on a sequence of 100 characters, `num_steps = 100`. For the next batch, we'll shift this window the next sequence of `num_steps` characters. In this way we can feed batches to the network and the cell states will continue through on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "Below is a function where I build the graph for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, hidden_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "    \n",
    "    # When we're using this network for sampling later, we'll be passing in\n",
    "    # one character at a time, so providing an option for that\n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # One-hot encoding the input and target characters\n",
    "    x_one_hot = tf.one_hot(inputs, num_classes)\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "\n",
    "    ### Build the RNN layers\n",
    "    # Use a basic GRU cell\n",
    "    gru = tf.contrib.rnn.GRUCell(hidden_size)\n",
    "    \n",
    "    # Add dropout to the cell\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(gru, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple GRU layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    ### Run the data through the RNN layers\n",
    "    # This makes a list where each element is on step in the sequence\n",
    "    rnn_inputs = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(x_one_hot, num_steps, 1)]\n",
    "    \n",
    "    # Run each sequence step through the RNN and collect the outputs\n",
    "    outputs, state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=initial_state)\n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one output row for each step for each batch\n",
    "    seq_output = tf.concat(outputs, axis=1)\n",
    "    output = tf.reshape(seq_output, [-1, hidden_size])\n",
    "    \n",
    "    # Now connect the RNN putputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((hidden_size, num_classes), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and batch\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    preds = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    # Reshape the targets to match the logits\n",
    "    y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    # Export the nodes\n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here I'm defining the hyperparameters for the network. \n",
    "\n",
    "* `batch_size` - Number of sequences running through the network in one pass.\n",
    "* `num_steps` - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "* `lstm_size` - The number of units in the hidden layers.\n",
    "* `num_layers` - Number of hidden LSTM layers to use\n",
    "* `learning_rate` - Learning rate for training\n",
    "* `keep_prob` - The dropout keep probability when training. If you're network is overfitting, try decreasing this.\n",
    "\n",
    "Here's some good advice from [Andrej Karpathy](https://github.com/karpathy/char-rnn#tips-and-tricks) on training the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "num_steps = 150\n",
    "hidden_size = 800\n",
    "num_layers = 2\n",
    "learning_rate = 0.0005\n",
    "keep_prob = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Time for training which is pretty straightforward. Here I pass in some data, and get an LSTM state back. Then I pass that state back in to the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I calculate the validation loss and save a checkpoint.\n",
    "\n",
    "Here I'm saving checkpoints with the format\n",
    "\n",
    "`i{iteration number}_l{# hidden layer units}_v{validation loss}.ckpt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15  Iteration 1/8505 Training loss: 4.4655 18.2071 sec/batch\n",
      "Epoch 1/15  Iteration 2/8505 Training loss: 4.3832 11.2126 sec/batch\n",
      "Epoch 1/15  Iteration 3/8505 Training loss: 4.2045 11.3433 sec/batch\n",
      "Epoch 1/15  Iteration 4/8505 Training loss: 5.1695 11.2834 sec/batch\n",
      "Epoch 1/15  Iteration 5/8505 Training loss: 5.3601 12.2730 sec/batch\n",
      "Epoch 1/15  Iteration 6/8505 Training loss: 5.0559 10.8563 sec/batch\n",
      "Epoch 1/15  Iteration 7/8505 Training loss: 4.8394 12.0140 sec/batch\n",
      "Epoch 1/15  Iteration 8/8505 Training loss: 4.6753 14.0895 sec/batch\n",
      "Epoch 1/15  Iteration 9/8505 Training loss: 4.5349 9.8205 sec/batch\n",
      "Epoch 1/15  Iteration 10/8505 Training loss: 4.4338 9.6365 sec/batch\n",
      "Epoch 1/15  Iteration 11/8505 Training loss: 4.3390 9.5656 sec/batch\n",
      "Epoch 1/15  Iteration 12/8505 Training loss: 4.2489 9.6711 sec/batch\n",
      "Epoch 1/15  Iteration 13/8505 Training loss: 4.1744 11.7492 sec/batch\n",
      "Epoch 1/15  Iteration 14/8505 Training loss: 4.1116 10.6166 sec/batch\n",
      "Epoch 1/15  Iteration 15/8505 Training loss: 4.0564 10.6753 sec/batch\n",
      "Epoch 1/15  Iteration 16/8505 Training loss: 4.0067 10.8185 sec/batch\n",
      "Epoch 1/15  Iteration 17/8505 Training loss: 3.9624 10.6907 sec/batch\n",
      "Epoch 1/15  Iteration 18/8505 Training loss: 3.9202 11.4658 sec/batch\n",
      "Epoch 1/15  Iteration 19/8505 Training loss: 3.8843 11.2478 sec/batch\n",
      "Epoch 1/15  Iteration 20/8505 Training loss: 3.8499 9.5773 sec/batch\n",
      "Epoch 1/15  Iteration 21/8505 Training loss: 3.8192 11.5208 sec/batch\n",
      "Epoch 1/15  Iteration 22/8505 Training loss: 3.7911 10.8293 sec/batch\n",
      "Epoch 1/15  Iteration 23/8505 Training loss: 3.7643 10.4697 sec/batch\n",
      "Epoch 1/15  Iteration 24/8505 Training loss: 3.7403 10.6692 sec/batch\n",
      "Epoch 1/15  Iteration 25/8505 Training loss: 3.7180 10.5001 sec/batch\n",
      "Epoch 1/15  Iteration 26/8505 Training loss: 3.6973 10.6186 sec/batch\n",
      "Epoch 1/15  Iteration 27/8505 Training loss: 3.6768 11.8375 sec/batch\n",
      "Epoch 1/15  Iteration 28/8505 Training loss: 3.6575 9.5651 sec/batch\n",
      "Epoch 1/15  Iteration 29/8505 Training loss: 3.6400 9.6635 sec/batch\n",
      "Epoch 1/15  Iteration 30/8505 Training loss: 3.6239 10.3712 sec/batch\n",
      "Epoch 1/15  Iteration 31/8505 Training loss: 3.6082 9.3999 sec/batch\n",
      "Epoch 1/15  Iteration 32/8505 Training loss: 3.5946 9.4757 sec/batch\n",
      "Epoch 1/15  Iteration 33/8505 Training loss: 3.5806 11.4075 sec/batch\n",
      "Epoch 1/15  Iteration 34/8505 Training loss: 3.5685 9.4382 sec/batch\n",
      "Epoch 1/15  Iteration 35/8505 Training loss: 3.5570 9.5533 sec/batch\n",
      "Epoch 1/15  Iteration 36/8505 Training loss: 3.5457 9.8981 sec/batch\n",
      "Epoch 1/15  Iteration 37/8505 Training loss: 3.5346 9.5014 sec/batch\n",
      "Epoch 1/15  Iteration 38/8505 Training loss: 3.5242 10.5994 sec/batch\n",
      "Epoch 1/15  Iteration 39/8505 Training loss: 3.5146 9.4670 sec/batch\n",
      "Epoch 1/15  Iteration 40/8505 Training loss: 3.5055 9.4002 sec/batch\n",
      "Epoch 1/15  Iteration 41/8505 Training loss: 3.4963 9.2443 sec/batch\n",
      "Epoch 1/15  Iteration 42/8505 Training loss: 3.4878 9.6661 sec/batch\n",
      "Epoch 1/15  Iteration 43/8505 Training loss: 3.4797 10.3023 sec/batch\n",
      "Epoch 1/15  Iteration 44/8505 Training loss: 3.4720 9.1972 sec/batch\n",
      "Epoch 1/15  Iteration 45/8505 Training loss: 3.4641 10.0374 sec/batch\n",
      "Epoch 1/15  Iteration 46/8505 Training loss: 3.4568 9.2739 sec/batch\n",
      "Epoch 1/15  Iteration 47/8505 Training loss: 3.4495 9.4859 sec/batch\n",
      "Epoch 1/15  Iteration 48/8505 Training loss: 3.4425 11.7831 sec/batch\n",
      "Epoch 1/15  Iteration 49/8505 Training loss: 3.4358 9.4851 sec/batch\n",
      "Epoch 1/15  Iteration 50/8505 Training loss: 3.4286 9.4223 sec/batch\n",
      "Epoch 1/15  Iteration 51/8505 Training loss: 3.4223 10.3466 sec/batch\n",
      "Epoch 1/15  Iteration 52/8505 Training loss: 3.4163 9.5530 sec/batch\n",
      "Epoch 1/15  Iteration 53/8505 Training loss: 3.4109 10.4518 sec/batch\n",
      "Epoch 1/15  Iteration 54/8505 Training loss: 3.4049 10.9181 sec/batch\n",
      "Epoch 1/15  Iteration 55/8505 Training loss: 3.3993 9.5230 sec/batch\n",
      "Epoch 1/15  Iteration 56/8505 Training loss: 3.3944 10.5388 sec/batch\n",
      "Epoch 1/15  Iteration 57/8505 Training loss: 3.3888 9.4198 sec/batch\n",
      "Epoch 1/15  Iteration 58/8505 Training loss: 3.3835 11.8017 sec/batch\n",
      "Epoch 1/15  Iteration 59/8505 Training loss: 3.3789 13.3935 sec/batch\n",
      "Epoch 1/15  Iteration 60/8505 Training loss: 3.3742 15.6136 sec/batch\n",
      "Epoch 1/15  Iteration 61/8505 Training loss: 3.3694 12.6557 sec/batch\n",
      "Epoch 1/15  Iteration 62/8505 Training loss: 3.3646 16.0091 sec/batch\n",
      "Epoch 1/15  Iteration 63/8505 Training loss: 3.3598 10.3727 sec/batch\n",
      "Epoch 1/15  Iteration 64/8505 Training loss: 3.3556 10.1442 sec/batch\n",
      "Epoch 1/15  Iteration 65/8505 Training loss: 3.3511 10.2276 sec/batch\n",
      "Epoch 1/15  Iteration 66/8505 Training loss: 3.3473 10.8109 sec/batch\n",
      "Epoch 1/15  Iteration 67/8505 Training loss: 3.3428 15.1303 sec/batch\n",
      "Epoch 1/15  Iteration 68/8505 Training loss: 3.3386 11.2550 sec/batch\n",
      "Epoch 1/15  Iteration 69/8505 Training loss: 3.3345 13.4609 sec/batch\n",
      "Epoch 1/15  Iteration 70/8505 Training loss: 3.3307 15.8454 sec/batch\n",
      "Epoch 1/15  Iteration 71/8505 Training loss: 3.3267 11.3072 sec/batch\n",
      "Epoch 1/15  Iteration 72/8505 Training loss: 3.3224 11.3415 sec/batch\n",
      "Epoch 1/15  Iteration 73/8505 Training loss: 3.3184 11.1148 sec/batch\n",
      "Epoch 1/15  Iteration 74/8505 Training loss: 3.3144 13.0320 sec/batch\n",
      "Epoch 1/15  Iteration 75/8505 Training loss: 3.3100 14.7383 sec/batch\n",
      "Epoch 1/15  Iteration 76/8505 Training loss: 3.3056 11.5918 sec/batch\n",
      "Epoch 1/15  Iteration 77/8505 Training loss: 3.3011 15.2090 sec/batch\n",
      "Epoch 1/15  Iteration 78/8505 Training loss: 3.2965 10.9957 sec/batch\n",
      "Epoch 1/15  Iteration 79/8505 Training loss: 3.2926 12.7690 sec/batch\n",
      "Epoch 1/15  Iteration 80/8505 Training loss: 3.2913 11.6969 sec/batch\n",
      "Epoch 1/15  Iteration 81/8505 Training loss: 3.2876 12.1506 sec/batch\n",
      "Epoch 1/15  Iteration 82/8505 Training loss: 3.2837 11.4665 sec/batch\n",
      "Epoch 1/15  Iteration 83/8505 Training loss: 3.2800 11.1596 sec/batch\n",
      "Epoch 1/15  Iteration 84/8505 Training loss: 3.2760 14.7237 sec/batch\n",
      "Epoch 1/15  Iteration 85/8505 Training loss: 3.2718 13.4767 sec/batch\n",
      "Epoch 1/15  Iteration 86/8505 Training loss: 3.2677 11.6953 sec/batch\n",
      "Epoch 1/15  Iteration 87/8505 Training loss: 3.2641 10.8397 sec/batch\n",
      "Epoch 1/15  Iteration 88/8505 Training loss: 3.2604 12.0733 sec/batch\n",
      "Epoch 1/15  Iteration 89/8505 Training loss: 3.2565 15.3939 sec/batch\n",
      "Epoch 1/15  Iteration 90/8505 Training loss: 3.2524 14.3707 sec/batch\n",
      "Epoch 1/15  Iteration 91/8505 Training loss: 3.2482 12.6285 sec/batch\n",
      "Epoch 1/15  Iteration 92/8505 Training loss: 3.2436 11.0047 sec/batch\n",
      "Epoch 1/15  Iteration 93/8505 Training loss: 3.2398 10.7402 sec/batch\n",
      "Epoch 1/15  Iteration 94/8505 Training loss: 3.2349 10.6322 sec/batch\n",
      "Epoch 1/15  Iteration 95/8505 Training loss: 3.2302 10.6862 sec/batch\n",
      "Epoch 1/15  Iteration 96/8505 Training loss: 3.2255 10.9287 sec/batch\n",
      "Epoch 1/15  Iteration 97/8505 Training loss: 3.2205 10.7005 sec/batch\n",
      "Epoch 1/15  Iteration 98/8505 Training loss: 3.2157 10.6945 sec/batch\n",
      "Epoch 1/15  Iteration 99/8505 Training loss: 3.2109 10.9208 sec/batch\n",
      "Epoch 1/15  Iteration 100/8505 Training loss: 3.2057 13.1361 sec/batch\n",
      "Epoch 1/15  Iteration 101/8505 Training loss: 3.2004 12.5555 sec/batch\n",
      "Epoch 1/15  Iteration 102/8505 Training loss: 3.1956 10.8619 sec/batch\n",
      "Epoch 1/15  Iteration 103/8505 Training loss: 3.1911 10.7512 sec/batch\n",
      "Epoch 1/15  Iteration 104/8505 Training loss: 3.1858 10.9006 sec/batch\n",
      "Epoch 1/15  Iteration 105/8505 Training loss: 3.1810 10.7018 sec/batch\n",
      "Epoch 1/15  Iteration 106/8505 Training loss: 3.1757 10.9201 sec/batch\n",
      "Epoch 1/15  Iteration 107/8505 Training loss: 3.1704 10.9999 sec/batch\n",
      "Epoch 1/15  Iteration 108/8505 Training loss: 3.1652 14.2571 sec/batch\n",
      "Epoch 1/15  Iteration 109/8505 Training loss: 3.1606 10.9474 sec/batch\n",
      "Epoch 1/15  Iteration 110/8505 Training loss: 3.1558 11.7039 sec/batch\n",
      "Epoch 1/15  Iteration 111/8505 Training loss: 3.1508 12.7196 sec/batch\n",
      "Epoch 1/15  Iteration 112/8505 Training loss: 3.1457 11.7361 sec/batch\n",
      "Epoch 1/15  Iteration 113/8505 Training loss: 3.1404 9.8207 sec/batch\n",
      "Epoch 1/15  Iteration 114/8505 Training loss: 3.1350 9.9829 sec/batch\n",
      "Epoch 1/15  Iteration 115/8505 Training loss: 3.1299 9.5058 sec/batch\n",
      "Epoch 1/15  Iteration 116/8505 Training loss: 3.1247 9.3537 sec/batch\n",
      "Epoch 1/15  Iteration 117/8505 Training loss: 3.1194 10.3744 sec/batch\n",
      "Epoch 1/15  Iteration 118/8505 Training loss: 3.1145 9.5429 sec/batch\n",
      "Epoch 1/15  Iteration 119/8505 Training loss: 3.1095 10.1964 sec/batch\n",
      "Epoch 1/15  Iteration 120/8505 Training loss: 3.1048 9.6320 sec/batch\n",
      "Epoch 1/15  Iteration 121/8505 Training loss: 3.0999 9.3241 sec/batch\n",
      "Epoch 1/15  Iteration 122/8505 Training loss: 3.0950 9.1567 sec/batch\n",
      "Epoch 1/15  Iteration 123/8505 Training loss: 3.0900 10.4008 sec/batch\n",
      "Epoch 1/15  Iteration 124/8505 Training loss: 3.0852 9.6470 sec/batch\n",
      "Epoch 1/15  Iteration 125/8505 Training loss: 3.0802 9.3101 sec/batch\n",
      "Epoch 1/15  Iteration 126/8505 Training loss: 3.0754 9.4436 sec/batch\n",
      "Epoch 1/15  Iteration 127/8505 Training loss: 3.0708 10.4315 sec/batch\n",
      "Epoch 1/15  Iteration 128/8505 Training loss: 3.0661 9.4692 sec/batch\n",
      "Epoch 1/15  Iteration 129/8505 Training loss: 3.0617 9.7114 sec/batch\n",
      "Epoch 1/15  Iteration 130/8505 Training loss: 3.0570 10.4445 sec/batch\n",
      "Epoch 1/15  Iteration 131/8505 Training loss: 3.0523 9.6039 sec/batch\n",
      "Epoch 1/15  Iteration 132/8505 Training loss: 3.0480 9.3772 sec/batch\n",
      "Epoch 1/15  Iteration 133/8505 Training loss: 3.0438 10.5778 sec/batch\n",
      "Epoch 1/15  Iteration 134/8505 Training loss: 3.0398 9.6105 sec/batch\n",
      "Epoch 1/15  Iteration 135/8505 Training loss: 3.0354 9.3346 sec/batch\n",
      "Epoch 1/15  Iteration 136/8505 Training loss: 3.0319 9.5250 sec/batch\n",
      "Epoch 1/15  Iteration 137/8505 Training loss: 3.0277 10.4175 sec/batch\n",
      "Epoch 1/15  Iteration 138/8505 Training loss: 3.0238 9.6578 sec/batch\n",
      "Epoch 1/15  Iteration 139/8505 Training loss: 3.0195 9.3443 sec/batch\n",
      "Epoch 1/15  Iteration 140/8505 Training loss: 3.0152 9.3067 sec/batch\n",
      "Epoch 1/15  Iteration 141/8505 Training loss: 3.0111 9.3551 sec/batch\n",
      "Epoch 1/15  Iteration 142/8505 Training loss: 3.0068 10.6852 sec/batch\n",
      "Epoch 1/15  Iteration 143/8505 Training loss: 3.0026 9.3601 sec/batch\n",
      "Epoch 1/15  Iteration 144/8505 Training loss: 2.9984 9.2912 sec/batch\n",
      "Epoch 1/15  Iteration 145/8505 Training loss: 2.9941 9.4239 sec/batch\n",
      "Epoch 1/15  Iteration 146/8505 Training loss: 2.9905 10.3349 sec/batch\n",
      "Epoch 1/15  Iteration 147/8505 Training loss: 2.9866 9.4126 sec/batch\n",
      "Epoch 1/15  Iteration 148/8505 Training loss: 2.9826 9.3117 sec/batch\n",
      "Epoch 1/15  Iteration 149/8505 Training loss: 2.9785 9.1948 sec/batch\n",
      "Epoch 1/15  Iteration 150/8505 Training loss: 2.9745 9.5765 sec/batch\n",
      "Epoch 1/15  Iteration 151/8505 Training loss: 2.9706 9.2850 sec/batch\n",
      "Epoch 1/15  Iteration 152/8505 Training loss: 2.9665 9.2472 sec/batch\n",
      "Epoch 1/15  Iteration 153/8505 Training loss: 2.9625 9.2909 sec/batch\n",
      "Epoch 1/15  Iteration 154/8505 Training loss: 2.9588 9.3339 sec/batch\n",
      "Epoch 1/15  Iteration 155/8505 Training loss: 2.9549 9.3216 sec/batch\n",
      "Epoch 1/15  Iteration 156/8505 Training loss: 2.9510 9.4055 sec/batch\n",
      "Epoch 1/15  Iteration 157/8505 Training loss: 2.9474 9.2883 sec/batch\n",
      "Epoch 1/15  Iteration 158/8505 Training loss: 2.9436 9.2936 sec/batch\n",
      "Epoch 1/15  Iteration 159/8505 Training loss: 2.9402 9.2759 sec/batch\n",
      "Epoch 1/15  Iteration 160/8505 Training loss: 2.9365 9.3268 sec/batch\n",
      "Epoch 1/15  Iteration 161/8505 Training loss: 2.9329 9.2347 sec/batch\n",
      "Epoch 1/15  Iteration 162/8505 Training loss: 2.9292 9.3199 sec/batch\n",
      "Epoch 1/15  Iteration 163/8505 Training loss: 2.9260 9.3577 sec/batch\n",
      "Epoch 1/15  Iteration 164/8505 Training loss: 2.9227 9.4570 sec/batch\n",
      "Epoch 1/15  Iteration 165/8505 Training loss: 2.9191 9.3426 sec/batch\n",
      "Epoch 1/15  Iteration 166/8505 Training loss: 2.9157 9.3004 sec/batch\n",
      "Epoch 1/15  Iteration 167/8505 Training loss: 2.9123 9.3645 sec/batch\n",
      "Epoch 1/15  Iteration 168/8505 Training loss: 2.9086 9.3794 sec/batch\n",
      "Epoch 1/15  Iteration 169/8505 Training loss: 2.9053 9.3646 sec/batch\n",
      "Epoch 1/15  Iteration 170/8505 Training loss: 2.9019 9.3036 sec/batch\n",
      "Epoch 1/15  Iteration 171/8505 Training loss: 2.8986 9.2399 sec/batch\n",
      "Epoch 1/15  Iteration 172/8505 Training loss: 2.8951 9.2946 sec/batch\n",
      "Epoch 1/15  Iteration 173/8505 Training loss: 2.8916 9.3025 sec/batch\n",
      "Epoch 1/15  Iteration 174/8505 Training loss: 2.8882 9.4593 sec/batch\n",
      "Epoch 1/15  Iteration 175/8505 Training loss: 2.8855 9.4031 sec/batch\n",
      "Epoch 1/15  Iteration 176/8505 Training loss: 2.8822 9.3209 sec/batch\n",
      "Epoch 1/15  Iteration 177/8505 Training loss: 2.8789 9.3551 sec/batch\n",
      "Epoch 1/15  Iteration 178/8505 Training loss: 2.8756 9.3003 sec/batch\n",
      "Epoch 1/15  Iteration 179/8505 Training loss: 2.8723 9.5058 sec/batch\n",
      "Epoch 1/15  Iteration 180/8505 Training loss: 2.8691 9.4441 sec/batch\n",
      "Epoch 1/15  Iteration 181/8505 Training loss: 2.8659 9.3941 sec/batch\n",
      "Epoch 1/15  Iteration 182/8505 Training loss: 2.8628 9.5624 sec/batch\n",
      "Epoch 1/15  Iteration 183/8505 Training loss: 2.8595 9.3992 sec/batch\n",
      "Epoch 1/15  Iteration 184/8505 Training loss: 2.8566 9.3267 sec/batch\n",
      "Epoch 1/15  Iteration 185/8505 Training loss: 2.8537 9.4482 sec/batch\n",
      "Epoch 1/15  Iteration 186/8505 Training loss: 2.8506 9.4241 sec/batch\n",
      "Epoch 1/15  Iteration 187/8505 Training loss: 2.8477 9.3538 sec/batch\n",
      "Epoch 1/15  Iteration 188/8505 Training loss: 2.8446 9.2735 sec/batch\n",
      "Epoch 1/15  Iteration 189/8505 Training loss: 2.8418 9.3539 sec/batch\n",
      "Epoch 1/15  Iteration 190/8505 Training loss: 2.8388 9.2582 sec/batch\n",
      "Epoch 1/15  Iteration 191/8505 Training loss: 2.8358 9.2779 sec/batch\n",
      "Epoch 1/15  Iteration 192/8505 Training loss: 2.8332 9.3616 sec/batch\n",
      "Epoch 1/15  Iteration 193/8505 Training loss: 2.8307 9.4108 sec/batch\n",
      "Epoch 1/15  Iteration 194/8505 Training loss: 2.8278 9.2725 sec/batch\n",
      "Epoch 1/15  Iteration 195/8505 Training loss: 2.8250 9.3580 sec/batch\n",
      "Epoch 1/15  Iteration 196/8505 Training loss: 2.8220 9.2337 sec/batch\n",
      "Epoch 1/15  Iteration 197/8505 Training loss: 2.8190 9.2545 sec/batch\n",
      "Epoch 1/15  Iteration 198/8505 Training loss: 2.8163 9.3955 sec/batch\n",
      "Epoch 1/15  Iteration 199/8505 Training loss: 2.8135 9.2990 sec/batch\n",
      "Epoch 1/15  Iteration 200/8505 Training loss: 2.8107 9.3694 sec/batch\n",
      "Validation loss: 2.21957 Saving checkpoint!\n",
      "Epoch 1/15  Iteration 201/8505 Training loss: 2.8082 11.8325 sec/batch\n",
      "Epoch 1/15  Iteration 202/8505 Training loss: 2.8055 9.4531 sec/batch\n",
      "Epoch 1/15  Iteration 203/8505 Training loss: 2.8029 9.4253 sec/batch\n",
      "Epoch 1/15  Iteration 204/8505 Training loss: 2.8001 9.3301 sec/batch\n",
      "Epoch 1/15  Iteration 205/8505 Training loss: 2.7974 9.3823 sec/batch\n",
      "Epoch 1/15  Iteration 206/8505 Training loss: 2.7946 9.3645 sec/batch\n",
      "Epoch 1/15  Iteration 207/8505 Training loss: 2.7921 9.4592 sec/batch\n",
      "Epoch 1/15  Iteration 208/8505 Training loss: 2.7894 9.4699 sec/batch\n",
      "Epoch 1/15  Iteration 209/8505 Training loss: 2.7867 9.5335 sec/batch\n",
      "Epoch 1/15  Iteration 210/8505 Training loss: 2.7840 9.5353 sec/batch\n",
      "Epoch 1/15  Iteration 211/8505 Training loss: 2.7814 9.5216 sec/batch\n",
      "Epoch 1/15  Iteration 212/8505 Training loss: 2.7788 9.2775 sec/batch\n",
      "Epoch 1/15  Iteration 213/8505 Training loss: 2.7763 9.2804 sec/batch\n",
      "Epoch 1/15  Iteration 214/8505 Training loss: 2.7736 9.2861 sec/batch\n",
      "Epoch 1/15  Iteration 215/8505 Training loss: 2.7710 9.4184 sec/batch\n",
      "Epoch 1/15  Iteration 216/8505 Training loss: 2.7685 9.3611 sec/batch\n",
      "Epoch 1/15  Iteration 217/8505 Training loss: 2.7661 9.2858 sec/batch\n",
      "Epoch 1/15  Iteration 218/8505 Training loss: 2.7637 9.2535 sec/batch\n",
      "Epoch 1/15  Iteration 219/8505 Training loss: 2.7614 9.3688 sec/batch\n",
      "Epoch 1/15  Iteration 220/8505 Training loss: 2.7588 9.4170 sec/batch\n",
      "Epoch 1/15  Iteration 221/8505 Training loss: 2.7564 9.2918 sec/batch\n",
      "Epoch 1/15  Iteration 222/8505 Training loss: 2.7539 9.2850 sec/batch\n",
      "Epoch 1/15  Iteration 223/8505 Training loss: 2.7515 9.2319 sec/batch\n",
      "Epoch 1/15  Iteration 224/8505 Training loss: 2.7489 9.2310 sec/batch\n",
      "Epoch 1/15  Iteration 225/8505 Training loss: 2.7463 9.2347 sec/batch\n",
      "Epoch 1/15  Iteration 226/8505 Training loss: 2.7439 9.3403 sec/batch\n",
      "Epoch 1/15  Iteration 227/8505 Training loss: 2.7415 9.2389 sec/batch\n",
      "Epoch 1/15  Iteration 228/8505 Training loss: 2.7390 9.2973 sec/batch\n",
      "Epoch 1/15  Iteration 229/8505 Training loss: 2.7367 9.2602 sec/batch\n",
      "Epoch 1/15  Iteration 230/8505 Training loss: 2.7344 9.3857 sec/batch\n",
      "Epoch 1/15  Iteration 231/8505 Training loss: 2.7320 9.2786 sec/batch\n",
      "Epoch 1/15  Iteration 232/8505 Training loss: 2.7297 9.2511 sec/batch\n",
      "Epoch 1/15  Iteration 233/8505 Training loss: 2.7274 9.2505 sec/batch\n",
      "Epoch 1/15  Iteration 234/8505 Training loss: 2.7251 9.2666 sec/batch\n",
      "Epoch 1/15  Iteration 235/8505 Training loss: 2.7229 9.2934 sec/batch\n",
      "Epoch 1/15  Iteration 236/8505 Training loss: 2.7208 9.3426 sec/batch\n",
      "Epoch 1/15  Iteration 237/8505 Training loss: 2.7185 9.3542 sec/batch\n",
      "Epoch 1/15  Iteration 238/8505 Training loss: 2.7162 9.3017 sec/batch\n",
      "Epoch 1/15  Iteration 239/8505 Training loss: 2.7139 9.2481 sec/batch\n",
      "Epoch 1/15  Iteration 240/8505 Training loss: 2.7117 9.2791 sec/batch\n",
      "Epoch 1/15  Iteration 241/8505 Training loss: 2.7094 9.2590 sec/batch\n",
      "Epoch 1/15  Iteration 242/8505 Training loss: 2.7073 9.3155 sec/batch\n",
      "Epoch 1/15  Iteration 243/8505 Training loss: 2.7051 9.2802 sec/batch\n",
      "Epoch 1/15  Iteration 244/8505 Training loss: 2.7029 9.1953 sec/batch\n",
      "Epoch 1/15  Iteration 245/8505 Training loss: 2.7008 9.3304 sec/batch\n",
      "Epoch 1/15  Iteration 246/8505 Training loss: 2.6985 9.4214 sec/batch\n",
      "Epoch 1/15  Iteration 247/8505 Training loss: 2.6962 9.3273 sec/batch\n",
      "Epoch 1/15  Iteration 248/8505 Training loss: 2.6942 9.3917 sec/batch\n",
      "Epoch 1/15  Iteration 249/8505 Training loss: 2.6920 9.2600 sec/batch\n",
      "Epoch 1/15  Iteration 250/8505 Training loss: 2.6897 9.2617 sec/batch\n",
      "Epoch 1/15  Iteration 251/8505 Training loss: 2.6875 9.2311 sec/batch\n",
      "Epoch 1/15  Iteration 252/8505 Training loss: 2.6853 9.2598 sec/batch\n",
      "Epoch 1/15  Iteration 253/8505 Training loss: 2.6831 9.2959 sec/batch\n",
      "Epoch 1/15  Iteration 254/8505 Training loss: 2.6810 9.3222 sec/batch\n",
      "Epoch 1/15  Iteration 255/8505 Training loss: 2.6790 9.1783 sec/batch\n",
      "Epoch 1/15  Iteration 256/8505 Training loss: 2.6769 9.3383 sec/batch\n",
      "Epoch 1/15  Iteration 257/8505 Training loss: 2.6748 9.2501 sec/batch\n",
      "Epoch 1/15  Iteration 258/8505 Training loss: 2.6729 9.1907 sec/batch\n",
      "Epoch 1/15  Iteration 259/8505 Training loss: 2.6708 9.3774 sec/batch\n",
      "Epoch 1/15  Iteration 260/8505 Training loss: 2.6687 9.2680 sec/batch\n",
      "Epoch 1/15  Iteration 261/8505 Training loss: 2.6666 9.3373 sec/batch\n",
      "Epoch 1/15  Iteration 262/8505 Training loss: 2.6644 9.4481 sec/batch\n",
      "Epoch 1/15  Iteration 263/8505 Training loss: 2.6625 9.2505 sec/batch\n",
      "Epoch 1/15  Iteration 264/8505 Training loss: 2.6604 9.3107 sec/batch\n",
      "Epoch 1/15  Iteration 265/8505 Training loss: 2.6584 9.2250 sec/batch\n",
      "Epoch 1/15  Iteration 266/8505 Training loss: 2.6563 9.2850 sec/batch\n",
      "Epoch 1/15  Iteration 267/8505 Training loss: 2.6544 9.2939 sec/batch\n",
      "Epoch 1/15  Iteration 268/8505 Training loss: 2.6525 9.3139 sec/batch\n",
      "Epoch 1/15  Iteration 269/8505 Training loss: 2.6506 9.2502 sec/batch\n",
      "Epoch 1/15  Iteration 270/8505 Training loss: 2.6487 9.2952 sec/batch\n",
      "Epoch 1/15  Iteration 271/8505 Training loss: 2.6467 9.2584 sec/batch\n",
      "Epoch 1/15  Iteration 272/8505 Training loss: 2.6448 9.2776 sec/batch\n",
      "Epoch 1/15  Iteration 273/8505 Training loss: 2.6429 9.6821 sec/batch\n",
      "Epoch 1/15  Iteration 274/8505 Training loss: 2.6410 9.3236 sec/batch\n",
      "Epoch 1/15  Iteration 275/8505 Training loss: 2.6391 9.2594 sec/batch\n",
      "Epoch 1/15  Iteration 276/8505 Training loss: 2.6372 9.2955 sec/batch\n",
      "Epoch 1/15  Iteration 277/8505 Training loss: 2.6354 9.4301 sec/batch\n",
      "Epoch 1/15  Iteration 278/8505 Training loss: 2.6335 9.3464 sec/batch\n",
      "Epoch 1/15  Iteration 279/8505 Training loss: 2.6317 9.2691 sec/batch\n",
      "Epoch 1/15  Iteration 280/8505 Training loss: 2.6297 9.2371 sec/batch\n",
      "Epoch 1/15  Iteration 281/8505 Training loss: 2.6277 9.2901 sec/batch\n",
      "Epoch 1/15  Iteration 282/8505 Training loss: 2.6258 9.2719 sec/batch\n",
      "Epoch 1/15  Iteration 283/8505 Training loss: 2.6239 9.3523 sec/batch\n",
      "Epoch 1/15  Iteration 284/8505 Training loss: 2.6220 9.2303 sec/batch\n",
      "Epoch 1/15  Iteration 285/8505 Training loss: 2.6201 9.3875 sec/batch\n",
      "Epoch 1/15  Iteration 286/8505 Training loss: 2.6183 9.3262 sec/batch\n",
      "Epoch 1/15  Iteration 287/8505 Training loss: 2.6165 9.3196 sec/batch\n",
      "Epoch 1/15  Iteration 288/8505 Training loss: 2.6148 9.2051 sec/batch\n",
      "Epoch 1/15  Iteration 289/8505 Training loss: 2.6130 9.3269 sec/batch\n",
      "Epoch 1/15  Iteration 290/8505 Training loss: 2.6111 9.2665 sec/batch\n",
      "Epoch 1/15  Iteration 291/8505 Training loss: 2.6092 9.2333 sec/batch\n",
      "Epoch 1/15  Iteration 292/8505 Training loss: 2.6074 9.3078 sec/batch\n",
      "Epoch 1/15  Iteration 293/8505 Training loss: 2.6056 9.4597 sec/batch\n",
      "Epoch 1/15  Iteration 294/8505 Training loss: 2.6039 9.3613 sec/batch\n",
      "Epoch 1/15  Iteration 295/8505 Training loss: 2.6022 9.2578 sec/batch\n",
      "Epoch 1/15  Iteration 296/8505 Training loss: 2.6004 9.2855 sec/batch\n",
      "Epoch 1/15  Iteration 297/8505 Training loss: 2.5986 9.2298 sec/batch\n",
      "Epoch 1/15  Iteration 298/8505 Training loss: 2.5968 9.3583 sec/batch\n",
      "Epoch 1/15  Iteration 299/8505 Training loss: 2.5951 9.3246 sec/batch\n",
      "Epoch 1/15  Iteration 300/8505 Training loss: 2.5933 9.2311 sec/batch\n",
      "Epoch 1/15  Iteration 301/8505 Training loss: 2.5916 9.2529 sec/batch\n",
      "Epoch 1/15  Iteration 302/8505 Training loss: 2.5898 9.3396 sec/batch\n",
      "Epoch 1/15  Iteration 303/8505 Training loss: 2.5881 9.3451 sec/batch\n",
      "Epoch 1/15  Iteration 304/8505 Training loss: 2.5864 9.2723 sec/batch\n",
      "Epoch 1/15  Iteration 305/8505 Training loss: 2.5846 9.5314 sec/batch\n",
      "Epoch 1/15  Iteration 306/8505 Training loss: 2.5829 9.3110 sec/batch\n",
      "Epoch 1/15  Iteration 307/8505 Training loss: 2.5811 9.4854 sec/batch\n",
      "Epoch 1/15  Iteration 308/8505 Training loss: 2.5793 9.2458 sec/batch\n",
      "Epoch 1/15  Iteration 309/8505 Training loss: 2.5777 9.2512 sec/batch\n",
      "Epoch 1/15  Iteration 310/8505 Training loss: 2.5760 9.4161 sec/batch\n",
      "Epoch 1/15  Iteration 311/8505 Training loss: 2.5742 9.3302 sec/batch\n",
      "Epoch 1/15  Iteration 312/8505 Training loss: 2.5725 9.3219 sec/batch\n",
      "Epoch 1/15  Iteration 313/8505 Training loss: 2.5708 9.2064 sec/batch\n",
      "Epoch 1/15  Iteration 314/8505 Training loss: 2.5691 9.3137 sec/batch\n",
      "Epoch 1/15  Iteration 315/8505 Training loss: 2.5675 9.2405 sec/batch\n",
      "Epoch 1/15  Iteration 316/8505 Training loss: 2.5658 9.6932 sec/batch\n",
      "Epoch 1/15  Iteration 317/8505 Training loss: 2.5641 9.4955 sec/batch\n",
      "Epoch 1/15  Iteration 318/8505 Training loss: 2.5625 9.3111 sec/batch\n",
      "Epoch 1/15  Iteration 319/8505 Training loss: 2.5608 9.4010 sec/batch\n",
      "Epoch 1/15  Iteration 320/8505 Training loss: 2.5592 9.2661 sec/batch\n",
      "Epoch 1/15  Iteration 321/8505 Training loss: 2.5576 9.3461 sec/batch\n",
      "Epoch 1/15  Iteration 322/8505 Training loss: 2.5560 9.3183 sec/batch\n",
      "Epoch 1/15  Iteration 323/8505 Training loss: 2.5544 9.2967 sec/batch\n",
      "Epoch 1/15  Iteration 324/8505 Training loss: 2.5529 9.2983 sec/batch\n",
      "Epoch 1/15  Iteration 325/8505 Training loss: 2.5513 9.2674 sec/batch\n",
      "Epoch 1/15  Iteration 326/8505 Training loss: 2.5497 9.2700 sec/batch\n",
      "Epoch 1/15  Iteration 327/8505 Training loss: 2.5481 9.2237 sec/batch\n",
      "Epoch 1/15  Iteration 328/8505 Training loss: 2.5465 9.3332 sec/batch\n",
      "Epoch 1/15  Iteration 329/8505 Training loss: 2.5449 9.2451 sec/batch\n",
      "Epoch 1/15  Iteration 330/8505 Training loss: 2.5433 9.3002 sec/batch\n",
      "Epoch 1/15  Iteration 331/8505 Training loss: 2.5416 9.3790 sec/batch\n",
      "Epoch 1/15  Iteration 332/8505 Training loss: 2.5402 9.2414 sec/batch\n",
      "Epoch 1/15  Iteration 333/8505 Training loss: 2.5386 9.3521 sec/batch\n",
      "Epoch 1/15  Iteration 334/8505 Training loss: 2.5371 9.2842 sec/batch\n",
      "Epoch 1/15  Iteration 335/8505 Training loss: 2.5355 9.1922 sec/batch\n",
      "Epoch 1/15  Iteration 336/8505 Training loss: 2.5340 9.3752 sec/batch\n",
      "Epoch 1/15  Iteration 337/8505 Training loss: 2.5326 9.5517 sec/batch\n",
      "Epoch 1/15  Iteration 338/8505 Training loss: 2.5312 9.3167 sec/batch\n",
      "Epoch 1/15  Iteration 339/8505 Training loss: 2.5297 9.2429 sec/batch\n",
      "Epoch 1/15  Iteration 340/8505 Training loss: 2.5282 9.2502 sec/batch\n",
      "Epoch 1/15  Iteration 341/8505 Training loss: 2.5266 9.3980 sec/batch\n",
      "Epoch 1/15  Iteration 342/8505 Training loss: 2.5251 9.3666 sec/batch\n",
      "Epoch 1/15  Iteration 343/8505 Training loss: 2.5236 9.3194 sec/batch\n",
      "Epoch 1/15  Iteration 344/8505 Training loss: 2.5221 9.2888 sec/batch\n",
      "Epoch 1/15  Iteration 345/8505 Training loss: 2.5205 9.2894 sec/batch\n",
      "Epoch 1/15  Iteration 346/8505 Training loss: 2.5191 9.2459 sec/batch\n",
      "Epoch 1/15  Iteration 347/8505 Training loss: 2.5176 9.2658 sec/batch\n",
      "Epoch 1/15  Iteration 348/8505 Training loss: 2.5160 9.2828 sec/batch\n",
      "Epoch 1/15  Iteration 349/8505 Training loss: 2.5145 9.5015 sec/batch\n",
      "Epoch 1/15  Iteration 350/8505 Training loss: 2.5131 9.2330 sec/batch\n",
      "Epoch 1/15  Iteration 351/8505 Training loss: 2.5117 9.3188 sec/batch\n",
      "Epoch 1/15  Iteration 352/8505 Training loss: 2.5102 9.2809 sec/batch\n",
      "Epoch 1/15  Iteration 353/8505 Training loss: 2.5086 9.3086 sec/batch\n",
      "Epoch 1/15  Iteration 354/8505 Training loss: 2.5071 9.2557 sec/batch\n",
      "Epoch 1/15  Iteration 355/8505 Training loss: 2.5057 9.2466 sec/batch\n",
      "Epoch 1/15  Iteration 356/8505 Training loss: 2.5042 9.2860 sec/batch\n",
      "Epoch 1/15  Iteration 357/8505 Training loss: 2.5028 9.2753 sec/batch\n",
      "Epoch 1/15  Iteration 358/8505 Training loss: 2.5012 9.2896 sec/batch\n",
      "Epoch 1/15  Iteration 359/8505 Training loss: 2.4997 9.3321 sec/batch\n",
      "Epoch 1/15  Iteration 360/8505 Training loss: 2.4982 9.3832 sec/batch\n",
      "Epoch 1/15  Iteration 361/8505 Training loss: 2.4967 9.2834 sec/batch\n",
      "Epoch 1/15  Iteration 362/8505 Training loss: 2.4952 9.3954 sec/batch\n",
      "Epoch 1/15  Iteration 363/8505 Training loss: 2.4938 9.2992 sec/batch\n",
      "Epoch 1/15  Iteration 364/8505 Training loss: 2.4923 9.2683 sec/batch\n",
      "Epoch 1/15  Iteration 365/8505 Training loss: 2.4911 9.2673 sec/batch\n",
      "Epoch 1/15  Iteration 366/8505 Training loss: 2.4897 9.2798 sec/batch\n",
      "Epoch 1/15  Iteration 367/8505 Training loss: 2.4884 9.3035 sec/batch\n",
      "Epoch 1/15  Iteration 368/8505 Training loss: 2.4870 9.3024 sec/batch\n",
      "Epoch 1/15  Iteration 369/8505 Training loss: 2.4855 9.2894 sec/batch\n",
      "Epoch 1/15  Iteration 370/8505 Training loss: 2.4841 9.2939 sec/batch\n",
      "Epoch 1/15  Iteration 371/8505 Training loss: 2.4827 9.2505 sec/batch\n",
      "Epoch 1/15  Iteration 372/8505 Training loss: 2.4813 9.2942 sec/batch\n",
      "Epoch 1/15  Iteration 373/8505 Training loss: 2.4800 9.3018 sec/batch\n",
      "Epoch 1/15  Iteration 374/8505 Training loss: 2.4787 9.2633 sec/batch\n",
      "Epoch 1/15  Iteration 375/8505 Training loss: 2.4773 9.2553 sec/batch\n",
      "Epoch 1/15  Iteration 376/8505 Training loss: 2.4759 9.3394 sec/batch\n",
      "Epoch 1/15  Iteration 377/8505 Training loss: 2.4744 9.2954 sec/batch\n",
      "Epoch 1/15  Iteration 378/8505 Training loss: 2.4731 9.2250 sec/batch\n",
      "Epoch 1/15  Iteration 379/8505 Training loss: 2.4717 9.2911 sec/batch\n",
      "Epoch 1/15  Iteration 380/8505 Training loss: 2.4705 9.3855 sec/batch\n",
      "Epoch 1/15  Iteration 381/8505 Training loss: 2.4691 9.3210 sec/batch\n",
      "Epoch 1/15  Iteration 382/8505 Training loss: 2.4677 9.4051 sec/batch\n",
      "Epoch 1/15  Iteration 383/8505 Training loss: 2.4664 9.2223 sec/batch\n",
      "Epoch 1/15  Iteration 384/8505 Training loss: 2.4650 9.3493 sec/batch\n",
      "Epoch 1/15  Iteration 385/8505 Training loss: 2.4636 9.2926 sec/batch\n",
      "Epoch 1/15  Iteration 386/8505 Training loss: 2.4623 9.3620 sec/batch\n",
      "Epoch 1/15  Iteration 387/8505 Training loss: 2.4609 9.2931 sec/batch\n",
      "Epoch 1/15  Iteration 388/8505 Training loss: 2.4595 9.5145 sec/batch\n",
      "Epoch 1/15  Iteration 389/8505 Training loss: 2.4582 9.3428 sec/batch\n",
      "Epoch 1/15  Iteration 390/8505 Training loss: 2.4568 9.2803 sec/batch\n",
      "Epoch 1/15  Iteration 391/8505 Training loss: 2.4555 9.3599 sec/batch\n",
      "Epoch 1/15  Iteration 392/8505 Training loss: 2.4541 9.2578 sec/batch\n",
      "Epoch 1/15  Iteration 393/8505 Training loss: 2.4528 9.3260 sec/batch\n",
      "Epoch 1/15  Iteration 394/8505 Training loss: 2.4515 9.4117 sec/batch\n",
      "Epoch 1/15  Iteration 395/8505 Training loss: 2.4502 9.2458 sec/batch\n",
      "Epoch 1/15  Iteration 396/8505 Training loss: 2.4489 9.3824 sec/batch\n",
      "Epoch 1/15  Iteration 397/8505 Training loss: 2.4476 9.3222 sec/batch\n",
      "Epoch 1/15  Iteration 398/8505 Training loss: 2.4464 9.3334 sec/batch\n",
      "Epoch 1/15  Iteration 399/8505 Training loss: 2.4451 9.3959 sec/batch\n",
      "Epoch 1/15  Iteration 400/8505 Training loss: 2.4438 9.4896 sec/batch\n",
      "Validation loss: 1.87679 Saving checkpoint!\n",
      "Epoch 1/15  Iteration 401/8505 Training loss: 2.4425 11.8764 sec/batch\n",
      "Epoch 1/15  Iteration 402/8505 Training loss: 2.4412 9.4431 sec/batch\n",
      "Epoch 1/15  Iteration 403/8505 Training loss: 2.4399 9.4506 sec/batch\n",
      "Epoch 1/15  Iteration 404/8505 Training loss: 2.4386 9.3780 sec/batch\n",
      "Epoch 1/15  Iteration 405/8505 Training loss: 2.4373 9.3200 sec/batch\n",
      "Epoch 1/15  Iteration 406/8505 Training loss: 2.4360 9.3380 sec/batch\n",
      "Epoch 1/15  Iteration 407/8505 Training loss: 2.4347 9.3725 sec/batch\n",
      "Epoch 1/15  Iteration 408/8505 Training loss: 2.4333 9.2751 sec/batch\n",
      "Epoch 1/15  Iteration 409/8505 Training loss: 2.4320 9.2944 sec/batch\n",
      "Epoch 1/15  Iteration 410/8505 Training loss: 2.4308 9.2586 sec/batch\n",
      "Epoch 1/15  Iteration 411/8505 Training loss: 2.4296 9.3406 sec/batch\n",
      "Epoch 1/15  Iteration 412/8505 Training loss: 2.4284 9.3022 sec/batch\n",
      "Epoch 1/15  Iteration 413/8505 Training loss: 2.4272 9.3091 sec/batch\n",
      "Epoch 1/15  Iteration 414/8505 Training loss: 2.4259 9.2749 sec/batch\n",
      "Epoch 1/15  Iteration 415/8505 Training loss: 2.4245 9.3835 sec/batch\n",
      "Epoch 1/15  Iteration 416/8505 Training loss: 2.4233 9.4073 sec/batch\n",
      "Epoch 1/15  Iteration 417/8505 Training loss: 2.4221 9.3428 sec/batch\n",
      "Epoch 1/15  Iteration 418/8505 Training loss: 2.4208 9.3923 sec/batch\n",
      "Epoch 1/15  Iteration 419/8505 Training loss: 2.4196 9.4114 sec/batch\n",
      "Epoch 1/15  Iteration 420/8505 Training loss: 2.4183 9.3081 sec/batch\n",
      "Epoch 1/15  Iteration 421/8505 Training loss: 2.4171 9.3723 sec/batch\n",
      "Epoch 1/15  Iteration 422/8505 Training loss: 2.4160 9.4103 sec/batch\n",
      "Epoch 1/15  Iteration 423/8505 Training loss: 2.4148 9.3080 sec/batch\n",
      "Epoch 1/15  Iteration 424/8505 Training loss: 2.4136 9.4055 sec/batch\n",
      "Epoch 1/15  Iteration 425/8505 Training loss: 2.4124 9.3297 sec/batch\n",
      "Epoch 1/15  Iteration 426/8505 Training loss: 2.4113 9.2856 sec/batch\n",
      "Epoch 1/15  Iteration 427/8505 Training loss: 2.4102 9.3013 sec/batch\n",
      "Epoch 1/15  Iteration 428/8505 Training loss: 2.4090 9.2796 sec/batch\n",
      "Epoch 1/15  Iteration 429/8505 Training loss: 2.4079 9.3623 sec/batch\n",
      "Epoch 1/15  Iteration 430/8505 Training loss: 2.4067 9.3547 sec/batch\n",
      "Epoch 1/15  Iteration 431/8505 Training loss: 2.4055 9.3499 sec/batch\n",
      "Epoch 1/15  Iteration 432/8505 Training loss: 2.4042 9.2779 sec/batch\n",
      "Epoch 1/15  Iteration 433/8505 Training loss: 2.4030 9.2798 sec/batch\n",
      "Epoch 1/15  Iteration 434/8505 Training loss: 2.4019 9.3205 sec/batch\n",
      "Epoch 1/15  Iteration 435/8505 Training loss: 2.4006 9.3638 sec/batch\n",
      "Epoch 1/15  Iteration 436/8505 Training loss: 2.3994 9.3014 sec/batch\n",
      "Epoch 1/15  Iteration 437/8505 Training loss: 2.3982 9.3869 sec/batch\n",
      "Epoch 1/15  Iteration 438/8505 Training loss: 2.3969 9.2922 sec/batch\n",
      "Epoch 1/15  Iteration 439/8505 Training loss: 2.3957 9.2559 sec/batch\n",
      "Epoch 1/15  Iteration 440/8505 Training loss: 2.3946 9.2764 sec/batch\n",
      "Epoch 1/15  Iteration 441/8505 Training loss: 2.3935 9.3214 sec/batch\n",
      "Epoch 1/15  Iteration 442/8505 Training loss: 2.3922 9.5002 sec/batch\n",
      "Epoch 1/15  Iteration 443/8505 Training loss: 2.3910 9.3781 sec/batch\n",
      "Epoch 1/15  Iteration 444/8505 Training loss: 2.3899 9.4523 sec/batch\n",
      "Epoch 1/15  Iteration 445/8505 Training loss: 2.3888 9.4808 sec/batch\n",
      "Epoch 1/15  Iteration 446/8505 Training loss: 2.3876 9.2489 sec/batch\n",
      "Epoch 1/15  Iteration 447/8505 Training loss: 2.3864 9.3583 sec/batch\n",
      "Epoch 1/15  Iteration 448/8505 Training loss: 2.3853 9.2319 sec/batch\n",
      "Epoch 1/15  Iteration 449/8505 Training loss: 2.3841 9.3740 sec/batch\n",
      "Epoch 1/15  Iteration 450/8505 Training loss: 2.3830 9.2208 sec/batch\n",
      "Epoch 1/15  Iteration 451/8505 Training loss: 2.3818 9.4401 sec/batch\n",
      "Epoch 1/15  Iteration 452/8505 Training loss: 2.3807 9.2805 sec/batch\n",
      "Epoch 1/15  Iteration 453/8505 Training loss: 2.3796 9.4126 sec/batch\n",
      "Epoch 1/15  Iteration 454/8505 Training loss: 2.3785 9.3304 sec/batch\n",
      "Epoch 1/15  Iteration 455/8505 Training loss: 2.3774 9.2569 sec/batch\n",
      "Epoch 1/15  Iteration 456/8505 Training loss: 2.3764 9.2793 sec/batch\n",
      "Epoch 1/15  Iteration 457/8505 Training loss: 2.3752 9.3061 sec/batch\n",
      "Epoch 1/15  Iteration 458/8505 Training loss: 2.3741 9.3081 sec/batch\n",
      "Epoch 1/15  Iteration 459/8505 Training loss: 2.3730 9.2944 sec/batch\n",
      "Epoch 1/15  Iteration 460/8505 Training loss: 2.3720 9.1929 sec/batch\n",
      "Epoch 1/15  Iteration 461/8505 Training loss: 2.3707 9.3111 sec/batch\n",
      "Epoch 1/15  Iteration 462/8505 Training loss: 2.3696 9.4382 sec/batch\n",
      "Epoch 1/15  Iteration 463/8505 Training loss: 2.3686 9.2383 sec/batch\n",
      "Epoch 1/15  Iteration 464/8505 Training loss: 2.3675 9.3600 sec/batch\n",
      "Epoch 1/15  Iteration 465/8505 Training loss: 2.3665 9.3382 sec/batch\n",
      "Epoch 1/15  Iteration 466/8505 Training loss: 2.3655 9.3460 sec/batch\n",
      "Epoch 1/15  Iteration 467/8505 Training loss: 2.3644 9.2990 sec/batch\n",
      "Epoch 1/15  Iteration 468/8505 Training loss: 2.3633 9.3432 sec/batch\n",
      "Epoch 1/15  Iteration 469/8505 Training loss: 2.3622 9.3694 sec/batch\n",
      "Epoch 1/15  Iteration 470/8505 Training loss: 2.3611 9.2873 sec/batch\n",
      "Epoch 1/15  Iteration 471/8505 Training loss: 2.3599 9.2992 sec/batch\n",
      "Epoch 1/15  Iteration 472/8505 Training loss: 2.3588 9.2871 sec/batch\n",
      "Epoch 1/15  Iteration 473/8505 Training loss: 2.3577 9.2175 sec/batch\n",
      "Epoch 1/15  Iteration 474/8505 Training loss: 2.3567 9.3250 sec/batch\n",
      "Epoch 1/15  Iteration 475/8505 Training loss: 2.3557 9.2633 sec/batch\n",
      "Epoch 1/15  Iteration 476/8505 Training loss: 2.3546 9.5757 sec/batch\n",
      "Epoch 1/15  Iteration 477/8505 Training loss: 2.3535 9.2527 sec/batch\n",
      "Epoch 1/15  Iteration 478/8505 Training loss: 2.3524 9.3882 sec/batch\n",
      "Epoch 1/15  Iteration 479/8505 Training loss: 2.3513 9.3169 sec/batch\n",
      "Epoch 1/15  Iteration 480/8505 Training loss: 2.3501 9.3194 sec/batch\n",
      "Epoch 1/15  Iteration 481/8505 Training loss: 2.3491 9.4894 sec/batch\n",
      "Epoch 1/15  Iteration 482/8505 Training loss: 2.3479 9.4164 sec/batch\n",
      "Epoch 1/15  Iteration 483/8505 Training loss: 2.3468 9.3495 sec/batch\n",
      "Epoch 1/15  Iteration 484/8505 Training loss: 2.3458 9.3491 sec/batch\n",
      "Epoch 1/15  Iteration 485/8505 Training loss: 2.3447 9.2684 sec/batch\n",
      "Epoch 1/15  Iteration 486/8505 Training loss: 2.3437 9.2822 sec/batch\n",
      "Epoch 1/15  Iteration 487/8505 Training loss: 2.3426 9.3031 sec/batch\n",
      "Epoch 1/15  Iteration 488/8505 Training loss: 2.3415 9.3419 sec/batch\n",
      "Epoch 1/15  Iteration 489/8505 Training loss: 2.3405 9.2644 sec/batch\n",
      "Epoch 1/15  Iteration 490/8505 Training loss: 2.3394 9.3098 sec/batch\n",
      "Epoch 1/15  Iteration 491/8505 Training loss: 2.3384 9.2922 sec/batch\n",
      "Epoch 1/15  Iteration 492/8505 Training loss: 2.3373 9.2721 sec/batch\n",
      "Epoch 1/15  Iteration 493/8505 Training loss: 2.3362 9.2597 sec/batch\n",
      "Epoch 1/15  Iteration 494/8505 Training loss: 2.3352 9.2501 sec/batch\n",
      "Epoch 1/15  Iteration 495/8505 Training loss: 2.3342 9.5916 sec/batch\n",
      "Epoch 1/15  Iteration 496/8505 Training loss: 2.3331 9.3734 sec/batch\n",
      "Epoch 1/15  Iteration 497/8505 Training loss: 2.3320 9.2901 sec/batch\n",
      "Epoch 1/15  Iteration 498/8505 Training loss: 2.3310 9.2574 sec/batch\n",
      "Epoch 1/15  Iteration 499/8505 Training loss: 2.3299 9.3241 sec/batch\n",
      "Epoch 1/15  Iteration 500/8505 Training loss: 2.3288 9.3130 sec/batch\n",
      "Epoch 1/15  Iteration 501/8505 Training loss: 2.3278 9.2970 sec/batch\n",
      "Epoch 1/15  Iteration 502/8505 Training loss: 2.3267 9.2772 sec/batch\n",
      "Epoch 1/15  Iteration 503/8505 Training loss: 2.3257 9.2764 sec/batch\n",
      "Epoch 1/15  Iteration 504/8505 Training loss: 2.3246 9.2558 sec/batch\n",
      "Epoch 1/15  Iteration 505/8505 Training loss: 2.3236 9.3629 sec/batch\n",
      "Epoch 1/15  Iteration 506/8505 Training loss: 2.3226 9.3021 sec/batch\n",
      "Epoch 1/15  Iteration 507/8505 Training loss: 2.3215 9.2719 sec/batch\n",
      "Epoch 1/15  Iteration 508/8505 Training loss: 2.3205 9.3275 sec/batch\n",
      "Epoch 1/15  Iteration 509/8505 Training loss: 2.3194 9.4821 sec/batch\n",
      "Epoch 1/15  Iteration 510/8505 Training loss: 2.3183 9.3314 sec/batch\n",
      "Epoch 1/15  Iteration 511/8505 Training loss: 2.3173 9.2966 sec/batch\n",
      "Epoch 1/15  Iteration 512/8505 Training loss: 2.3162 9.3322 sec/batch\n",
      "Epoch 1/15  Iteration 513/8505 Training loss: 2.3152 9.3248 sec/batch\n",
      "Epoch 1/15  Iteration 514/8505 Training loss: 2.3142 9.3299 sec/batch\n",
      "Epoch 1/15  Iteration 515/8505 Training loss: 2.3132 9.2486 sec/batch\n",
      "Epoch 1/15  Iteration 516/8505 Training loss: 2.3122 9.3005 sec/batch\n",
      "Epoch 1/15  Iteration 517/8505 Training loss: 2.3112 9.2881 sec/batch\n",
      "Epoch 1/15  Iteration 518/8505 Training loss: 2.3101 9.2792 sec/batch\n",
      "Epoch 1/15  Iteration 519/8505 Training loss: 2.3091 9.2943 sec/batch\n",
      "Epoch 1/15  Iteration 520/8505 Training loss: 2.3081 9.4284 sec/batch\n",
      "Epoch 1/15  Iteration 521/8505 Training loss: 2.3071 9.2945 sec/batch\n",
      "Epoch 1/15  Iteration 522/8505 Training loss: 2.3060 9.2754 sec/batch\n",
      "Epoch 1/15  Iteration 523/8505 Training loss: 2.3050 9.3016 sec/batch\n",
      "Epoch 1/15  Iteration 524/8505 Training loss: 2.3040 9.2906 sec/batch\n",
      "Epoch 1/15  Iteration 525/8505 Training loss: 2.3029 9.2844 sec/batch\n",
      "Epoch 1/15  Iteration 526/8505 Training loss: 2.3019 9.2614 sec/batch\n",
      "Epoch 1/15  Iteration 527/8505 Training loss: 2.3009 9.3359 sec/batch\n",
      "Epoch 1/15  Iteration 528/8505 Training loss: 2.2998 9.3212 sec/batch\n",
      "Epoch 1/15  Iteration 529/8505 Training loss: 2.2989 9.2834 sec/batch\n",
      "Epoch 1/15  Iteration 530/8505 Training loss: 2.2980 9.2863 sec/batch\n",
      "Epoch 1/15  Iteration 531/8505 Training loss: 2.2970 9.3099 sec/batch\n",
      "Epoch 1/15  Iteration 532/8505 Training loss: 2.2960 9.2281 sec/batch\n",
      "Epoch 1/15  Iteration 533/8505 Training loss: 2.2950 9.2812 sec/batch\n",
      "Epoch 1/15  Iteration 534/8505 Training loss: 2.2940 9.2696 sec/batch\n",
      "Epoch 1/15  Iteration 535/8505 Training loss: 2.2930 9.2921 sec/batch\n",
      "Epoch 1/15  Iteration 536/8505 Training loss: 2.2921 9.2607 sec/batch\n",
      "Epoch 1/15  Iteration 537/8505 Training loss: 2.2910 9.2561 sec/batch\n",
      "Epoch 1/15  Iteration 538/8505 Training loss: 2.2900 9.2999 sec/batch\n",
      "Epoch 1/15  Iteration 539/8505 Training loss: 2.2890 9.2637 sec/batch\n",
      "Epoch 1/15  Iteration 540/8505 Training loss: 2.2881 9.2547 sec/batch\n",
      "Epoch 1/15  Iteration 541/8505 Training loss: 2.2872 9.2398 sec/batch\n",
      "Epoch 1/15  Iteration 542/8505 Training loss: 2.2862 9.2913 sec/batch\n",
      "Epoch 1/15  Iteration 543/8505 Training loss: 2.2852 9.2970 sec/batch\n",
      "Epoch 1/15  Iteration 544/8505 Training loss: 2.2843 9.2866 sec/batch\n",
      "Epoch 1/15  Iteration 545/8505 Training loss: 2.2833 9.2699 sec/batch\n",
      "Epoch 1/15  Iteration 546/8505 Training loss: 2.2824 9.3455 sec/batch\n",
      "Epoch 1/15  Iteration 547/8505 Training loss: 2.2815 9.3172 sec/batch\n",
      "Epoch 1/15  Iteration 548/8505 Training loss: 2.2806 9.2422 sec/batch\n",
      "Epoch 1/15  Iteration 549/8505 Training loss: 2.2796 9.2700 sec/batch\n",
      "Epoch 1/15  Iteration 550/8505 Training loss: 2.2786 9.4506 sec/batch\n",
      "Epoch 1/15  Iteration 551/8505 Training loss: 2.2777 9.2424 sec/batch\n",
      "Epoch 1/15  Iteration 552/8505 Training loss: 2.2768 9.2232 sec/batch\n",
      "Epoch 1/15  Iteration 553/8505 Training loss: 2.2759 9.2962 sec/batch\n",
      "Epoch 1/15  Iteration 554/8505 Training loss: 2.2751 9.2715 sec/batch\n",
      "Epoch 1/15  Iteration 555/8505 Training loss: 2.2742 9.1924 sec/batch\n",
      "Epoch 1/15  Iteration 556/8505 Training loss: 2.2732 9.3663 sec/batch\n",
      "Epoch 1/15  Iteration 557/8505 Training loss: 2.2723 9.2610 sec/batch\n",
      "Epoch 1/15  Iteration 558/8505 Training loss: 2.2715 9.3110 sec/batch\n",
      "Epoch 1/15  Iteration 559/8505 Training loss: 2.2706 9.2342 sec/batch\n",
      "Epoch 1/15  Iteration 560/8505 Training loss: 2.2697 9.2623 sec/batch\n",
      "Epoch 1/15  Iteration 561/8505 Training loss: 2.2689 9.2945 sec/batch\n",
      "Epoch 1/15  Iteration 562/8505 Training loss: 2.2679 9.2419 sec/batch\n",
      "Epoch 1/15  Iteration 563/8505 Training loss: 2.2670 9.2470 sec/batch\n",
      "Epoch 1/15  Iteration 564/8505 Training loss: 2.2661 9.2678 sec/batch\n",
      "Epoch 1/15  Iteration 565/8505 Training loss: 2.2651 9.3585 sec/batch\n",
      "Epoch 1/15  Iteration 566/8505 Training loss: 2.2641 9.2260 sec/batch\n",
      "Epoch 1/15  Iteration 567/8505 Training loss: 2.2632 9.2774 sec/batch\n",
      "Epoch 2/15  Iteration 568/8505 Training loss: 1.8519 9.2221 sec/batch\n",
      "Epoch 2/15  Iteration 569/8505 Training loss: 1.8195 9.2910 sec/batch\n",
      "Epoch 2/15  Iteration 570/8505 Training loss: 1.8045 9.3425 sec/batch\n",
      "Epoch 2/15  Iteration 571/8505 Training loss: 1.7974 9.2379 sec/batch\n",
      "Epoch 2/15  Iteration 572/8505 Training loss: 1.7951 9.2248 sec/batch\n",
      "Epoch 2/15  Iteration 573/8505 Training loss: 1.7891 9.2693 sec/batch\n",
      "Epoch 2/15  Iteration 574/8505 Training loss: 1.7833 9.3692 sec/batch\n",
      "Epoch 2/15  Iteration 575/8505 Training loss: 1.7802 9.3398 sec/batch\n",
      "Epoch 2/15  Iteration 576/8505 Training loss: 1.7788 9.3157 sec/batch\n",
      "Epoch 2/15  Iteration 577/8505 Training loss: 1.7708 9.2582 sec/batch\n",
      "Epoch 2/15  Iteration 578/8505 Training loss: 1.7732 9.1898 sec/batch\n",
      "Epoch 2/15  Iteration 579/8505 Training loss: 1.7727 9.1994 sec/batch\n",
      "Epoch 2/15  Iteration 580/8505 Training loss: 1.7715 9.2980 sec/batch\n",
      "Epoch 2/15  Iteration 581/8505 Training loss: 1.7719 9.3186 sec/batch\n",
      "Epoch 2/15  Iteration 582/8505 Training loss: 1.7688 9.1717 sec/batch\n",
      "Epoch 2/15  Iteration 583/8505 Training loss: 1.7675 9.2712 sec/batch\n",
      "Epoch 2/15  Iteration 584/8505 Training loss: 1.7663 9.2959 sec/batch\n",
      "Epoch 2/15  Iteration 585/8505 Training loss: 1.7648 9.2802 sec/batch\n",
      "Epoch 2/15  Iteration 586/8505 Training loss: 1.7640 9.3370 sec/batch\n",
      "Epoch 2/15  Iteration 587/8505 Training loss: 1.7641 9.2216 sec/batch\n",
      "Epoch 2/15  Iteration 588/8505 Training loss: 1.7642 9.2234 sec/batch\n",
      "Epoch 2/15  Iteration 589/8505 Training loss: 1.7623 9.2674 sec/batch\n",
      "Epoch 2/15  Iteration 590/8505 Training loss: 1.7603 9.2523 sec/batch\n",
      "Epoch 2/15  Iteration 591/8505 Training loss: 1.7600 9.1868 sec/batch\n",
      "Epoch 2/15  Iteration 592/8505 Training loss: 1.7595 9.2367 sec/batch\n",
      "Epoch 2/15  Iteration 593/8505 Training loss: 1.7585 9.1891 sec/batch\n",
      "Epoch 2/15  Iteration 594/8505 Training loss: 1.7577 9.2016 sec/batch\n",
      "Epoch 2/15  Iteration 595/8505 Training loss: 1.7555 9.2153 sec/batch\n",
      "Epoch 2/15  Iteration 596/8505 Training loss: 1.7552 9.2993 sec/batch\n",
      "Epoch 2/15  Iteration 597/8505 Training loss: 1.7542 9.2364 sec/batch\n",
      "Epoch 2/15  Iteration 598/8505 Training loss: 1.7518 9.3003 sec/batch\n",
      "Epoch 2/15  Iteration 599/8505 Training loss: 1.7506 9.3400 sec/batch\n",
      "Epoch 2/15  Iteration 600/8505 Training loss: 1.7500 9.3054 sec/batch\n",
      "Validation loss: 1.6809 Saving checkpoint!\n",
      "Epoch 2/15  Iteration 601/8505 Training loss: 1.7514 9.2519 sec/batch\n",
      "Epoch 2/15  Iteration 602/8505 Training loss: 1.7506 9.3042 sec/batch\n",
      "Epoch 2/15  Iteration 603/8505 Training loss: 1.7502 9.2741 sec/batch\n",
      "Epoch 2/15  Iteration 604/8505 Training loss: 1.7503 9.2515 sec/batch\n",
      "Epoch 2/15  Iteration 605/8505 Training loss: 1.7500 9.2693 sec/batch\n",
      "Epoch 2/15  Iteration 606/8505 Training loss: 1.7495 9.2800 sec/batch\n",
      "Epoch 2/15  Iteration 607/8505 Training loss: 1.7481 9.3413 sec/batch\n",
      "Epoch 2/15  Iteration 608/8505 Training loss: 1.7473 9.3875 sec/batch\n",
      "Epoch 2/15  Iteration 609/8505 Training loss: 1.7469 9.3348 sec/batch\n",
      "Epoch 2/15  Iteration 610/8505 Training loss: 1.7474 9.2916 sec/batch\n",
      "Epoch 2/15  Iteration 611/8505 Training loss: 1.7466 9.2437 sec/batch\n",
      "Epoch 2/15  Iteration 612/8505 Training loss: 1.7461 9.3348 sec/batch\n",
      "Epoch 2/15  Iteration 613/8505 Training loss: 1.7454 9.3360 sec/batch\n",
      "Epoch 2/15  Iteration 614/8505 Training loss: 1.7453 9.3088 sec/batch\n",
      "Epoch 2/15  Iteration 615/8505 Training loss: 1.7447 9.3330 sec/batch\n",
      "Epoch 2/15  Iteration 616/8505 Training loss: 1.7436 9.3108 sec/batch\n",
      "Epoch 2/15  Iteration 617/8505 Training loss: 1.7421 9.3349 sec/batch\n",
      "Epoch 2/15  Iteration 618/8505 Training loss: 1.7416 9.2887 sec/batch\n",
      "Epoch 2/15  Iteration 619/8505 Training loss: 1.7411 9.3413 sec/batch\n",
      "Epoch 2/15  Iteration 620/8505 Training loss: 1.7402 9.2448 sec/batch\n",
      "Epoch 2/15  Iteration 621/8505 Training loss: 1.7402 9.2364 sec/batch\n",
      "Epoch 2/15  Iteration 622/8505 Training loss: 1.7397 9.2522 sec/batch\n",
      "Epoch 2/15  Iteration 623/8505 Training loss: 1.7391 9.2853 sec/batch\n",
      "Epoch 2/15  Iteration 624/8505 Training loss: 1.7382 9.3395 sec/batch\n",
      "Epoch 2/15  Iteration 625/8505 Training loss: 1.7380 9.2796 sec/batch\n",
      "Epoch 2/15  Iteration 626/8505 Training loss: 1.7384 9.2522 sec/batch\n",
      "Epoch 2/15  Iteration 627/8505 Training loss: 1.7386 9.3501 sec/batch\n",
      "Epoch 2/15  Iteration 628/8505 Training loss: 1.7384 9.2011 sec/batch\n",
      "Epoch 2/15  Iteration 629/8505 Training loss: 1.7379 9.3324 sec/batch\n",
      "Epoch 2/15  Iteration 630/8505 Training loss: 1.7375 9.2582 sec/batch\n",
      "Epoch 2/15  Iteration 631/8505 Training loss: 1.7373 9.4622 sec/batch\n",
      "Epoch 2/15  Iteration 632/8505 Training loss: 1.7367 9.2879 sec/batch\n",
      "Epoch 2/15  Iteration 633/8505 Training loss: 1.7367 9.2427 sec/batch\n",
      "Epoch 2/15  Iteration 634/8505 Training loss: 1.7363 9.3472 sec/batch\n",
      "Epoch 2/15  Iteration 635/8505 Training loss: 1.7359 9.3622 sec/batch\n",
      "Epoch 2/15  Iteration 636/8505 Training loss: 1.7357 9.2288 sec/batch\n",
      "Epoch 2/15  Iteration 637/8505 Training loss: 1.7353 9.1994 sec/batch\n",
      "Epoch 2/15  Iteration 638/8505 Training loss: 1.7347 9.2751 sec/batch\n",
      "Epoch 2/15  Iteration 639/8505 Training loss: 1.7343 9.2985 sec/batch\n",
      "Epoch 2/15  Iteration 640/8505 Training loss: 1.7339 9.2103 sec/batch\n",
      "Epoch 2/15  Iteration 641/8505 Training loss: 1.7338 9.3302 sec/batch\n",
      "Epoch 2/15  Iteration 642/8505 Training loss: 1.7334 9.2944 sec/batch\n",
      "Epoch 2/15  Iteration 643/8505 Training loss: 1.7330 9.2331 sec/batch\n",
      "Epoch 2/15  Iteration 644/8505 Training loss: 1.7327 9.2335 sec/batch\n",
      "Epoch 2/15  Iteration 645/8505 Training loss: 1.7324 9.2218 sec/batch\n",
      "Epoch 2/15  Iteration 646/8505 Training loss: 1.7322 9.2743 sec/batch\n",
      "Epoch 2/15  Iteration 647/8505 Training loss: 1.7315 9.3243 sec/batch\n",
      "Epoch 2/15  Iteration 648/8505 Training loss: 1.7312 9.2990 sec/batch\n",
      "Epoch 2/15  Iteration 649/8505 Training loss: 1.7307 9.1951 sec/batch\n",
      "Epoch 2/15  Iteration 650/8505 Training loss: 1.7303 9.3625 sec/batch\n",
      "Epoch 2/15  Iteration 651/8505 Training loss: 1.7295 9.3696 sec/batch\n",
      "Epoch 2/15  Iteration 652/8505 Training loss: 1.7291 9.2666 sec/batch\n",
      "Epoch 2/15  Iteration 653/8505 Training loss: 1.7286 9.2455 sec/batch\n",
      "Epoch 2/15  Iteration 654/8505 Training loss: 1.7286 9.3779 sec/batch\n",
      "Epoch 2/15  Iteration 655/8505 Training loss: 1.7283 9.3154 sec/batch\n",
      "Epoch 2/15  Iteration 656/8505 Training loss: 1.7277 9.3460 sec/batch\n",
      "Epoch 2/15  Iteration 657/8505 Training loss: 1.7272 9.2256 sec/batch\n",
      "Epoch 2/15  Iteration 658/8505 Training loss: 1.7265 9.5286 sec/batch\n",
      "Epoch 2/15  Iteration 659/8505 Training loss: 1.7260 9.4457 sec/batch\n",
      "Epoch 2/15  Iteration 660/8505 Training loss: 1.7262 9.2555 sec/batch\n",
      "Epoch 2/15  Iteration 661/8505 Training loss: 1.7258 9.2619 sec/batch\n",
      "Epoch 2/15  Iteration 662/8505 Training loss: 1.7257 9.4590 sec/batch\n",
      "Epoch 2/15  Iteration 663/8505 Training loss: 1.7258 9.2792 sec/batch\n",
      "Epoch 2/15  Iteration 664/8505 Training loss: 1.7252 9.6148 sec/batch\n",
      "Epoch 2/15  Iteration 665/8505 Training loss: 1.7247 9.2473 sec/batch\n",
      "Epoch 2/15  Iteration 666/8505 Training loss: 1.7243 9.3127 sec/batch\n",
      "Epoch 2/15  Iteration 667/8505 Training loss: 1.7239 9.2766 sec/batch\n",
      "Epoch 2/15  Iteration 668/8505 Training loss: 1.7235 9.2118 sec/batch\n",
      "Epoch 2/15  Iteration 669/8505 Training loss: 1.7232 9.3199 sec/batch\n",
      "Epoch 2/15  Iteration 670/8505 Training loss: 1.7231 9.2628 sec/batch\n",
      "Epoch 2/15  Iteration 671/8505 Training loss: 1.7227 9.2753 sec/batch\n",
      "Epoch 2/15  Iteration 672/8505 Training loss: 1.7228 9.2455 sec/batch\n",
      "Epoch 2/15  Iteration 673/8505 Training loss: 1.7225 9.3446 sec/batch\n",
      "Epoch 2/15  Iteration 674/8505 Training loss: 1.7223 9.1756 sec/batch\n",
      "Epoch 2/15  Iteration 675/8505 Training loss: 1.7218 9.1895 sec/batch\n",
      "Epoch 2/15  Iteration 676/8505 Training loss: 1.7215 9.1934 sec/batch\n",
      "Epoch 2/15  Iteration 677/8505 Training loss: 1.7212 9.3068 sec/batch\n",
      "Epoch 2/15  Iteration 678/8505 Training loss: 1.7209 9.2514 sec/batch\n",
      "Epoch 2/15  Iteration 679/8505 Training loss: 1.7206 9.2665 sec/batch\n",
      "Epoch 2/15  Iteration 680/8505 Training loss: 1.7202 9.2467 sec/batch\n",
      "Epoch 2/15  Iteration 681/8505 Training loss: 1.7194 9.2348 sec/batch\n",
      "Epoch 2/15  Iteration 682/8505 Training loss: 1.7190 9.3643 sec/batch\n",
      "Epoch 2/15  Iteration 683/8505 Training loss: 1.7183 9.2565 sec/batch\n",
      "Epoch 2/15  Iteration 684/8505 Training loss: 1.7177 9.5169 sec/batch\n",
      "Epoch 2/15  Iteration 685/8505 Training loss: 1.7176 9.2892 sec/batch\n",
      "Epoch 2/15  Iteration 686/8505 Training loss: 1.7172 9.2414 sec/batch\n",
      "Epoch 2/15  Iteration 687/8505 Training loss: 1.7169 9.2536 sec/batch\n",
      "Epoch 2/15  Iteration 688/8505 Training loss: 1.7169 9.2891 sec/batch\n",
      "Epoch 2/15  Iteration 689/8505 Training loss: 1.7168 9.2624 sec/batch\n",
      "Epoch 2/15  Iteration 690/8505 Training loss: 1.7165 9.2476 sec/batch\n",
      "Epoch 2/15  Iteration 691/8505 Training loss: 1.7161 9.3439 sec/batch\n",
      "Epoch 2/15  Iteration 692/8505 Training loss: 1.7157 9.3442 sec/batch\n",
      "Epoch 2/15  Iteration 693/8505 Training loss: 1.7155 9.4925 sec/batch\n",
      "Epoch 2/15  Iteration 694/8505 Training loss: 1.7151 9.2776 sec/batch\n",
      "Epoch 2/15  Iteration 695/8505 Training loss: 1.7149 9.2228 sec/batch\n",
      "Epoch 2/15  Iteration 696/8505 Training loss: 1.7147 9.2585 sec/batch\n",
      "Epoch 2/15  Iteration 697/8505 Training loss: 1.7145 9.3728 sec/batch\n",
      "Epoch 2/15  Iteration 698/8505 Training loss: 1.7141 9.3084 sec/batch\n",
      "Epoch 2/15  Iteration 699/8505 Training loss: 1.7140 9.3268 sec/batch\n",
      "Epoch 2/15  Iteration 700/8505 Training loss: 1.7140 9.2947 sec/batch\n",
      "Epoch 2/15  Iteration 701/8505 Training loss: 1.7141 9.2440 sec/batch\n",
      "Epoch 2/15  Iteration 702/8505 Training loss: 1.7141 9.2403 sec/batch\n",
      "Epoch 2/15  Iteration 703/8505 Training loss: 1.7140 9.2902 sec/batch\n",
      "Epoch 2/15  Iteration 704/8505 Training loss: 1.7137 9.2681 sec/batch\n",
      "Epoch 2/15  Iteration 705/8505 Training loss: 1.7136 9.2480 sec/batch\n",
      "Epoch 2/15  Iteration 706/8505 Training loss: 1.7131 9.1561 sec/batch\n",
      "Epoch 2/15  Iteration 707/8505 Training loss: 1.7128 9.2759 sec/batch\n",
      "Epoch 2/15  Iteration 708/8505 Training loss: 1.7122 9.1584 sec/batch\n",
      "Epoch 2/15  Iteration 709/8505 Training loss: 1.7116 9.2435 sec/batch\n",
      "Epoch 2/15  Iteration 710/8505 Training loss: 1.7111 9.2260 sec/batch\n",
      "Epoch 2/15  Iteration 711/8505 Training loss: 1.7107 9.2661 sec/batch\n",
      "Epoch 2/15  Iteration 712/8505 Training loss: 1.7102 9.3162 sec/batch\n",
      "Epoch 2/15  Iteration 713/8505 Training loss: 1.7101 9.2785 sec/batch\n",
      "Epoch 2/15  Iteration 714/8505 Training loss: 1.7097 9.2655 sec/batch\n",
      "Epoch 2/15  Iteration 715/8505 Training loss: 1.7092 9.2534 sec/batch\n",
      "Epoch 2/15  Iteration 716/8505 Training loss: 1.7087 9.2292 sec/batch\n",
      "Epoch 2/15  Iteration 717/8505 Training loss: 1.7083 9.3346 sec/batch\n",
      "Epoch 2/15  Iteration 718/8505 Training loss: 1.7079 9.2787 sec/batch\n",
      "Epoch 2/15  Iteration 719/8505 Training loss: 1.7073 9.2650 sec/batch\n",
      "Epoch 2/15  Iteration 720/8505 Training loss: 1.7067 9.2581 sec/batch\n",
      "Epoch 2/15  Iteration 721/8505 Training loss: 1.7065 9.2931 sec/batch\n",
      "Epoch 2/15  Iteration 722/8505 Training loss: 1.7060 9.2801 sec/batch\n",
      "Epoch 2/15  Iteration 723/8505 Training loss: 1.7054 9.2606 sec/batch\n",
      "Epoch 2/15  Iteration 724/8505 Training loss: 1.7052 9.2532 sec/batch\n",
      "Epoch 2/15  Iteration 725/8505 Training loss: 1.7048 9.2622 sec/batch\n",
      "Epoch 2/15  Iteration 726/8505 Training loss: 1.7048 9.2613 sec/batch\n",
      "Epoch 2/15  Iteration 727/8505 Training loss: 1.7043 9.2349 sec/batch\n",
      "Epoch 2/15  Iteration 728/8505 Training loss: 1.7041 9.2743 sec/batch\n",
      "Epoch 2/15  Iteration 729/8505 Training loss: 1.7036 9.2528 sec/batch\n",
      "Epoch 2/15  Iteration 730/8505 Training loss: 1.7033 9.2105 sec/batch\n",
      "Epoch 2/15  Iteration 731/8505 Training loss: 1.7030 9.3063 sec/batch\n",
      "Epoch 2/15  Iteration 732/8505 Training loss: 1.7027 9.2707 sec/batch\n",
      "Epoch 2/15  Iteration 733/8505 Training loss: 1.7024 9.2535 sec/batch\n",
      "Epoch 2/15  Iteration 734/8505 Training loss: 1.7022 9.2518 sec/batch\n",
      "Epoch 2/15  Iteration 735/8505 Training loss: 1.7018 9.3615 sec/batch\n",
      "Epoch 2/15  Iteration 736/8505 Training loss: 1.7017 9.3562 sec/batch\n",
      "Epoch 2/15  Iteration 737/8505 Training loss: 1.7015 9.2882 sec/batch\n",
      "Epoch 2/15  Iteration 738/8505 Training loss: 1.7014 9.2354 sec/batch\n",
      "Epoch 2/15  Iteration 739/8505 Training loss: 1.7011 9.3441 sec/batch\n",
      "Epoch 2/15  Iteration 740/8505 Training loss: 1.7007 9.3974 sec/batch\n",
      "Epoch 2/15  Iteration 741/8505 Training loss: 1.7001 9.3565 sec/batch\n",
      "Epoch 2/15  Iteration 742/8505 Training loss: 1.6997 9.1761 sec/batch\n",
      "Epoch 2/15  Iteration 743/8505 Training loss: 1.6995 9.4625 sec/batch\n",
      "Epoch 2/15  Iteration 744/8505 Training loss: 1.6992 9.3330 sec/batch\n",
      "Epoch 2/15  Iteration 745/8505 Training loss: 1.6989 9.3709 sec/batch\n",
      "Epoch 2/15  Iteration 746/8505 Training loss: 1.6985 9.2890 sec/batch\n",
      "Epoch 2/15  Iteration 747/8505 Training loss: 1.6983 9.5253 sec/batch\n",
      "Epoch 2/15  Iteration 748/8505 Training loss: 1.6979 9.5957 sec/batch\n",
      "Epoch 2/15  Iteration 749/8505 Training loss: 1.6974 9.3298 sec/batch\n",
      "Epoch 2/15  Iteration 750/8505 Training loss: 1.6971 9.3124 sec/batch\n",
      "Epoch 2/15  Iteration 751/8505 Training loss: 1.6968 9.2772 sec/batch\n",
      "Epoch 2/15  Iteration 752/8505 Training loss: 1.6965 9.2660 sec/batch\n",
      "Epoch 2/15  Iteration 753/8505 Training loss: 1.6961 9.1895 sec/batch\n",
      "Epoch 2/15  Iteration 754/8505 Training loss: 1.6957 9.2394 sec/batch\n",
      "Epoch 2/15  Iteration 755/8505 Training loss: 1.6951 9.3287 sec/batch\n",
      "Epoch 2/15  Iteration 756/8505 Training loss: 1.6950 9.3168 sec/batch\n",
      "Epoch 2/15  Iteration 757/8505 Training loss: 1.6945 9.3029 sec/batch\n",
      "Epoch 2/15  Iteration 758/8505 Training loss: 1.6940 9.2548 sec/batch\n",
      "Epoch 2/15  Iteration 759/8505 Training loss: 1.6935 9.2097 sec/batch\n",
      "Epoch 2/15  Iteration 760/8505 Training loss: 1.6934 9.2387 sec/batch\n",
      "Epoch 2/15  Iteration 761/8505 Training loss: 1.6931 9.9701 sec/batch\n",
      "Epoch 2/15  Iteration 762/8505 Training loss: 1.6927 9.3042 sec/batch\n",
      "Epoch 2/15  Iteration 763/8505 Training loss: 1.6924 11.6347 sec/batch\n",
      "Epoch 2/15  Iteration 764/8505 Training loss: 1.6920 9.4370 sec/batch\n",
      "Epoch 2/15  Iteration 765/8505 Training loss: 1.6917 9.2141 sec/batch\n",
      "Epoch 2/15  Iteration 766/8505 Training loss: 1.6915 9.2862 sec/batch\n",
      "Epoch 2/15  Iteration 767/8505 Training loss: 1.6913 9.1977 sec/batch\n",
      "Epoch 2/15  Iteration 768/8505 Training loss: 1.6911 9.2877 sec/batch\n",
      "Epoch 2/15  Iteration 769/8505 Training loss: 1.6908 9.1921 sec/batch\n",
      "Epoch 2/15  Iteration 770/8505 Training loss: 1.6906 9.1869 sec/batch\n",
      "Epoch 2/15  Iteration 771/8505 Training loss: 1.6901 9.2689 sec/batch\n",
      "Epoch 2/15  Iteration 772/8505 Training loss: 1.6897 9.1966 sec/batch\n",
      "Epoch 2/15  Iteration 773/8505 Training loss: 1.6893 9.2297 sec/batch\n",
      "Epoch 2/15  Iteration 774/8505 Training loss: 1.6891 9.2688 sec/batch\n",
      "Epoch 2/15  Iteration 775/8505 Training loss: 1.6886 9.3004 sec/batch\n",
      "Epoch 2/15  Iteration 776/8505 Training loss: 1.6882 9.2252 sec/batch\n",
      "Epoch 2/15  Iteration 777/8505 Training loss: 1.6876 9.2468 sec/batch\n",
      "Epoch 2/15  Iteration 778/8505 Training loss: 1.6872 9.2430 sec/batch\n",
      "Epoch 2/15  Iteration 779/8505 Training loss: 1.6869 9.3042 sec/batch\n",
      "Epoch 2/15  Iteration 780/8505 Training loss: 1.6865 9.2648 sec/batch\n",
      "Epoch 2/15  Iteration 781/8505 Training loss: 1.6861 9.4776 sec/batch\n",
      "Epoch 2/15  Iteration 782/8505 Training loss: 1.6857 9.2122 sec/batch\n",
      "Epoch 2/15  Iteration 783/8505 Training loss: 1.6855 9.3222 sec/batch\n",
      "Epoch 2/15  Iteration 784/8505 Training loss: 1.6850 9.2452 sec/batch\n",
      "Epoch 2/15  Iteration 785/8505 Training loss: 1.6847 9.2680 sec/batch\n",
      "Epoch 2/15  Iteration 786/8505 Training loss: 1.6843 9.2557 sec/batch\n",
      "Epoch 2/15  Iteration 787/8505 Training loss: 1.6837 9.3779 sec/batch\n",
      "Epoch 2/15  Iteration 788/8505 Training loss: 1.6833 9.1875 sec/batch\n",
      "Epoch 2/15  Iteration 789/8505 Training loss: 1.6829 9.2572 sec/batch\n",
      "Epoch 2/15  Iteration 790/8505 Training loss: 1.6825 9.3042 sec/batch\n",
      "Epoch 2/15  Iteration 791/8505 Training loss: 1.6820 9.3392 sec/batch\n",
      "Epoch 2/15  Iteration 792/8505 Training loss: 1.6816 9.2490 sec/batch\n",
      "Epoch 2/15  Iteration 793/8505 Training loss: 1.6813 9.2540 sec/batch\n",
      "Epoch 2/15  Iteration 794/8505 Training loss: 1.6809 9.2372 sec/batch\n",
      "Epoch 2/15  Iteration 795/8505 Training loss: 1.6804 9.2347 sec/batch\n",
      "Epoch 2/15  Iteration 796/8505 Training loss: 1.6801 9.3208 sec/batch\n",
      "Epoch 2/15  Iteration 797/8505 Training loss: 1.6797 9.3121 sec/batch\n",
      "Epoch 2/15  Iteration 798/8505 Training loss: 1.6792 9.3695 sec/batch\n",
      "Epoch 2/15  Iteration 799/8505 Training loss: 1.6789 9.4552 sec/batch\n",
      "Epoch 2/15  Iteration 800/8505 Training loss: 1.6785 9.5719 sec/batch\n",
      "Validation loss: 1.54311 Saving checkpoint!\n",
      "Epoch 2/15  Iteration 801/8505 Training loss: 1.6783 9.3134 sec/batch\n",
      "Epoch 2/15  Iteration 802/8505 Training loss: 1.6780 9.2573 sec/batch\n",
      "Epoch 2/15  Iteration 803/8505 Training loss: 1.6777 9.2974 sec/batch\n",
      "Epoch 2/15  Iteration 804/8505 Training loss: 1.6774 9.4299 sec/batch\n",
      "Epoch 2/15  Iteration 805/8505 Training loss: 1.6770 9.2444 sec/batch\n",
      "Epoch 2/15  Iteration 806/8505 Training loss: 1.6766 9.3463 sec/batch\n",
      "Epoch 2/15  Iteration 807/8505 Training loss: 1.6762 9.3081 sec/batch\n",
      "Epoch 2/15  Iteration 808/8505 Training loss: 1.6757 9.2595 sec/batch\n",
      "Epoch 2/15  Iteration 809/8505 Training loss: 1.6755 9.2351 sec/batch\n",
      "Epoch 2/15  Iteration 810/8505 Training loss: 1.6751 9.2000 sec/batch\n",
      "Epoch 2/15  Iteration 811/8505 Training loss: 1.6749 9.2464 sec/batch\n",
      "Epoch 2/15  Iteration 812/8505 Training loss: 1.6747 9.2414 sec/batch\n",
      "Epoch 2/15  Iteration 813/8505 Training loss: 1.6743 9.2512 sec/batch\n",
      "Epoch 2/15  Iteration 814/8505 Training loss: 1.6740 9.2489 sec/batch\n",
      "Epoch 2/15  Iteration 815/8505 Training loss: 1.6738 9.2438 sec/batch\n",
      "Epoch 2/15  Iteration 816/8505 Training loss: 1.6735 9.2650 sec/batch\n",
      "Epoch 2/15  Iteration 817/8505 Training loss: 1.6731 9.2570 sec/batch\n",
      "Epoch 2/15  Iteration 818/8505 Training loss: 1.6728 9.3537 sec/batch\n",
      "Epoch 2/15  Iteration 819/8505 Training loss: 1.6724 9.2624 sec/batch\n",
      "Epoch 2/15  Iteration 820/8505 Training loss: 1.6719 9.3542 sec/batch\n",
      "Epoch 2/15  Iteration 821/8505 Training loss: 1.6716 9.2561 sec/batch\n",
      "Epoch 2/15  Iteration 822/8505 Training loss: 1.6714 9.2506 sec/batch\n",
      "Epoch 2/15  Iteration 823/8505 Training loss: 1.6710 9.3966 sec/batch\n",
      "Epoch 2/15  Iteration 824/8505 Training loss: 1.6708 9.4005 sec/batch\n",
      "Epoch 2/15  Iteration 825/8505 Training loss: 1.6705 9.2164 sec/batch\n",
      "Epoch 2/15  Iteration 826/8505 Training loss: 1.6701 9.2977 sec/batch\n",
      "Epoch 2/15  Iteration 827/8505 Training loss: 1.6698 9.2054 sec/batch\n",
      "Epoch 2/15  Iteration 828/8505 Training loss: 1.6694 9.2026 sec/batch\n",
      "Epoch 2/15  Iteration 829/8505 Training loss: 1.6690 9.2620 sec/batch\n",
      "Epoch 2/15  Iteration 830/8505 Training loss: 1.6688 9.3189 sec/batch\n",
      "Epoch 2/15  Iteration 831/8505 Training loss: 1.6684 9.2765 sec/batch\n",
      "Epoch 2/15  Iteration 832/8505 Training loss: 1.6681 9.2662 sec/batch\n",
      "Epoch 2/15  Iteration 833/8505 Training loss: 1.6678 9.2480 sec/batch\n",
      "Epoch 2/15  Iteration 834/8505 Training loss: 1.6674 9.3297 sec/batch\n",
      "Epoch 2/15  Iteration 835/8505 Training loss: 1.6671 9.3464 sec/batch\n",
      "Epoch 2/15  Iteration 836/8505 Training loss: 1.6668 9.1573 sec/batch\n",
      "Epoch 2/15  Iteration 837/8505 Training loss: 1.6665 9.2234 sec/batch\n",
      "Epoch 2/15  Iteration 838/8505 Training loss: 1.6662 9.2755 sec/batch\n",
      "Epoch 2/15  Iteration 839/8505 Training loss: 1.6658 9.3473 sec/batch\n",
      "Epoch 2/15  Iteration 840/8505 Training loss: 1.6656 9.2657 sec/batch\n",
      "Epoch 2/15  Iteration 841/8505 Training loss: 1.6654 9.2836 sec/batch\n",
      "Epoch 2/15  Iteration 842/8505 Training loss: 1.6651 9.5535 sec/batch\n",
      "Epoch 2/15  Iteration 843/8505 Training loss: 1.6649 9.2814 sec/batch\n",
      "Epoch 2/15  Iteration 844/8505 Training loss: 1.6646 9.2278 sec/batch\n",
      "Epoch 2/15  Iteration 845/8505 Training loss: 1.6644 9.4159 sec/batch\n",
      "Epoch 2/15  Iteration 846/8505 Training loss: 1.6642 9.3513 sec/batch\n",
      "Epoch 2/15  Iteration 847/8505 Training loss: 1.6638 9.2729 sec/batch\n",
      "Epoch 2/15  Iteration 848/8505 Training loss: 1.6633 9.3190 sec/batch\n",
      "Epoch 2/15  Iteration 849/8505 Training loss: 1.6631 9.2688 sec/batch\n",
      "Epoch 2/15  Iteration 850/8505 Training loss: 1.6628 9.2398 sec/batch\n",
      "Epoch 2/15  Iteration 851/8505 Training loss: 1.6623 9.2809 sec/batch\n",
      "Epoch 2/15  Iteration 852/8505 Training loss: 1.6620 9.2994 sec/batch\n",
      "Epoch 2/15  Iteration 853/8505 Training loss: 1.6618 9.3083 sec/batch\n",
      "Epoch 2/15  Iteration 854/8505 Training loss: 1.6616 9.3148 sec/batch\n",
      "Epoch 2/15  Iteration 855/8505 Training loss: 1.6615 9.3122 sec/batch\n",
      "Epoch 2/15  Iteration 856/8505 Training loss: 1.6612 9.1529 sec/batch\n",
      "Epoch 2/15  Iteration 857/8505 Training loss: 1.6609 9.2407 sec/batch\n",
      "Epoch 2/15  Iteration 858/8505 Training loss: 1.6605 9.2872 sec/batch\n",
      "Epoch 2/15  Iteration 859/8505 Training loss: 1.6602 9.3413 sec/batch\n",
      "Epoch 2/15  Iteration 860/8505 Training loss: 1.6598 9.2059 sec/batch\n",
      "Epoch 2/15  Iteration 861/8505 Training loss: 1.6595 9.2264 sec/batch\n",
      "Epoch 2/15  Iteration 862/8505 Training loss: 1.6593 9.3387 sec/batch\n",
      "Epoch 2/15  Iteration 863/8505 Training loss: 1.6590 9.2953 sec/batch\n",
      "Epoch 2/15  Iteration 864/8505 Training loss: 1.6586 9.2556 sec/batch\n",
      "Epoch 2/15  Iteration 865/8505 Training loss: 1.6583 9.3048 sec/batch\n",
      "Epoch 2/15  Iteration 866/8505 Training loss: 1.6581 9.2396 sec/batch\n",
      "Epoch 2/15  Iteration 867/8505 Training loss: 1.6578 9.2486 sec/batch\n",
      "Epoch 2/15  Iteration 868/8505 Training loss: 1.6575 9.3354 sec/batch\n",
      "Epoch 2/15  Iteration 869/8505 Training loss: 1.6571 9.2116 sec/batch\n",
      "Epoch 2/15  Iteration 870/8505 Training loss: 1.6569 9.2402 sec/batch\n",
      "Epoch 2/15  Iteration 871/8505 Training loss: 1.6566 9.3048 sec/batch\n",
      "Epoch 2/15  Iteration 872/8505 Training loss: 1.6563 9.2641 sec/batch\n",
      "Epoch 2/15  Iteration 873/8505 Training loss: 1.6560 9.2154 sec/batch\n",
      "Epoch 2/15  Iteration 874/8505 Training loss: 1.6557 9.2645 sec/batch\n",
      "Epoch 2/15  Iteration 875/8505 Training loss: 1.6554 9.2469 sec/batch\n",
      "Epoch 2/15  Iteration 876/8505 Training loss: 1.6552 9.2249 sec/batch\n",
      "Epoch 2/15  Iteration 877/8505 Training loss: 1.6549 9.2888 sec/batch\n",
      "Epoch 2/15  Iteration 878/8505 Training loss: 1.6546 9.2376 sec/batch\n",
      "Epoch 2/15  Iteration 879/8505 Training loss: 1.6543 9.2836 sec/batch\n",
      "Epoch 2/15  Iteration 880/8505 Training loss: 1.6539 9.2013 sec/batch\n",
      "Epoch 2/15  Iteration 881/8505 Training loss: 1.6536 9.2739 sec/batch\n",
      "Epoch 2/15  Iteration 882/8505 Training loss: 1.6533 9.2556 sec/batch\n",
      "Epoch 2/15  Iteration 883/8505 Training loss: 1.6529 9.4815 sec/batch\n",
      "Epoch 2/15  Iteration 884/8505 Training loss: 1.6527 9.3270 sec/batch\n",
      "Epoch 2/15  Iteration 885/8505 Training loss: 1.6524 9.2854 sec/batch\n",
      "Epoch 2/15  Iteration 886/8505 Training loss: 1.6521 9.2936 sec/batch\n",
      "Epoch 2/15  Iteration 887/8505 Training loss: 1.6518 9.2972 sec/batch\n",
      "Epoch 2/15  Iteration 888/8505 Training loss: 1.6515 9.2406 sec/batch\n",
      "Epoch 2/15  Iteration 889/8505 Training loss: 1.6513 9.1842 sec/batch\n",
      "Epoch 2/15  Iteration 890/8505 Training loss: 1.6510 9.3328 sec/batch\n",
      "Epoch 2/15  Iteration 891/8505 Training loss: 1.6509 9.2736 sec/batch\n",
      "Epoch 2/15  Iteration 892/8505 Training loss: 1.6508 9.2271 sec/batch\n",
      "Epoch 2/15  Iteration 893/8505 Training loss: 1.6505 9.2315 sec/batch\n",
      "Epoch 2/15  Iteration 894/8505 Training loss: 1.6503 9.2928 sec/batch\n",
      "Epoch 2/15  Iteration 895/8505 Training loss: 1.6500 9.2941 sec/batch\n",
      "Epoch 2/15  Iteration 896/8505 Training loss: 1.6497 9.3095 sec/batch\n",
      "Epoch 2/15  Iteration 897/8505 Training loss: 1.6495 9.2755 sec/batch\n",
      "Epoch 2/15  Iteration 898/8505 Training loss: 1.6491 9.2541 sec/batch\n",
      "Epoch 2/15  Iteration 899/8505 Training loss: 1.6489 9.2657 sec/batch\n",
      "Epoch 2/15  Iteration 900/8505 Training loss: 1.6486 9.3159 sec/batch\n",
      "Epoch 2/15  Iteration 901/8505 Training loss: 1.6483 9.3167 sec/batch\n",
      "Epoch 2/15  Iteration 902/8505 Training loss: 1.6481 9.3097 sec/batch\n",
      "Epoch 2/15  Iteration 903/8505 Training loss: 1.6478 9.2377 sec/batch\n",
      "Epoch 2/15  Iteration 904/8505 Training loss: 1.6477 9.3512 sec/batch\n",
      "Epoch 2/15  Iteration 905/8505 Training loss: 1.6476 9.2969 sec/batch\n",
      "Epoch 2/15  Iteration 906/8505 Training loss: 1.6474 9.2519 sec/batch\n",
      "Epoch 2/15  Iteration 907/8505 Training loss: 1.6471 9.3285 sec/batch\n",
      "Epoch 2/15  Iteration 908/8505 Training loss: 1.6468 9.2894 sec/batch\n",
      "Epoch 2/15  Iteration 909/8505 Training loss: 1.6465 9.2647 sec/batch\n",
      "Epoch 2/15  Iteration 910/8505 Training loss: 1.6463 9.2822 sec/batch\n",
      "Epoch 2/15  Iteration 911/8505 Training loss: 1.6460 9.2390 sec/batch\n",
      "Epoch 2/15  Iteration 912/8505 Training loss: 1.6457 9.2743 sec/batch\n",
      "Epoch 2/15  Iteration 913/8505 Training loss: 1.6455 9.3156 sec/batch\n",
      "Epoch 2/15  Iteration 914/8505 Training loss: 1.6453 9.4625 sec/batch\n",
      "Epoch 2/15  Iteration 915/8505 Training loss: 1.6449 9.1931 sec/batch\n",
      "Epoch 2/15  Iteration 916/8505 Training loss: 1.6446 9.2708 sec/batch\n",
      "Epoch 2/15  Iteration 917/8505 Training loss: 1.6444 9.1920 sec/batch\n",
      "Epoch 2/15  Iteration 918/8505 Training loss: 1.6442 9.2583 sec/batch\n",
      "Epoch 2/15  Iteration 919/8505 Training loss: 1.6440 9.3590 sec/batch\n",
      "Epoch 2/15  Iteration 920/8505 Training loss: 1.6437 9.3042 sec/batch\n",
      "Epoch 2/15  Iteration 921/8505 Training loss: 1.6434 9.3147 sec/batch\n",
      "Epoch 2/15  Iteration 922/8505 Training loss: 1.6432 9.2852 sec/batch\n",
      "Epoch 2/15  Iteration 923/8505 Training loss: 1.6430 9.2679 sec/batch\n",
      "Epoch 2/15  Iteration 924/8505 Training loss: 1.6427 9.2560 sec/batch\n",
      "Epoch 2/15  Iteration 925/8505 Training loss: 1.6424 9.2559 sec/batch\n",
      "Epoch 2/15  Iteration 926/8505 Training loss: 1.6421 9.3930 sec/batch\n",
      "Epoch 2/15  Iteration 927/8505 Training loss: 1.6418 9.3712 sec/batch\n",
      "Epoch 2/15  Iteration 928/8505 Training loss: 1.6415 9.2130 sec/batch\n",
      "Epoch 2/15  Iteration 929/8505 Training loss: 1.6412 9.2866 sec/batch\n",
      "Epoch 2/15  Iteration 930/8505 Training loss: 1.6409 9.2294 sec/batch\n",
      "Epoch 2/15  Iteration 931/8505 Training loss: 1.6406 9.2391 sec/batch\n",
      "Epoch 2/15  Iteration 932/8505 Training loss: 1.6405 9.3064 sec/batch\n",
      "Epoch 2/15  Iteration 933/8505 Training loss: 1.6403 9.2813 sec/batch\n",
      "Epoch 2/15  Iteration 934/8505 Training loss: 1.6402 9.2737 sec/batch\n",
      "Epoch 2/15  Iteration 935/8505 Training loss: 1.6398 9.3181 sec/batch\n",
      "Epoch 2/15  Iteration 936/8505 Training loss: 1.6396 9.3133 sec/batch\n",
      "Epoch 2/15  Iteration 937/8505 Training loss: 1.6392 9.2713 sec/batch\n",
      "Epoch 2/15  Iteration 938/8505 Training loss: 1.6390 9.2990 sec/batch\n",
      "Epoch 2/15  Iteration 939/8505 Training loss: 1.6387 9.2189 sec/batch\n",
      "Epoch 2/15  Iteration 940/8505 Training loss: 1.6386 9.2405 sec/batch\n",
      "Epoch 2/15  Iteration 941/8505 Training loss: 1.6384 9.3685 sec/batch\n",
      "Epoch 2/15  Iteration 942/8505 Training loss: 1.6381 9.3027 sec/batch\n",
      "Epoch 2/15  Iteration 943/8505 Training loss: 1.6379 9.2656 sec/batch\n",
      "Epoch 2/15  Iteration 944/8505 Training loss: 1.6375 9.2609 sec/batch\n",
      "Epoch 2/15  Iteration 945/8505 Training loss: 1.6373 9.2924 sec/batch\n",
      "Epoch 2/15  Iteration 946/8505 Training loss: 1.6372 9.4796 sec/batch\n",
      "Epoch 2/15  Iteration 947/8505 Training loss: 1.6370 9.2451 sec/batch\n",
      "Epoch 2/15  Iteration 948/8505 Training loss: 1.6368 9.2767 sec/batch\n",
      "Epoch 2/15  Iteration 949/8505 Training loss: 1.6365 9.2522 sec/batch\n",
      "Epoch 2/15  Iteration 950/8505 Training loss: 1.6363 9.2971 sec/batch\n",
      "Epoch 2/15  Iteration 951/8505 Training loss: 1.6360 9.2132 sec/batch\n",
      "Epoch 2/15  Iteration 952/8505 Training loss: 1.6357 9.2938 sec/batch\n",
      "Epoch 2/15  Iteration 953/8505 Training loss: 1.6354 9.2118 sec/batch\n",
      "Epoch 2/15  Iteration 954/8505 Training loss: 1.6351 9.3018 sec/batch\n",
      "Epoch 2/15  Iteration 955/8505 Training loss: 1.6347 9.2947 sec/batch\n",
      "Epoch 2/15  Iteration 956/8505 Training loss: 1.6345 9.2903 sec/batch\n",
      "Epoch 2/15  Iteration 957/8505 Training loss: 1.6341 9.2654 sec/batch\n",
      "Epoch 2/15  Iteration 958/8505 Training loss: 1.6338 9.2432 sec/batch\n",
      "Epoch 2/15  Iteration 959/8505 Training loss: 1.6335 9.1981 sec/batch\n",
      "Epoch 2/15  Iteration 960/8505 Training loss: 1.6333 9.3035 sec/batch\n",
      "Epoch 2/15  Iteration 961/8505 Training loss: 1.6330 9.2487 sec/batch\n",
      "Epoch 2/15  Iteration 962/8505 Training loss: 1.6328 9.2297 sec/batch\n",
      "Epoch 2/15  Iteration 963/8505 Training loss: 1.6325 9.2245 sec/batch\n",
      "Epoch 2/15  Iteration 964/8505 Training loss: 1.6324 9.4790 sec/batch\n",
      "Epoch 2/15  Iteration 965/8505 Training loss: 1.6322 9.2673 sec/batch\n",
      "Epoch 2/15  Iteration 966/8505 Training loss: 1.6320 9.2772 sec/batch\n",
      "Epoch 2/15  Iteration 967/8505 Training loss: 1.6318 9.2482 sec/batch\n",
      "Epoch 2/15  Iteration 968/8505 Training loss: 1.6314 9.2983 sec/batch\n",
      "Epoch 2/15  Iteration 969/8505 Training loss: 1.6311 9.2485 sec/batch\n",
      "Epoch 2/15  Iteration 970/8505 Training loss: 1.6307 9.2339 sec/batch\n",
      "Epoch 2/15  Iteration 971/8505 Training loss: 1.6305 9.3818 sec/batch\n",
      "Epoch 2/15  Iteration 972/8505 Training loss: 1.6302 9.3565 sec/batch\n",
      "Epoch 2/15  Iteration 973/8505 Training loss: 1.6299 9.3055 sec/batch\n",
      "Epoch 2/15  Iteration 974/8505 Training loss: 1.6296 9.2847 sec/batch\n",
      "Epoch 2/15  Iteration 975/8505 Training loss: 1.6293 9.2321 sec/batch\n",
      "Epoch 2/15  Iteration 976/8505 Training loss: 1.6290 9.3362 sec/batch\n",
      "Epoch 2/15  Iteration 977/8505 Training loss: 1.6288 9.2573 sec/batch\n",
      "Epoch 2/15  Iteration 978/8505 Training loss: 1.6286 9.2613 sec/batch\n",
      "Epoch 2/15  Iteration 979/8505 Training loss: 1.6284 9.2091 sec/batch\n",
      "Epoch 2/15  Iteration 980/8505 Training loss: 1.6283 9.2586 sec/batch\n",
      "Epoch 2/15  Iteration 981/8505 Training loss: 1.6280 9.2865 sec/batch\n",
      "Epoch 2/15  Iteration 982/8505 Training loss: 1.6276 9.2930 sec/batch\n",
      "Epoch 2/15  Iteration 983/8505 Training loss: 1.6272 9.2452 sec/batch\n",
      "Epoch 2/15  Iteration 984/8505 Training loss: 1.6270 9.3560 sec/batch\n",
      "Epoch 2/15  Iteration 985/8505 Training loss: 1.6267 9.3070 sec/batch\n",
      "Epoch 2/15  Iteration 986/8505 Training loss: 1.6265 9.2537 sec/batch\n",
      "Epoch 2/15  Iteration 987/8505 Training loss: 1.6262 9.2607 sec/batch\n",
      "Epoch 2/15  Iteration 988/8505 Training loss: 1.6260 9.3310 sec/batch\n",
      "Epoch 2/15  Iteration 989/8505 Training loss: 1.6258 9.3269 sec/batch\n",
      "Epoch 2/15  Iteration 990/8505 Training loss: 1.6255 9.2774 sec/batch\n",
      "Epoch 2/15  Iteration 991/8505 Training loss: 1.6253 9.2537 sec/batch\n",
      "Epoch 2/15  Iteration 992/8505 Training loss: 1.6250 9.3094 sec/batch\n",
      "Epoch 2/15  Iteration 993/8505 Training loss: 1.6248 9.2951 sec/batch\n",
      "Epoch 2/15  Iteration 994/8505 Training loss: 1.6247 9.3332 sec/batch\n",
      "Epoch 2/15  Iteration 995/8505 Training loss: 1.6244 9.2171 sec/batch\n",
      "Epoch 2/15  Iteration 996/8505 Training loss: 1.6242 9.2612 sec/batch\n",
      "Epoch 2/15  Iteration 997/8505 Training loss: 1.6239 9.2817 sec/batch\n",
      "Epoch 2/15  Iteration 998/8505 Training loss: 1.6237 9.2747 sec/batch\n",
      "Epoch 2/15  Iteration 999/8505 Training loss: 1.6234 9.3686 sec/batch\n",
      "Epoch 2/15  Iteration 1000/8505 Training loss: 1.6231 9.3475 sec/batch\n",
      "Validation loss: 1.45326 Saving checkpoint!\n",
      "Epoch 2/15  Iteration 1001/8505 Training loss: 1.6231 9.5110 sec/batch\n",
      "Epoch 2/15  Iteration 1002/8505 Training loss: 1.6228 9.3651 sec/batch\n",
      "Epoch 2/15  Iteration 1003/8505 Training loss: 1.6225 9.6453 sec/batch\n",
      "Epoch 2/15  Iteration 1004/8505 Training loss: 1.6222 9.3941 sec/batch\n",
      "Epoch 2/15  Iteration 1005/8505 Training loss: 1.6219 9.3868 sec/batch\n",
      "Epoch 2/15  Iteration 1006/8505 Training loss: 1.6216 9.3495 sec/batch\n",
      "Epoch 2/15  Iteration 1007/8505 Training loss: 1.6215 9.2665 sec/batch\n",
      "Epoch 2/15  Iteration 1008/8505 Training loss: 1.6212 9.2370 sec/batch\n",
      "Epoch 2/15  Iteration 1009/8505 Training loss: 1.6209 9.3834 sec/batch\n",
      "Epoch 2/15  Iteration 1010/8505 Training loss: 1.6207 9.4800 sec/batch\n",
      "Epoch 2/15  Iteration 1011/8505 Training loss: 1.6204 9.4783 sec/batch\n",
      "Epoch 2/15  Iteration 1012/8505 Training loss: 1.6202 9.3768 sec/batch\n",
      "Epoch 2/15  Iteration 1013/8505 Training loss: 1.6199 9.2954 sec/batch\n",
      "Epoch 2/15  Iteration 1014/8505 Training loss: 1.6197 9.3740 sec/batch\n",
      "Epoch 2/15  Iteration 1015/8505 Training loss: 1.6194 9.3955 sec/batch\n",
      "Epoch 2/15  Iteration 1016/8505 Training loss: 1.6191 9.3558 sec/batch\n",
      "Epoch 2/15  Iteration 1017/8505 Training loss: 1.6189 9.6603 sec/batch\n",
      "Epoch 2/15  Iteration 1018/8505 Training loss: 1.6186 9.4749 sec/batch\n",
      "Epoch 2/15  Iteration 1019/8505 Training loss: 1.6183 9.3657 sec/batch\n",
      "Epoch 2/15  Iteration 1020/8505 Training loss: 1.6181 9.3022 sec/batch\n",
      "Epoch 2/15  Iteration 1021/8505 Training loss: 1.6179 9.3338 sec/batch\n",
      "Epoch 2/15  Iteration 1022/8505 Training loss: 1.6176 9.4689 sec/batch\n",
      "Epoch 2/15  Iteration 1023/8505 Training loss: 1.6174 9.3548 sec/batch\n",
      "Epoch 2/15  Iteration 1024/8505 Training loss: 1.6171 9.3834 sec/batch\n",
      "Epoch 2/15  Iteration 1025/8505 Training loss: 1.6169 9.4691 sec/batch\n",
      "Epoch 2/15  Iteration 1026/8505 Training loss: 1.6166 9.3802 sec/batch\n",
      "Epoch 2/15  Iteration 1027/8505 Training loss: 1.6164 9.2975 sec/batch\n",
      "Epoch 2/15  Iteration 1028/8505 Training loss: 1.6161 9.2554 sec/batch\n",
      "Epoch 2/15  Iteration 1029/8505 Training loss: 1.6158 9.3217 sec/batch\n",
      "Epoch 2/15  Iteration 1030/8505 Training loss: 1.6156 9.2637 sec/batch\n",
      "Epoch 2/15  Iteration 1031/8505 Training loss: 1.6153 9.3293 sec/batch\n",
      "Epoch 2/15  Iteration 1032/8505 Training loss: 1.6152 9.2581 sec/batch\n",
      "Epoch 2/15  Iteration 1033/8505 Training loss: 1.6150 9.3634 sec/batch\n",
      "Epoch 2/15  Iteration 1034/8505 Training loss: 1.6147 9.2575 sec/batch\n",
      "Epoch 2/15  Iteration 1035/8505 Training loss: 1.6145 9.2055 sec/batch\n",
      "Epoch 2/15  Iteration 1036/8505 Training loss: 1.6142 9.2605 sec/batch\n",
      "Epoch 2/15  Iteration 1037/8505 Training loss: 1.6139 9.3159 sec/batch\n",
      "Epoch 2/15  Iteration 1038/8505 Training loss: 1.6137 9.2737 sec/batch\n",
      "Epoch 2/15  Iteration 1039/8505 Training loss: 1.6134 9.2757 sec/batch\n",
      "Epoch 2/15  Iteration 1040/8505 Training loss: 1.6131 9.2349 sec/batch\n",
      "Epoch 2/15  Iteration 1041/8505 Training loss: 1.6129 9.3319 sec/batch\n",
      "Epoch 2/15  Iteration 1042/8505 Training loss: 1.6127 9.2792 sec/batch\n",
      "Epoch 2/15  Iteration 1043/8505 Training loss: 1.6125 9.2093 sec/batch\n",
      "Epoch 2/15  Iteration 1044/8505 Training loss: 1.6123 9.2259 sec/batch\n",
      "Epoch 2/15  Iteration 1045/8505 Training loss: 1.6120 9.2872 sec/batch\n",
      "Epoch 2/15  Iteration 1046/8505 Training loss: 1.6117 9.2251 sec/batch\n",
      "Epoch 2/15  Iteration 1047/8505 Training loss: 1.6114 9.1871 sec/batch\n",
      "Epoch 2/15  Iteration 1048/8505 Training loss: 1.6112 9.2172 sec/batch\n",
      "Epoch 2/15  Iteration 1049/8505 Training loss: 1.6109 9.3574 sec/batch\n",
      "Epoch 2/15  Iteration 1050/8505 Training loss: 1.6107 9.2596 sec/batch\n",
      "Epoch 2/15  Iteration 1051/8505 Training loss: 1.6105 9.2353 sec/batch\n",
      "Epoch 2/15  Iteration 1052/8505 Training loss: 1.6102 9.2078 sec/batch\n",
      "Epoch 2/15  Iteration 1053/8505 Training loss: 1.6100 9.2194 sec/batch\n",
      "Epoch 2/15  Iteration 1054/8505 Training loss: 1.6097 9.3074 sec/batch\n",
      "Epoch 2/15  Iteration 1055/8505 Training loss: 1.6095 9.2904 sec/batch\n",
      "Epoch 2/15  Iteration 1056/8505 Training loss: 1.6092 9.1841 sec/batch\n",
      "Epoch 2/15  Iteration 1057/8505 Training loss: 1.6090 9.2534 sec/batch\n",
      "Epoch 2/15  Iteration 1058/8505 Training loss: 1.6087 9.1709 sec/batch\n",
      "Epoch 2/15  Iteration 1059/8505 Training loss: 1.6085 9.2692 sec/batch\n",
      "Epoch 2/15  Iteration 1060/8505 Training loss: 1.6082 9.2501 sec/batch\n",
      "Epoch 2/15  Iteration 1061/8505 Training loss: 1.6079 9.2587 sec/batch\n",
      "Epoch 2/15  Iteration 1062/8505 Training loss: 1.6077 9.3158 sec/batch\n",
      "Epoch 2/15  Iteration 1063/8505 Training loss: 1.6074 9.2253 sec/batch\n",
      "Epoch 2/15  Iteration 1064/8505 Training loss: 1.6072 9.2533 sec/batch\n",
      "Epoch 2/15  Iteration 1065/8505 Training loss: 1.6070 9.2486 sec/batch\n",
      "Epoch 2/15  Iteration 1066/8505 Training loss: 1.6066 9.1668 sec/batch\n",
      "Epoch 2/15  Iteration 1067/8505 Training loss: 1.6063 9.3208 sec/batch\n",
      "Epoch 2/15  Iteration 1068/8505 Training loss: 1.6060 9.3190 sec/batch\n",
      "Epoch 2/15  Iteration 1069/8505 Training loss: 1.6057 9.2406 sec/batch\n",
      "Epoch 2/15  Iteration 1070/8505 Training loss: 1.6054 9.3409 sec/batch\n",
      "Epoch 2/15  Iteration 1071/8505 Training loss: 1.6051 9.2231 sec/batch\n",
      "Epoch 2/15  Iteration 1072/8505 Training loss: 1.6049 9.3720 sec/batch\n",
      "Epoch 2/15  Iteration 1073/8505 Training loss: 1.6046 9.2218 sec/batch\n",
      "Epoch 2/15  Iteration 1074/8505 Training loss: 1.6044 9.3257 sec/batch\n",
      "Epoch 2/15  Iteration 1075/8505 Training loss: 1.6041 9.2645 sec/batch\n",
      "Epoch 2/15  Iteration 1076/8505 Training loss: 1.6038 9.1905 sec/batch\n",
      "Epoch 2/15  Iteration 1077/8505 Training loss: 1.6035 9.3019 sec/batch\n",
      "Epoch 2/15  Iteration 1078/8505 Training loss: 1.6032 9.2561 sec/batch\n",
      "Epoch 2/15  Iteration 1079/8505 Training loss: 1.6029 9.2316 sec/batch\n",
      "Epoch 2/15  Iteration 1080/8505 Training loss: 1.6026 9.2238 sec/batch\n",
      "Epoch 2/15  Iteration 1081/8505 Training loss: 1.6024 9.3082 sec/batch\n",
      "Epoch 2/15  Iteration 1082/8505 Training loss: 1.6021 9.2078 sec/batch\n",
      "Epoch 2/15  Iteration 1083/8505 Training loss: 1.6019 9.3406 sec/batch\n",
      "Epoch 2/15  Iteration 1084/8505 Training loss: 1.6016 9.2546 sec/batch\n",
      "Epoch 2/15  Iteration 1085/8505 Training loss: 1.6013 9.2722 sec/batch\n",
      "Epoch 2/15  Iteration 1086/8505 Training loss: 1.6010 9.2831 sec/batch\n",
      "Epoch 2/15  Iteration 1087/8505 Training loss: 1.6008 9.2273 sec/batch\n",
      "Epoch 2/15  Iteration 1088/8505 Training loss: 1.6005 9.3115 sec/batch\n",
      "Epoch 2/15  Iteration 1089/8505 Training loss: 1.6002 9.3656 sec/batch\n",
      "Epoch 2/15  Iteration 1090/8505 Training loss: 1.6000 9.9603 sec/batch\n",
      "Epoch 2/15  Iteration 1091/8505 Training loss: 1.5997 9.5055 sec/batch\n",
      "Epoch 2/15  Iteration 1092/8505 Training loss: 1.5994 9.2711 sec/batch\n",
      "Epoch 2/15  Iteration 1093/8505 Training loss: 1.5991 9.2721 sec/batch\n",
      "Epoch 2/15  Iteration 1094/8505 Training loss: 1.5987 9.3144 sec/batch\n",
      "Epoch 2/15  Iteration 1095/8505 Training loss: 1.5985 9.1447 sec/batch\n",
      "Epoch 2/15  Iteration 1096/8505 Training loss: 1.5983 9.3453 sec/batch\n",
      "Epoch 2/15  Iteration 1097/8505 Training loss: 1.5981 9.3036 sec/batch\n",
      "Epoch 2/15  Iteration 1098/8505 Training loss: 1.5978 9.2731 sec/batch\n",
      "Epoch 2/15  Iteration 1099/8505 Training loss: 1.5975 9.3021 sec/batch\n",
      "Epoch 2/15  Iteration 1100/8505 Training loss: 1.5973 9.2974 sec/batch\n",
      "Epoch 2/15  Iteration 1101/8505 Training loss: 1.5970 9.2608 sec/batch\n",
      "Epoch 2/15  Iteration 1102/8505 Training loss: 1.5967 9.2575 sec/batch\n",
      "Epoch 2/15  Iteration 1103/8505 Training loss: 1.5965 9.2409 sec/batch\n",
      "Epoch 2/15  Iteration 1104/8505 Training loss: 1.5962 9.3361 sec/batch\n",
      "Epoch 2/15  Iteration 1105/8505 Training loss: 1.5959 9.3440 sec/batch\n",
      "Epoch 2/15  Iteration 1106/8505 Training loss: 1.5957 9.2892 sec/batch\n",
      "Epoch 2/15  Iteration 1107/8505 Training loss: 1.5955 9.2898 sec/batch\n",
      "Epoch 2/15  Iteration 1108/8505 Training loss: 1.5953 9.3270 sec/batch\n",
      "Epoch 2/15  Iteration 1109/8505 Training loss: 1.5950 9.2866 sec/batch\n",
      "Epoch 2/15  Iteration 1110/8505 Training loss: 1.5948 9.2542 sec/batch\n",
      "Epoch 2/15  Iteration 1111/8505 Training loss: 1.5946 9.3375 sec/batch\n",
      "Epoch 2/15  Iteration 1112/8505 Training loss: 1.5944 9.2016 sec/batch\n",
      "Epoch 2/15  Iteration 1113/8505 Training loss: 1.5942 9.2488 sec/batch\n",
      "Epoch 2/15  Iteration 1114/8505 Training loss: 1.5940 9.2600 sec/batch\n",
      "Epoch 2/15  Iteration 1115/8505 Training loss: 1.5938 9.2678 sec/batch\n",
      "Epoch 2/15  Iteration 1116/8505 Training loss: 1.5935 9.2122 sec/batch\n",
      "Epoch 2/15  Iteration 1117/8505 Training loss: 1.5932 9.2670 sec/batch\n",
      "Epoch 2/15  Iteration 1118/8505 Training loss: 1.5931 9.2670 sec/batch\n",
      "Epoch 2/15  Iteration 1119/8505 Training loss: 1.5929 9.2550 sec/batch\n",
      "Epoch 2/15  Iteration 1120/8505 Training loss: 1.5927 9.2495 sec/batch\n",
      "Epoch 2/15  Iteration 1121/8505 Training loss: 1.5926 9.2912 sec/batch\n",
      "Epoch 2/15  Iteration 1122/8505 Training loss: 1.5924 9.2249 sec/batch\n",
      "Epoch 2/15  Iteration 1123/8505 Training loss: 1.5921 9.2863 sec/batch\n",
      "Epoch 2/15  Iteration 1124/8505 Training loss: 1.5920 9.2793 sec/batch\n",
      "Epoch 2/15  Iteration 1125/8505 Training loss: 1.5918 9.1869 sec/batch\n",
      "Epoch 2/15  Iteration 1126/8505 Training loss: 1.5917 9.3464 sec/batch\n",
      "Epoch 2/15  Iteration 1127/8505 Training loss: 1.5915 9.2131 sec/batch\n",
      "Epoch 2/15  Iteration 1128/8505 Training loss: 1.5913 9.2466 sec/batch\n",
      "Epoch 2/15  Iteration 1129/8505 Training loss: 1.5911 9.2503 sec/batch\n",
      "Epoch 2/15  Iteration 1130/8505 Training loss: 1.5908 9.3146 sec/batch\n",
      "Epoch 2/15  Iteration 1131/8505 Training loss: 1.5905 9.3396 sec/batch\n",
      "Epoch 2/15  Iteration 1132/8505 Training loss: 1.5902 9.3589 sec/batch\n",
      "Epoch 2/15  Iteration 1133/8505 Training loss: 1.5900 9.2257 sec/batch\n",
      "Epoch 2/15  Iteration 1134/8505 Training loss: 1.5897 9.2605 sec/batch\n",
      "Epoch 3/15  Iteration 1135/8505 Training loss: 1.6176 9.2095 sec/batch\n",
      "Epoch 3/15  Iteration 1136/8505 Training loss: 1.5526 9.3334 sec/batch\n",
      "Epoch 3/15  Iteration 1137/8505 Training loss: 1.5385 9.3143 sec/batch\n",
      "Epoch 3/15  Iteration 1138/8505 Training loss: 1.5291 9.2737 sec/batch\n",
      "Epoch 3/15  Iteration 1139/8505 Training loss: 1.5252 9.3021 sec/batch\n",
      "Epoch 3/15  Iteration 1140/8505 Training loss: 1.5188 9.2659 sec/batch\n",
      "Epoch 3/15  Iteration 1141/8505 Training loss: 1.5106 9.3107 sec/batch\n",
      "Epoch 3/15  Iteration 1142/8505 Training loss: 1.5055 9.3449 sec/batch\n",
      "Epoch 3/15  Iteration 1143/8505 Training loss: 1.5039 9.2650 sec/batch\n",
      "Epoch 3/15  Iteration 1144/8505 Training loss: 1.4927 9.2320 sec/batch\n",
      "Epoch 3/15  Iteration 1145/8505 Training loss: 1.4952 9.2431 sec/batch\n",
      "Epoch 3/15  Iteration 1146/8505 Training loss: 1.4945 9.2390 sec/batch\n",
      "Epoch 3/15  Iteration 1147/8505 Training loss: 1.4920 9.2713 sec/batch\n",
      "Epoch 3/15  Iteration 1148/8505 Training loss: 1.4925 9.2814 sec/batch\n",
      "Epoch 3/15  Iteration 1149/8505 Training loss: 1.4901 9.3205 sec/batch\n",
      "Epoch 3/15  Iteration 1150/8505 Training loss: 1.4885 9.4733 sec/batch\n",
      "Epoch 3/15  Iteration 1151/8505 Training loss: 1.4870 9.2903 sec/batch\n",
      "Epoch 3/15  Iteration 1152/8505 Training loss: 1.4861 9.2655 sec/batch\n",
      "Epoch 3/15  Iteration 1153/8505 Training loss: 1.4845 9.2542 sec/batch\n",
      "Epoch 3/15  Iteration 1154/8505 Training loss: 1.4846 9.2679 sec/batch\n",
      "Epoch 3/15  Iteration 1155/8505 Training loss: 1.4845 9.1929 sec/batch\n",
      "Epoch 3/15  Iteration 1156/8505 Training loss: 1.4825 9.3115 sec/batch\n",
      "Epoch 3/15  Iteration 1157/8505 Training loss: 1.4809 9.2212 sec/batch\n",
      "Epoch 3/15  Iteration 1158/8505 Training loss: 1.4814 9.2605 sec/batch\n",
      "Epoch 3/15  Iteration 1159/8505 Training loss: 1.4812 9.3665 sec/batch\n",
      "Epoch 3/15  Iteration 1160/8505 Training loss: 1.4803 9.2338 sec/batch\n",
      "Epoch 3/15  Iteration 1161/8505 Training loss: 1.4793 9.2355 sec/batch\n",
      "Epoch 3/15  Iteration 1162/8505 Training loss: 1.4771 9.2583 sec/batch\n",
      "Epoch 3/15  Iteration 1163/8505 Training loss: 1.4771 9.2248 sec/batch\n",
      "Epoch 3/15  Iteration 1164/8505 Training loss: 1.4767 9.2884 sec/batch\n",
      "Epoch 3/15  Iteration 1165/8505 Training loss: 1.4751 9.1749 sec/batch\n",
      "Epoch 3/15  Iteration 1166/8505 Training loss: 1.4740 9.2605 sec/batch\n",
      "Epoch 3/15  Iteration 1167/8505 Training loss: 1.4740 9.2158 sec/batch\n",
      "Epoch 3/15  Iteration 1168/8505 Training loss: 1.4743 9.1878 sec/batch\n",
      "Epoch 3/15  Iteration 1169/8505 Training loss: 1.4740 9.2262 sec/batch\n",
      "Epoch 3/15  Iteration 1170/8505 Training loss: 1.4733 9.2671 sec/batch\n",
      "Epoch 3/15  Iteration 1171/8505 Training loss: 1.4738 9.3019 sec/batch\n",
      "Epoch 3/15  Iteration 1172/8505 Training loss: 1.4738 9.2411 sec/batch\n",
      "Epoch 3/15  Iteration 1173/8505 Training loss: 1.4735 9.2714 sec/batch\n",
      "Epoch 3/15  Iteration 1174/8505 Training loss: 1.4724 9.3152 sec/batch\n",
      "Epoch 3/15  Iteration 1175/8505 Training loss: 1.4716 9.2682 sec/batch\n",
      "Epoch 3/15  Iteration 1176/8505 Training loss: 1.4713 9.2325 sec/batch\n",
      "Epoch 3/15  Iteration 1177/8505 Training loss: 1.4719 9.2204 sec/batch\n",
      "Epoch 3/15  Iteration 1178/8505 Training loss: 1.4711 9.2288 sec/batch\n",
      "Epoch 3/15  Iteration 1179/8505 Training loss: 1.4709 9.3444 sec/batch\n",
      "Epoch 3/15  Iteration 1180/8505 Training loss: 1.4708 9.2774 sec/batch\n",
      "Epoch 3/15  Iteration 1181/8505 Training loss: 1.4710 9.2450 sec/batch\n",
      "Epoch 3/15  Iteration 1182/8505 Training loss: 1.4707 9.2696 sec/batch\n",
      "Epoch 3/15  Iteration 1183/8505 Training loss: 1.4699 9.2834 sec/batch\n",
      "Epoch 3/15  Iteration 1184/8505 Training loss: 1.4686 9.2731 sec/batch\n",
      "Epoch 3/15  Iteration 1185/8505 Training loss: 1.4681 9.2947 sec/batch\n",
      "Epoch 3/15  Iteration 1186/8505 Training loss: 1.4683 9.2825 sec/batch\n",
      "Epoch 3/15  Iteration 1187/8505 Training loss: 1.4676 9.2522 sec/batch\n",
      "Epoch 3/15  Iteration 1188/8505 Training loss: 1.4679 9.3507 sec/batch\n",
      "Epoch 3/15  Iteration 1189/8505 Training loss: 1.4676 9.2581 sec/batch\n",
      "Epoch 3/15  Iteration 1190/8505 Training loss: 1.4675 9.3632 sec/batch\n",
      "Epoch 3/15  Iteration 1191/8505 Training loss: 1.4670 9.2837 sec/batch\n",
      "Epoch 3/15  Iteration 1192/8505 Training loss: 1.4669 9.2117 sec/batch\n",
      "Epoch 3/15  Iteration 1193/8505 Training loss: 1.4676 9.3131 sec/batch\n",
      "Epoch 3/15  Iteration 1194/8505 Training loss: 1.4681 9.2606 sec/batch\n",
      "Epoch 3/15  Iteration 1195/8505 Training loss: 1.4680 9.3309 sec/batch\n",
      "Epoch 3/15  Iteration 1196/8505 Training loss: 1.4678 9.3226 sec/batch\n",
      "Epoch 3/15  Iteration 1197/8505 Training loss: 1.4674 9.2787 sec/batch\n",
      "Epoch 3/15  Iteration 1198/8505 Training loss: 1.4671 9.2289 sec/batch\n",
      "Epoch 3/15  Iteration 1199/8505 Training loss: 1.4665 9.2209 sec/batch\n",
      "Epoch 3/15  Iteration 1200/8505 Training loss: 1.4664 9.2523 sec/batch\n",
      "Validation loss: 1.39422 Saving checkpoint!\n",
      "Epoch 3/15  Iteration 1201/8505 Training loss: 1.4677 10.0125 sec/batch\n",
      "Epoch 3/15  Iteration 1202/8505 Training loss: 1.4679 9.4248 sec/batch\n",
      "Epoch 3/15  Iteration 1203/8505 Training loss: 1.4679 9.3069 sec/batch\n",
      "Epoch 3/15  Iteration 1204/8505 Training loss: 1.4676 9.2322 sec/batch\n",
      "Epoch 3/15  Iteration 1205/8505 Training loss: 1.4672 9.3284 sec/batch\n",
      "Epoch 3/15  Iteration 1206/8505 Training loss: 1.4670 9.2471 sec/batch\n",
      "Epoch 3/15  Iteration 1207/8505 Training loss: 1.4670 9.3119 sec/batch\n",
      "Epoch 3/15  Iteration 1208/8505 Training loss: 1.4672 9.3374 sec/batch\n",
      "Epoch 3/15  Iteration 1209/8505 Training loss: 1.4671 9.2939 sec/batch\n",
      "Epoch 3/15  Iteration 1210/8505 Training loss: 1.4669 9.3971 sec/batch\n",
      "Epoch 3/15  Iteration 1211/8505 Training loss: 1.4667 9.2905 sec/batch\n",
      "Epoch 3/15  Iteration 1212/8505 Training loss: 1.4664 9.2376 sec/batch\n",
      "Epoch 3/15  Iteration 1213/8505 Training loss: 1.4665 9.1776 sec/batch\n",
      "Epoch 3/15  Iteration 1214/8505 Training loss: 1.4658 9.3631 sec/batch\n",
      "Epoch 3/15  Iteration 1215/8505 Training loss: 1.4657 9.3249 sec/batch\n",
      "Epoch 3/15  Iteration 1216/8505 Training loss: 1.4651 9.2499 sec/batch\n",
      "Epoch 3/15  Iteration 1217/8505 Training loss: 1.4649 9.2791 sec/batch\n",
      "Epoch 3/15  Iteration 1218/8505 Training loss: 1.4644 9.1565 sec/batch\n",
      "Epoch 3/15  Iteration 1219/8505 Training loss: 1.4645 9.2297 sec/batch\n",
      "Epoch 3/15  Iteration 1220/8505 Training loss: 1.4643 9.2678 sec/batch\n",
      "Epoch 3/15  Iteration 1221/8505 Training loss: 1.4644 9.2911 sec/batch\n",
      "Epoch 3/15  Iteration 1222/8505 Training loss: 1.4643 9.2915 sec/batch\n",
      "Epoch 3/15  Iteration 1223/8505 Training loss: 1.4638 9.2331 sec/batch\n",
      "Epoch 3/15  Iteration 1224/8505 Training loss: 1.4635 9.2100 sec/batch\n",
      "Epoch 3/15  Iteration 1225/8505 Training loss: 1.4631 9.2788 sec/batch\n",
      "Epoch 3/15  Iteration 1226/8505 Training loss: 1.4629 9.2716 sec/batch\n",
      "Epoch 3/15  Iteration 1227/8505 Training loss: 1.4631 9.2899 sec/batch\n",
      "Epoch 3/15  Iteration 1228/8505 Training loss: 1.4632 9.2951 sec/batch\n",
      "Epoch 3/15  Iteration 1229/8505 Training loss: 1.4632 9.2745 sec/batch\n",
      "Epoch 3/15  Iteration 1230/8505 Training loss: 1.4636 9.2645 sec/batch\n",
      "Epoch 3/15  Iteration 1231/8505 Training loss: 1.4633 9.2841 sec/batch\n",
      "Epoch 3/15  Iteration 1232/8505 Training loss: 1.4630 9.2895 sec/batch\n",
      "Epoch 3/15  Iteration 1233/8505 Training loss: 1.4628 9.2039 sec/batch\n",
      "Epoch 3/15  Iteration 1234/8505 Training loss: 1.4628 9.2843 sec/batch\n",
      "Epoch 3/15  Iteration 1235/8505 Training loss: 1.4626 9.3896 sec/batch\n",
      "Epoch 3/15  Iteration 1236/8505 Training loss: 1.4625 9.2376 sec/batch\n",
      "Epoch 3/15  Iteration 1237/8505 Training loss: 1.4626 9.2806 sec/batch\n",
      "Epoch 3/15  Iteration 1238/8505 Training loss: 1.4623 9.2828 sec/batch\n",
      "Epoch 3/15  Iteration 1239/8505 Training loss: 1.4625 9.2554 sec/batch\n",
      "Epoch 3/15  Iteration 1240/8505 Training loss: 1.4620 9.2846 sec/batch\n",
      "Epoch 3/15  Iteration 1241/8505 Training loss: 1.4618 9.2723 sec/batch\n",
      "Epoch 3/15  Iteration 1242/8505 Training loss: 1.4615 9.2543 sec/batch\n",
      "Epoch 3/15  Iteration 1243/8505 Training loss: 1.4611 9.2388 sec/batch\n",
      "Epoch 3/15  Iteration 1244/8505 Training loss: 1.4610 9.1961 sec/batch\n",
      "Epoch 3/15  Iteration 1245/8505 Training loss: 1.4608 9.3482 sec/batch\n",
      "Epoch 3/15  Iteration 1246/8505 Training loss: 1.4606 9.2618 sec/batch\n",
      "Epoch 3/15  Iteration 1247/8505 Training loss: 1.4605 9.2096 sec/batch\n",
      "Epoch 3/15  Iteration 1248/8505 Training loss: 1.4599 9.2531 sec/batch\n",
      "Epoch 3/15  Iteration 1249/8505 Training loss: 1.4596 9.2274 sec/batch\n",
      "Epoch 3/15  Iteration 1250/8505 Training loss: 1.4591 9.2489 sec/batch\n",
      "Epoch 3/15  Iteration 1251/8505 Training loss: 1.4587 9.2902 sec/batch\n",
      "Epoch 3/15  Iteration 1252/8505 Training loss: 1.4586 9.1899 sec/batch\n",
      "Epoch 3/15  Iteration 1253/8505 Training loss: 1.4584 9.2430 sec/batch\n",
      "Epoch 3/15  Iteration 1254/8505 Training loss: 1.4584 9.2545 sec/batch\n",
      "Epoch 3/15  Iteration 1255/8505 Training loss: 1.4585 9.2346 sec/batch\n",
      "Epoch 3/15  Iteration 1256/8505 Training loss: 1.4585 9.2140 sec/batch\n",
      "Epoch 3/15  Iteration 1257/8505 Training loss: 1.4585 9.2695 sec/batch\n",
      "Epoch 3/15  Iteration 1258/8505 Training loss: 1.4584 9.2786 sec/batch\n",
      "Epoch 3/15  Iteration 1259/8505 Training loss: 1.4582 9.2896 sec/batch\n",
      "Epoch 3/15  Iteration 1260/8505 Training loss: 1.4580 9.2855 sec/batch\n",
      "Epoch 3/15  Iteration 1261/8505 Training loss: 1.4578 9.5202 sec/batch\n",
      "Epoch 3/15  Iteration 1262/8505 Training loss: 1.4577 9.3530 sec/batch\n",
      "Epoch 3/15  Iteration 1263/8505 Training loss: 1.4578 9.2873 sec/batch\n",
      "Epoch 3/15  Iteration 1264/8505 Training loss: 1.4576 9.2586 sec/batch\n",
      "Epoch 3/15  Iteration 1265/8505 Training loss: 1.4575 9.2513 sec/batch\n",
      "Epoch 3/15  Iteration 1266/8505 Training loss: 1.4575 9.3009 sec/batch\n",
      "Epoch 3/15  Iteration 1267/8505 Training loss: 1.4578 9.2474 sec/batch\n",
      "Epoch 3/15  Iteration 1268/8505 Training loss: 1.4581 9.2597 sec/batch\n",
      "Epoch 3/15  Iteration 1269/8505 Training loss: 1.4584 9.2988 sec/batch\n",
      "Epoch 3/15  Iteration 1270/8505 Training loss: 1.4585 9.5114 sec/batch\n",
      "Epoch 3/15  Iteration 1271/8505 Training loss: 1.4584 9.2685 sec/batch\n",
      "Epoch 3/15  Iteration 1272/8505 Training loss: 1.4583 9.1628 sec/batch\n",
      "Epoch 3/15  Iteration 1273/8505 Training loss: 1.4579 9.1856 sec/batch\n",
      "Epoch 3/15  Iteration 1274/8505 Training loss: 1.4579 9.3080 sec/batch\n",
      "Epoch 3/15  Iteration 1275/8505 Training loss: 1.4576 9.2830 sec/batch\n",
      "Epoch 3/15  Iteration 1276/8505 Training loss: 1.4571 9.3150 sec/batch\n",
      "Epoch 3/15  Iteration 1277/8505 Training loss: 1.4568 9.2898 sec/batch\n",
      "Epoch 3/15  Iteration 1278/8505 Training loss: 1.4566 9.2482 sec/batch\n",
      "Epoch 3/15  Iteration 1279/8505 Training loss: 1.4564 9.2799 sec/batch\n",
      "Epoch 3/15  Iteration 1280/8505 Training loss: 1.4564 9.1930 sec/batch\n",
      "Epoch 3/15  Iteration 1281/8505 Training loss: 1.4562 9.2456 sec/batch\n",
      "Epoch 3/15  Iteration 1282/8505 Training loss: 1.4560 9.2833 sec/batch\n",
      "Epoch 3/15  Iteration 1283/8505 Training loss: 1.4558 9.2728 sec/batch\n",
      "Epoch 3/15  Iteration 1284/8505 Training loss: 1.4555 9.2255 sec/batch\n",
      "Epoch 3/15  Iteration 1285/8505 Training loss: 1.4553 9.2573 sec/batch\n",
      "Epoch 3/15  Iteration 1286/8505 Training loss: 1.4549 9.3014 sec/batch\n",
      "Epoch 3/15  Iteration 1287/8505 Training loss: 1.4545 9.2176 sec/batch\n",
      "Epoch 3/15  Iteration 1288/8505 Training loss: 1.4545 9.2610 sec/batch\n",
      "Epoch 3/15  Iteration 1289/8505 Training loss: 1.4542 9.3271 sec/batch\n",
      "Epoch 3/15  Iteration 1290/8505 Training loss: 1.4537 9.2642 sec/batch\n",
      "Epoch 3/15  Iteration 1291/8505 Training loss: 1.4539 9.2418 sec/batch\n",
      "Epoch 3/15  Iteration 1292/8505 Training loss: 1.4538 9.3341 sec/batch\n",
      "Epoch 3/15  Iteration 1293/8505 Training loss: 1.4539 9.2295 sec/batch\n",
      "Epoch 3/15  Iteration 1294/8505 Training loss: 1.4536 9.4621 sec/batch\n",
      "Epoch 3/15  Iteration 1295/8505 Training loss: 1.4536 9.3215 sec/batch\n",
      "Epoch 3/15  Iteration 1296/8505 Training loss: 1.4533 9.3443 sec/batch\n",
      "Epoch 3/15  Iteration 1297/8505 Training loss: 1.4532 9.2975 sec/batch\n",
      "Epoch 3/15  Iteration 1298/8505 Training loss: 1.4531 9.2804 sec/batch\n",
      "Epoch 3/15  Iteration 1299/8505 Training loss: 1.4531 9.2272 sec/batch\n",
      "Epoch 3/15  Iteration 1300/8505 Training loss: 1.4530 9.3344 sec/batch\n",
      "Epoch 3/15  Iteration 1301/8505 Training loss: 1.4528 9.3813 sec/batch\n",
      "Epoch 3/15  Iteration 1302/8505 Training loss: 1.4526 9.2125 sec/batch\n",
      "Epoch 3/15  Iteration 1303/8505 Training loss: 1.4526 9.2453 sec/batch\n",
      "Epoch 3/15  Iteration 1304/8505 Training loss: 1.4526 9.2865 sec/batch\n",
      "Epoch 3/15  Iteration 1305/8505 Training loss: 1.4529 9.3006 sec/batch\n",
      "Epoch 3/15  Iteration 1306/8505 Training loss: 1.4528 9.2477 sec/batch\n",
      "Epoch 3/15  Iteration 1307/8505 Training loss: 1.4525 9.2727 sec/batch\n",
      "Epoch 3/15  Iteration 1308/8505 Training loss: 1.4521 9.2885 sec/batch\n",
      "Epoch 3/15  Iteration 1309/8505 Training loss: 1.4521 9.2651 sec/batch\n",
      "Epoch 3/15  Iteration 1310/8505 Training loss: 1.4521 9.2527 sec/batch\n",
      "Epoch 3/15  Iteration 1311/8505 Training loss: 1.4520 9.2518 sec/batch\n",
      "Epoch 3/15  Iteration 1312/8505 Training loss: 1.4518 9.3088 sec/batch\n",
      "Epoch 3/15  Iteration 1313/8505 Training loss: 1.4516 9.3145 sec/batch\n",
      "Epoch 3/15  Iteration 1314/8505 Training loss: 1.4516 9.2430 sec/batch\n",
      "Epoch 3/15  Iteration 1315/8505 Training loss: 1.4514 9.2198 sec/batch\n",
      "Epoch 3/15  Iteration 1316/8505 Training loss: 1.4512 9.3393 sec/batch\n",
      "Epoch 3/15  Iteration 1317/8505 Training loss: 1.4511 9.2816 sec/batch\n",
      "Epoch 3/15  Iteration 1318/8505 Training loss: 1.4510 9.2719 sec/batch\n",
      "Epoch 3/15  Iteration 1319/8505 Training loss: 1.4509 9.1754 sec/batch\n",
      "Epoch 3/15  Iteration 1320/8505 Training loss: 1.4507 9.2720 sec/batch\n",
      "Epoch 3/15  Iteration 1321/8505 Training loss: 1.4505 9.3737 sec/batch\n",
      "Epoch 3/15  Iteration 1322/8505 Training loss: 1.4502 9.2862 sec/batch\n",
      "Epoch 3/15  Iteration 1323/8505 Training loss: 1.4503 9.2618 sec/batch\n",
      "Epoch 3/15  Iteration 1324/8505 Training loss: 1.4500 9.2617 sec/batch\n",
      "Epoch 3/15  Iteration 1325/8505 Training loss: 1.4498 9.3057 sec/batch\n",
      "Epoch 3/15  Iteration 1326/8505 Training loss: 1.4495 9.2163 sec/batch\n",
      "Epoch 3/15  Iteration 1327/8505 Training loss: 1.4497 9.2782 sec/batch\n",
      "Epoch 3/15  Iteration 1328/8505 Training loss: 1.4495 9.4904 sec/batch\n",
      "Epoch 3/15  Iteration 1329/8505 Training loss: 1.4494 9.3299 sec/batch\n",
      "Epoch 3/15  Iteration 1330/8505 Training loss: 1.4493 9.2591 sec/batch\n",
      "Epoch 3/15  Iteration 1331/8505 Training loss: 1.4491 9.2297 sec/batch\n",
      "Epoch 3/15  Iteration 1332/8505 Training loss: 1.4491 9.2488 sec/batch\n",
      "Epoch 3/15  Iteration 1333/8505 Training loss: 1.4491 9.2910 sec/batch\n",
      "Epoch 3/15  Iteration 1334/8505 Training loss: 1.4490 9.2438 sec/batch\n",
      "Epoch 3/15  Iteration 1335/8505 Training loss: 1.4490 9.2110 sec/batch\n",
      "Epoch 3/15  Iteration 1336/8505 Training loss: 1.4488 9.2714 sec/batch\n",
      "Epoch 3/15  Iteration 1337/8505 Training loss: 1.4489 9.2455 sec/batch\n",
      "Epoch 3/15  Iteration 1338/8505 Training loss: 1.4485 9.2081 sec/batch\n",
      "Epoch 3/15  Iteration 1339/8505 Training loss: 1.4482 9.3591 sec/batch\n",
      "Epoch 3/15  Iteration 1340/8505 Training loss: 1.4481 9.2819 sec/batch\n",
      "Epoch 3/15  Iteration 1341/8505 Training loss: 1.4481 9.3543 sec/batch\n",
      "Epoch 3/15  Iteration 1342/8505 Training loss: 1.4479 9.2164 sec/batch\n",
      "Epoch 3/15  Iteration 1343/8505 Training loss: 1.4476 9.2408 sec/batch\n",
      "Epoch 3/15  Iteration 1344/8505 Training loss: 1.4472 9.3262 sec/batch\n",
      "Epoch 3/15  Iteration 1345/8505 Training loss: 1.4470 9.3211 sec/batch\n",
      "Epoch 3/15  Iteration 1346/8505 Training loss: 1.4469 9.2779 sec/batch\n",
      "Epoch 3/15  Iteration 1347/8505 Training loss: 1.4466 9.2492 sec/batch\n",
      "Epoch 3/15  Iteration 1348/8505 Training loss: 1.4465 9.2854 sec/batch\n",
      "Epoch 3/15  Iteration 1349/8505 Training loss: 1.4463 9.3488 sec/batch\n",
      "Epoch 3/15  Iteration 1350/8505 Training loss: 1.4462 9.3352 sec/batch\n",
      "Epoch 3/15  Iteration 1351/8505 Training loss: 1.4459 9.3314 sec/batch\n",
      "Epoch 3/15  Iteration 1352/8505 Training loss: 1.4457 9.3951 sec/batch\n",
      "Epoch 3/15  Iteration 1353/8505 Training loss: 1.4454 9.3780 sec/batch\n",
      "Epoch 3/15  Iteration 1354/8505 Training loss: 1.4450 9.2443 sec/batch\n",
      "Epoch 3/15  Iteration 1355/8505 Training loss: 1.4448 9.3989 sec/batch\n",
      "Epoch 3/15  Iteration 1356/8505 Training loss: 1.4446 9.3254 sec/batch\n",
      "Epoch 3/15  Iteration 1357/8505 Training loss: 1.4444 9.2807 sec/batch\n",
      "Epoch 3/15  Iteration 1358/8505 Training loss: 1.4441 9.2558 sec/batch\n",
      "Epoch 3/15  Iteration 1359/8505 Training loss: 1.4439 9.2910 sec/batch\n",
      "Epoch 3/15  Iteration 1360/8505 Training loss: 1.4437 9.2912 sec/batch\n",
      "Epoch 3/15  Iteration 1361/8505 Training loss: 1.4435 9.2787 sec/batch\n",
      "Epoch 3/15  Iteration 1362/8505 Training loss: 1.4432 9.2551 sec/batch\n",
      "Epoch 3/15  Iteration 1363/8505 Training loss: 1.4430 9.3433 sec/batch\n",
      "Epoch 3/15  Iteration 1364/8505 Training loss: 1.4428 9.3289 sec/batch\n",
      "Epoch 3/15  Iteration 1365/8505 Training loss: 1.4425 9.3399 sec/batch\n",
      "Epoch 3/15  Iteration 1366/8505 Training loss: 1.4424 9.2995 sec/batch\n",
      "Epoch 3/15  Iteration 1367/8505 Training loss: 1.4422 10.2217 sec/batch\n",
      "Epoch 3/15  Iteration 1368/8505 Training loss: 1.4419 9.3770 sec/batch\n",
      "Epoch 3/15  Iteration 1369/8505 Training loss: 1.4417 9.3716 sec/batch\n",
      "Epoch 3/15  Iteration 1370/8505 Training loss: 1.4415 9.2732 sec/batch\n",
      "Epoch 3/15  Iteration 1371/8505 Training loss: 1.4414 9.3239 sec/batch\n",
      "Epoch 3/15  Iteration 1372/8505 Training loss: 1.4413 9.2384 sec/batch\n",
      "Epoch 3/15  Iteration 1373/8505 Training loss: 1.4410 9.1883 sec/batch\n",
      "Epoch 3/15  Iteration 1374/8505 Training loss: 1.4407 9.3285 sec/batch\n",
      "Epoch 3/15  Iteration 1375/8505 Training loss: 1.4405 9.2444 sec/batch\n",
      "Epoch 3/15  Iteration 1376/8505 Training loss: 1.4405 9.2447 sec/batch\n",
      "Epoch 3/15  Iteration 1377/8505 Training loss: 1.4402 9.3153 sec/batch\n",
      "Epoch 3/15  Iteration 1378/8505 Training loss: 1.4402 9.3190 sec/batch\n",
      "Epoch 3/15  Iteration 1379/8505 Training loss: 1.4401 9.4205 sec/batch\n",
      "Epoch 3/15  Iteration 1380/8505 Training loss: 1.4399 9.2942 sec/batch\n",
      "Epoch 3/15  Iteration 1381/8505 Training loss: 1.4398 9.2377 sec/batch\n",
      "Epoch 3/15  Iteration 1382/8505 Training loss: 1.4398 9.3114 sec/batch\n",
      "Epoch 3/15  Iteration 1383/8505 Training loss: 1.4397 9.3611 sec/batch\n",
      "Epoch 3/15  Iteration 1384/8505 Training loss: 1.4395 9.3379 sec/batch\n",
      "Epoch 3/15  Iteration 1385/8505 Training loss: 1.4394 9.4113 sec/batch\n",
      "Epoch 3/15  Iteration 1386/8505 Training loss: 1.4391 9.2790 sec/batch\n",
      "Epoch 3/15  Iteration 1387/8505 Training loss: 1.4388 10.1871 sec/batch\n",
      "Epoch 3/15  Iteration 1388/8505 Training loss: 1.4386 9.3163 sec/batch\n",
      "Epoch 3/15  Iteration 1389/8505 Training loss: 1.4387 9.3434 sec/batch\n",
      "Epoch 3/15  Iteration 1390/8505 Training loss: 1.4386 9.3059 sec/batch\n",
      "Epoch 3/15  Iteration 1391/8505 Training loss: 1.4385 9.3907 sec/batch\n",
      "Epoch 3/15  Iteration 1392/8505 Training loss: 1.4384 9.3149 sec/batch\n",
      "Epoch 3/15  Iteration 1393/8505 Training loss: 1.4382 9.2990 sec/batch\n",
      "Epoch 3/15  Iteration 1394/8505 Training loss: 1.4381 9.3375 sec/batch\n",
      "Epoch 3/15  Iteration 1395/8505 Training loss: 1.4379 9.3192 sec/batch\n",
      "Epoch 3/15  Iteration 1396/8505 Training loss: 1.4378 9.2849 sec/batch\n",
      "Epoch 3/15  Iteration 1397/8505 Training loss: 1.4377 9.1898 sec/batch\n",
      "Epoch 3/15  Iteration 1398/8505 Training loss: 1.4375 9.4367 sec/batch\n",
      "Epoch 3/15  Iteration 1399/8505 Training loss: 1.4374 9.3195 sec/batch\n",
      "Epoch 3/15  Iteration 1400/8505 Training loss: 1.4372 9.3748 sec/batch\n",
      "Validation loss: 1.35322 Saving checkpoint!\n",
      "Epoch 3/15  Iteration 1401/8505 Training loss: 1.4374 9.3801 sec/batch\n",
      "Epoch 3/15  Iteration 1402/8505 Training loss: 1.4373 9.2141 sec/batch\n",
      "Epoch 3/15  Iteration 1403/8505 Training loss: 1.4371 9.2671 sec/batch\n",
      "Epoch 3/15  Iteration 1404/8505 Training loss: 1.4370 9.2990 sec/batch\n",
      "Epoch 3/15  Iteration 1405/8505 Training loss: 1.4369 9.3010 sec/batch\n",
      "Epoch 3/15  Iteration 1406/8505 Training loss: 1.4367 9.3212 sec/batch\n",
      "Epoch 3/15  Iteration 1407/8505 Training loss: 1.4367 9.3693 sec/batch\n",
      "Epoch 3/15  Iteration 1408/8505 Training loss: 1.4366 9.4102 sec/batch\n",
      "Epoch 3/15  Iteration 1409/8505 Training loss: 1.4365 9.3280 sec/batch\n",
      "Epoch 3/15  Iteration 1410/8505 Training loss: 1.4365 9.4688 sec/batch\n",
      "Epoch 3/15  Iteration 1411/8505 Training loss: 1.4363 9.2886 sec/batch\n",
      "Epoch 3/15  Iteration 1412/8505 Training loss: 1.4363 9.2401 sec/batch\n",
      "Epoch 3/15  Iteration 1413/8505 Training loss: 1.4362 9.2243 sec/batch\n",
      "Epoch 3/15  Iteration 1414/8505 Training loss: 1.4360 9.3956 sec/batch\n",
      "Epoch 3/15  Iteration 1415/8505 Training loss: 1.4357 9.1888 sec/batch\n",
      "Epoch 3/15  Iteration 1416/8505 Training loss: 1.4356 9.2501 sec/batch\n",
      "Epoch 3/15  Iteration 1417/8505 Training loss: 1.4355 9.3699 sec/batch\n",
      "Epoch 3/15  Iteration 1418/8505 Training loss: 1.4352 9.3009 sec/batch\n",
      "Epoch 3/15  Iteration 1419/8505 Training loss: 1.4350 9.2630 sec/batch\n",
      "Epoch 3/15  Iteration 1420/8505 Training loss: 1.4350 9.3097 sec/batch\n",
      "Epoch 3/15  Iteration 1421/8505 Training loss: 1.4350 9.3436 sec/batch\n",
      "Epoch 3/15  Iteration 1422/8505 Training loss: 1.4350 9.2241 sec/batch\n",
      "Epoch 3/15  Iteration 1423/8505 Training loss: 1.4349 9.2420 sec/batch\n",
      "Epoch 3/15  Iteration 1424/8505 Training loss: 1.4347 9.2786 sec/batch\n",
      "Epoch 3/15  Iteration 1425/8505 Training loss: 1.4345 9.2092 sec/batch\n",
      "Epoch 3/15  Iteration 1426/8505 Training loss: 1.4343 9.3507 sec/batch\n",
      "Epoch 3/15  Iteration 1427/8505 Training loss: 1.4341 9.2622 sec/batch\n",
      "Epoch 3/15  Iteration 1428/8505 Training loss: 1.4340 9.5631 sec/batch\n",
      "Epoch 3/15  Iteration 1429/8505 Training loss: 1.4340 9.3772 sec/batch\n",
      "Epoch 3/15  Iteration 1430/8505 Training loss: 1.4337 9.4617 sec/batch\n",
      "Epoch 3/15  Iteration 1431/8505 Training loss: 1.4336 9.3220 sec/batch\n",
      "Epoch 3/15  Iteration 1432/8505 Training loss: 1.4334 10.5922 sec/batch\n",
      "Epoch 3/15  Iteration 1433/8505 Training loss: 1.4333 12.5553 sec/batch\n",
      "Epoch 3/15  Iteration 1434/8505 Training loss: 1.4333 11.1138 sec/batch\n",
      "Epoch 3/15  Iteration 1435/8505 Training loss: 1.4331 9.5511 sec/batch\n",
      "Epoch 3/15  Iteration 1436/8505 Training loss: 1.4329 9.4683 sec/batch\n",
      "Epoch 3/15  Iteration 1437/8505 Training loss: 1.4328 9.4640 sec/batch\n",
      "Epoch 3/15  Iteration 1438/8505 Training loss: 1.4327 9.2748 sec/batch\n",
      "Epoch 3/15  Iteration 1439/8505 Training loss: 1.4327 9.3129 sec/batch\n",
      "Epoch 3/15  Iteration 1440/8505 Training loss: 1.4326 9.3332 sec/batch\n",
      "Epoch 3/15  Iteration 1441/8505 Training loss: 1.4324 9.2812 sec/batch\n",
      "Epoch 3/15  Iteration 1442/8505 Training loss: 1.4323 9.2360 sec/batch\n",
      "Epoch 3/15  Iteration 1443/8505 Training loss: 1.4323 9.2610 sec/batch\n",
      "Epoch 3/15  Iteration 1444/8505 Training loss: 1.4322 9.2169 sec/batch\n",
      "Epoch 3/15  Iteration 1445/8505 Training loss: 1.4322 9.2845 sec/batch\n",
      "Epoch 3/15  Iteration 1446/8505 Training loss: 1.4320 9.2652 sec/batch\n",
      "Epoch 3/15  Iteration 1447/8505 Training loss: 1.4318 9.2973 sec/batch\n",
      "Epoch 3/15  Iteration 1448/8505 Training loss: 1.4317 9.3018 sec/batch\n",
      "Epoch 3/15  Iteration 1449/8505 Training loss: 1.4316 9.3325 sec/batch\n",
      "Epoch 3/15  Iteration 1450/8505 Training loss: 1.4314 9.2379 sec/batch\n",
      "Epoch 3/15  Iteration 1451/8505 Training loss: 1.4313 9.2935 sec/batch\n",
      "Epoch 3/15  Iteration 1452/8505 Training loss: 1.4312 9.2821 sec/batch\n",
      "Epoch 3/15  Iteration 1453/8505 Training loss: 1.4311 9.3078 sec/batch\n",
      "Epoch 3/15  Iteration 1454/8505 Training loss: 1.4310 9.2475 sec/batch\n",
      "Epoch 3/15  Iteration 1455/8505 Training loss: 1.4309 9.2092 sec/batch\n",
      "Epoch 3/15  Iteration 1456/8505 Training loss: 1.4308 9.3076 sec/batch\n",
      "Epoch 3/15  Iteration 1457/8505 Training loss: 1.4307 9.2811 sec/batch\n",
      "Epoch 3/15  Iteration 1458/8505 Training loss: 1.4307 9.3010 sec/batch\n",
      "Epoch 3/15  Iteration 1459/8505 Training loss: 1.4307 9.2608 sec/batch\n",
      "Epoch 3/15  Iteration 1460/8505 Training loss: 1.4306 9.2459 sec/batch\n",
      "Epoch 3/15  Iteration 1461/8505 Training loss: 1.4305 9.2431 sec/batch\n",
      "Epoch 3/15  Iteration 1462/8505 Training loss: 1.4304 9.3064 sec/batch\n",
      "Epoch 3/15  Iteration 1463/8505 Training loss: 1.4302 9.3012 sec/batch\n",
      "Epoch 3/15  Iteration 1464/8505 Training loss: 1.4301 9.5242 sec/batch\n",
      "Epoch 3/15  Iteration 1465/8505 Training loss: 1.4300 9.2714 sec/batch\n",
      "Epoch 3/15  Iteration 1466/8505 Training loss: 1.4299 9.3351 sec/batch\n",
      "Epoch 3/15  Iteration 1467/8505 Training loss: 1.4298 9.2569 sec/batch\n",
      "Epoch 3/15  Iteration 1468/8505 Training loss: 1.4296 9.3245 sec/batch\n",
      "Epoch 3/15  Iteration 1469/8505 Training loss: 1.4295 9.2271 sec/batch\n",
      "Epoch 3/15  Iteration 1470/8505 Training loss: 1.4294 9.2817 sec/batch\n",
      "Epoch 3/15  Iteration 1471/8505 Training loss: 1.4295 9.2109 sec/batch\n",
      "Epoch 3/15  Iteration 1472/8505 Training loss: 1.4295 9.3925 sec/batch\n",
      "Epoch 3/15  Iteration 1473/8505 Training loss: 1.4295 9.2614 sec/batch\n",
      "Epoch 3/15  Iteration 1474/8505 Training loss: 1.4293 9.3091 sec/batch\n",
      "Epoch 3/15  Iteration 1475/8505 Training loss: 1.4292 9.2760 sec/batch\n",
      "Epoch 3/15  Iteration 1476/8505 Training loss: 1.4291 9.2412 sec/batch\n",
      "Epoch 3/15  Iteration 1477/8505 Training loss: 1.4289 9.3271 sec/batch\n",
      "Epoch 3/15  Iteration 1478/8505 Training loss: 1.4288 9.2970 sec/batch\n",
      "Epoch 3/15  Iteration 1479/8505 Training loss: 1.4286 9.2222 sec/batch\n",
      "Epoch 3/15  Iteration 1480/8505 Training loss: 1.4285 9.2841 sec/batch\n",
      "Epoch 3/15  Iteration 1481/8505 Training loss: 1.4284 9.2468 sec/batch\n",
      "Epoch 3/15  Iteration 1482/8505 Training loss: 1.4283 9.2561 sec/batch\n",
      "Epoch 3/15  Iteration 1483/8505 Training loss: 1.4282 9.2159 sec/batch\n",
      "Epoch 3/15  Iteration 1484/8505 Training loss: 1.4281 9.2955 sec/batch\n",
      "Epoch 3/15  Iteration 1485/8505 Training loss: 1.4281 9.2159 sec/batch\n",
      "Epoch 3/15  Iteration 1486/8505 Training loss: 1.4280 9.2166 sec/batch\n",
      "Epoch 3/15  Iteration 1487/8505 Training loss: 1.4278 9.2162 sec/batch\n",
      "Epoch 3/15  Iteration 1488/8505 Training loss: 1.4277 9.3060 sec/batch\n",
      "Epoch 3/15  Iteration 1489/8505 Training loss: 1.4277 9.2102 sec/batch\n",
      "Epoch 3/15  Iteration 1490/8505 Training loss: 1.4276 9.3013 sec/batch\n",
      "Epoch 3/15  Iteration 1491/8505 Training loss: 1.4276 9.3013 sec/batch\n",
      "Epoch 3/15  Iteration 1492/8505 Training loss: 1.4274 9.3090 sec/batch\n",
      "Epoch 3/15  Iteration 1493/8505 Training loss: 1.4272 9.4069 sec/batch\n",
      "Epoch 3/15  Iteration 1494/8505 Training loss: 1.4270 9.3377 sec/batch\n",
      "Epoch 3/15  Iteration 1495/8505 Training loss: 1.4270 9.3221 sec/batch\n",
      "Epoch 3/15  Iteration 1496/8505 Training loss: 1.4268 9.3824 sec/batch\n",
      "Epoch 3/15  Iteration 1497/8505 Training loss: 1.4268 9.3055 sec/batch\n",
      "Epoch 3/15  Iteration 1498/8505 Training loss: 1.4266 9.3491 sec/batch\n",
      "Epoch 3/15  Iteration 1499/8505 Training loss: 1.4267 9.3378 sec/batch\n",
      "Epoch 3/15  Iteration 1500/8505 Training loss: 1.4265 9.2670 sec/batch\n",
      "Epoch 3/15  Iteration 1501/8505 Training loss: 1.4266 9.3147 sec/batch\n",
      "Epoch 3/15  Iteration 1502/8505 Training loss: 1.4264 9.2578 sec/batch\n",
      "Epoch 3/15  Iteration 1503/8505 Training loss: 1.4262 9.2317 sec/batch\n",
      "Epoch 3/15  Iteration 1504/8505 Training loss: 1.4260 9.2928 sec/batch\n",
      "Epoch 3/15  Iteration 1505/8505 Training loss: 1.4259 9.3398 sec/batch\n",
      "Epoch 3/15  Iteration 1506/8505 Training loss: 1.4259 9.2696 sec/batch\n",
      "Epoch 3/15  Iteration 1507/8505 Training loss: 1.4259 9.2891 sec/batch\n",
      "Epoch 3/15  Iteration 1508/8505 Training loss: 1.4258 9.2343 sec/batch\n",
      "Epoch 3/15  Iteration 1509/8505 Training loss: 1.4257 9.2434 sec/batch\n",
      "Epoch 3/15  Iteration 1510/8505 Training loss: 1.4257 9.2756 sec/batch\n",
      "Epoch 3/15  Iteration 1511/8505 Training loss: 1.4255 9.3314 sec/batch\n",
      "Epoch 3/15  Iteration 1512/8505 Training loss: 1.4254 9.3332 sec/batch\n",
      "Epoch 3/15  Iteration 1513/8505 Training loss: 1.4254 9.3019 sec/batch\n",
      "Epoch 3/15  Iteration 1514/8505 Training loss: 1.4254 9.3244 sec/batch\n",
      "Epoch 3/15  Iteration 1515/8505 Training loss: 1.4254 9.2497 sec/batch\n",
      "Epoch 3/15  Iteration 1516/8505 Training loss: 1.4252 9.2505 sec/batch\n",
      "Epoch 3/15  Iteration 1517/8505 Training loss: 1.4251 9.2975 sec/batch\n",
      "Epoch 3/15  Iteration 1518/8505 Training loss: 1.4249 9.2831 sec/batch\n",
      "Epoch 3/15  Iteration 1519/8505 Training loss: 1.4248 9.2710 sec/batch\n",
      "Epoch 3/15  Iteration 1520/8505 Training loss: 1.4246 9.2422 sec/batch\n",
      "Epoch 3/15  Iteration 1521/8505 Training loss: 1.4244 9.3156 sec/batch\n",
      "Epoch 3/15  Iteration 1522/8505 Training loss: 1.4242 9.2975 sec/batch\n",
      "Epoch 3/15  Iteration 1523/8505 Training loss: 1.4241 9.2897 sec/batch\n",
      "Epoch 3/15  Iteration 1524/8505 Training loss: 1.4239 9.2391 sec/batch\n",
      "Epoch 3/15  Iteration 1525/8505 Training loss: 1.4237 9.2320 sec/batch\n",
      "Epoch 3/15  Iteration 1526/8505 Training loss: 1.4235 9.3478 sec/batch\n",
      "Epoch 3/15  Iteration 1527/8505 Training loss: 1.4234 9.4509 sec/batch\n",
      "Epoch 3/15  Iteration 1528/8505 Training loss: 1.4233 9.4388 sec/batch\n",
      "Epoch 3/15  Iteration 1529/8505 Training loss: 1.4232 9.2513 sec/batch\n",
      "Epoch 3/15  Iteration 1530/8505 Training loss: 1.4231 9.2965 sec/batch\n",
      "Epoch 3/15  Iteration 1531/8505 Training loss: 1.4230 9.2812 sec/batch\n",
      "Epoch 3/15  Iteration 1532/8505 Training loss: 1.4230 9.2938 sec/batch\n",
      "Epoch 3/15  Iteration 1533/8505 Training loss: 1.4230 9.2963 sec/batch\n",
      "Epoch 3/15  Iteration 1534/8505 Training loss: 1.4228 9.3111 sec/batch\n",
      "Epoch 3/15  Iteration 1535/8505 Training loss: 1.4226 9.2717 sec/batch\n",
      "Epoch 3/15  Iteration 1536/8505 Training loss: 1.4225 9.1790 sec/batch\n",
      "Epoch 3/15  Iteration 1537/8505 Training loss: 1.4222 9.3426 sec/batch\n",
      "Epoch 3/15  Iteration 1538/8505 Training loss: 1.4221 9.3384 sec/batch\n",
      "Epoch 3/15  Iteration 1539/8505 Training loss: 1.4220 9.2040 sec/batch\n",
      "Epoch 3/15  Iteration 1540/8505 Training loss: 1.4219 9.3353 sec/batch\n",
      "Epoch 3/15  Iteration 1541/8505 Training loss: 1.4217 9.3400 sec/batch\n",
      "Epoch 3/15  Iteration 1542/8505 Training loss: 1.4216 9.2715 sec/batch\n",
      "Epoch 3/15  Iteration 1543/8505 Training loss: 1.4214 9.2877 sec/batch\n",
      "Epoch 3/15  Iteration 1544/8505 Training loss: 1.4213 9.2334 sec/batch\n",
      "Epoch 3/15  Iteration 1545/8505 Training loss: 1.4213 9.2318 sec/batch\n",
      "Epoch 3/15  Iteration 1546/8505 Training loss: 1.4213 9.3029 sec/batch\n",
      "Epoch 3/15  Iteration 1547/8505 Training loss: 1.4213 9.3046 sec/batch\n",
      "Epoch 3/15  Iteration 1548/8505 Training loss: 1.4211 9.2422 sec/batch\n",
      "Epoch 3/15  Iteration 1549/8505 Training loss: 1.4208 9.2958 sec/batch\n",
      "Epoch 3/15  Iteration 1550/8505 Training loss: 1.4206 9.3190 sec/batch\n",
      "Epoch 3/15  Iteration 1551/8505 Training loss: 1.4205 9.2436 sec/batch\n",
      "Epoch 3/15  Iteration 1552/8505 Training loss: 1.4204 9.3174 sec/batch\n",
      "Epoch 3/15  Iteration 1553/8505 Training loss: 1.4203 9.2358 sec/batch\n",
      "Epoch 3/15  Iteration 1554/8505 Training loss: 1.4202 9.4118 sec/batch\n",
      "Epoch 3/15  Iteration 1555/8505 Training loss: 1.4201 9.2633 sec/batch\n",
      "Epoch 3/15  Iteration 1556/8505 Training loss: 1.4200 9.2676 sec/batch\n",
      "Epoch 3/15  Iteration 1557/8505 Training loss: 1.4199 9.2476 sec/batch\n",
      "Epoch 3/15  Iteration 1558/8505 Training loss: 1.4199 9.2202 sec/batch\n",
      "Epoch 3/15  Iteration 1559/8505 Training loss: 1.4197 9.2764 sec/batch\n",
      "Epoch 3/15  Iteration 1560/8505 Training loss: 1.4197 9.2593 sec/batch\n",
      "Epoch 3/15  Iteration 1561/8505 Training loss: 1.4196 9.2921 sec/batch\n",
      "Epoch 3/15  Iteration 1562/8505 Training loss: 1.4195 9.3088 sec/batch\n",
      "Epoch 3/15  Iteration 1563/8505 Training loss: 1.4194 9.4001 sec/batch\n",
      "Epoch 3/15  Iteration 1564/8505 Training loss: 1.4193 9.2464 sec/batch\n",
      "Epoch 3/15  Iteration 1565/8505 Training loss: 1.4192 9.4984 sec/batch\n",
      "Epoch 3/15  Iteration 1566/8505 Training loss: 1.4191 9.3533 sec/batch\n",
      "Epoch 3/15  Iteration 1567/8505 Training loss: 1.4189 9.6646 sec/batch\n",
      "Epoch 3/15  Iteration 1568/8505 Training loss: 1.4189 9.2231 sec/batch\n",
      "Epoch 3/15  Iteration 1569/8505 Training loss: 1.4187 9.3261 sec/batch\n",
      "Epoch 3/15  Iteration 1570/8505 Training loss: 1.4185 9.3018 sec/batch\n",
      "Epoch 3/15  Iteration 1571/8505 Training loss: 1.4184 9.2872 sec/batch\n",
      "Epoch 3/15  Iteration 1572/8505 Training loss: 1.4182 9.3182 sec/batch\n",
      "Epoch 3/15  Iteration 1573/8505 Training loss: 1.4180 9.3661 sec/batch\n",
      "Epoch 3/15  Iteration 1574/8505 Training loss: 1.4180 9.2716 sec/batch\n",
      "Epoch 3/15  Iteration 1575/8505 Training loss: 1.4179 9.2519 sec/batch\n",
      "Epoch 3/15  Iteration 1576/8505 Training loss: 1.4177 9.2576 sec/batch\n",
      "Epoch 3/15  Iteration 1577/8505 Training loss: 1.4176 9.2886 sec/batch\n",
      "Epoch 3/15  Iteration 1578/8505 Training loss: 1.4175 9.2614 sec/batch\n",
      "Epoch 3/15  Iteration 1579/8505 Training loss: 1.4174 9.2729 sec/batch\n",
      "Epoch 3/15  Iteration 1580/8505 Training loss: 1.4173 9.3251 sec/batch\n",
      "Epoch 3/15  Iteration 1581/8505 Training loss: 1.4172 9.2741 sec/batch\n",
      "Epoch 3/15  Iteration 1582/8505 Training loss: 1.4170 9.2940 sec/batch\n",
      "Epoch 3/15  Iteration 1583/8505 Training loss: 1.4169 9.2436 sec/batch\n",
      "Epoch 3/15  Iteration 1584/8505 Training loss: 1.4168 9.2585 sec/batch\n",
      "Epoch 3/15  Iteration 1585/8505 Training loss: 1.4166 9.3318 sec/batch\n",
      "Epoch 3/15  Iteration 1586/8505 Training loss: 1.4165 9.2271 sec/batch\n",
      "Epoch 3/15  Iteration 1587/8505 Training loss: 1.4164 9.3180 sec/batch\n",
      "Epoch 3/15  Iteration 1588/8505 Training loss: 1.4163 9.2798 sec/batch\n",
      "Epoch 3/15  Iteration 1589/8505 Training loss: 1.4162 9.3275 sec/batch\n",
      "Epoch 3/15  Iteration 1590/8505 Training loss: 1.4161 9.4370 sec/batch\n",
      "Epoch 3/15  Iteration 1591/8505 Training loss: 1.4160 9.2733 sec/batch\n",
      "Epoch 3/15  Iteration 1592/8505 Training loss: 1.4158 9.2989 sec/batch\n",
      "Epoch 3/15  Iteration 1593/8505 Training loss: 1.4157 9.2247 sec/batch\n",
      "Epoch 3/15  Iteration 1594/8505 Training loss: 1.4156 9.2744 sec/batch\n",
      "Epoch 3/15  Iteration 1595/8505 Training loss: 1.4155 9.3722 sec/batch\n",
      "Epoch 3/15  Iteration 1596/8505 Training loss: 1.4153 9.2747 sec/batch\n",
      "Epoch 3/15  Iteration 1597/8505 Training loss: 1.4152 9.2315 sec/batch\n",
      "Epoch 3/15  Iteration 1598/8505 Training loss: 1.4151 9.2699 sec/batch\n",
      "Epoch 3/15  Iteration 1599/8505 Training loss: 1.4151 9.3737 sec/batch\n",
      "Epoch 3/15  Iteration 1600/8505 Training loss: 1.4151 9.2364 sec/batch\n",
      "Validation loss: 1.31956 Saving checkpoint!\n",
      "Epoch 3/15  Iteration 1601/8505 Training loss: 1.4151 9.3070 sec/batch\n",
      "Epoch 3/15  Iteration 1602/8505 Training loss: 1.4150 9.1999 sec/batch\n",
      "Epoch 3/15  Iteration 1603/8505 Training loss: 1.4149 9.2916 sec/batch\n",
      "Epoch 3/15  Iteration 1604/8505 Training loss: 1.4148 9.3091 sec/batch\n",
      "Epoch 3/15  Iteration 1605/8505 Training loss: 1.4146 9.3203 sec/batch\n",
      "Epoch 3/15  Iteration 1606/8505 Training loss: 1.4145 9.2450 sec/batch\n",
      "Epoch 3/15  Iteration 1607/8505 Training loss: 1.4143 9.2824 sec/batch\n",
      "Epoch 3/15  Iteration 1608/8505 Training loss: 1.4143 9.2875 sec/batch\n",
      "Epoch 3/15  Iteration 1609/8505 Training loss: 1.4142 9.3101 sec/batch\n",
      "Epoch 3/15  Iteration 1610/8505 Training loss: 1.4141 9.6367 sec/batch\n",
      "Epoch 3/15  Iteration 1611/8505 Training loss: 1.4139 9.3704 sec/batch\n",
      "Epoch 3/15  Iteration 1612/8505 Training loss: 1.4139 9.3303 sec/batch\n",
      "Epoch 3/15  Iteration 1613/8505 Training loss: 1.4137 9.2080 sec/batch\n",
      "Epoch 3/15  Iteration 1614/8505 Training loss: 1.4135 9.3243 sec/batch\n",
      "Epoch 3/15  Iteration 1615/8505 Training loss: 1.4134 9.2285 sec/batch\n",
      "Epoch 3/15  Iteration 1616/8505 Training loss: 1.4132 9.2494 sec/batch\n",
      "Epoch 3/15  Iteration 1617/8505 Training loss: 1.4131 9.2504 sec/batch\n",
      "Epoch 3/15  Iteration 1618/8505 Training loss: 1.4130 9.3988 sec/batch\n",
      "Epoch 3/15  Iteration 1619/8505 Training loss: 1.4129 9.2805 sec/batch\n",
      "Epoch 3/15  Iteration 1620/8505 Training loss: 1.4128 9.3344 sec/batch\n",
      "Epoch 3/15  Iteration 1621/8505 Training loss: 1.4127 9.1931 sec/batch\n",
      "Epoch 3/15  Iteration 1622/8505 Training loss: 1.4125 9.2715 sec/batch\n",
      "Epoch 3/15  Iteration 1623/8505 Training loss: 1.4124 9.2837 sec/batch\n",
      "Epoch 3/15  Iteration 1624/8505 Training loss: 1.4123 9.2840 sec/batch\n",
      "Epoch 3/15  Iteration 1625/8505 Training loss: 1.4122 9.2869 sec/batch\n",
      "Epoch 3/15  Iteration 1626/8505 Training loss: 1.4121 9.2815 sec/batch\n",
      "Epoch 3/15  Iteration 1627/8505 Training loss: 1.4119 9.2376 sec/batch\n",
      "Epoch 3/15  Iteration 1628/8505 Training loss: 1.4118 9.3777 sec/batch\n",
      "Epoch 3/15  Iteration 1629/8505 Training loss: 1.4117 9.2749 sec/batch\n",
      "Epoch 3/15  Iteration 1630/8505 Training loss: 1.4116 9.3000 sec/batch\n",
      "Epoch 3/15  Iteration 1631/8505 Training loss: 1.4115 9.2877 sec/batch\n",
      "Epoch 3/15  Iteration 1632/8505 Training loss: 1.4114 9.2592 sec/batch\n",
      "Epoch 3/15  Iteration 1633/8505 Training loss: 1.4112 9.3131 sec/batch\n",
      "Epoch 3/15  Iteration 1634/8505 Training loss: 1.4110 9.3160 sec/batch\n",
      "Epoch 3/15  Iteration 1635/8505 Training loss: 1.4108 9.2799 sec/batch\n",
      "Epoch 3/15  Iteration 1636/8505 Training loss: 1.4107 9.3388 sec/batch\n",
      "Epoch 3/15  Iteration 1637/8505 Training loss: 1.4105 9.2842 sec/batch\n",
      "Epoch 3/15  Iteration 1638/8505 Training loss: 1.4103 9.2378 sec/batch\n",
      "Epoch 3/15  Iteration 1639/8505 Training loss: 1.4102 9.2434 sec/batch\n",
      "Epoch 3/15  Iteration 1640/8505 Training loss: 1.4101 9.3603 sec/batch\n",
      "Epoch 3/15  Iteration 1641/8505 Training loss: 1.4100 9.2811 sec/batch\n",
      "Epoch 3/15  Iteration 1642/8505 Training loss: 1.4098 9.2799 sec/batch\n",
      "Epoch 3/15  Iteration 1643/8505 Training loss: 1.4097 9.2590 sec/batch\n",
      "Epoch 3/15  Iteration 1644/8505 Training loss: 1.4094 9.2747 sec/batch\n",
      "Epoch 3/15  Iteration 1645/8505 Training loss: 1.4092 9.2962 sec/batch\n",
      "Epoch 3/15  Iteration 1646/8505 Training loss: 1.4090 9.4388 sec/batch\n",
      "Epoch 3/15  Iteration 1647/8505 Training loss: 1.4088 9.2445 sec/batch\n",
      "Epoch 3/15  Iteration 1648/8505 Training loss: 1.4087 9.2029 sec/batch\n",
      "Epoch 3/15  Iteration 1649/8505 Training loss: 1.4086 9.3214 sec/batch\n",
      "Epoch 3/15  Iteration 1650/8505 Training loss: 1.4085 9.2348 sec/batch\n",
      "Epoch 3/15  Iteration 1651/8505 Training loss: 1.4083 9.2552 sec/batch\n",
      "Epoch 3/15  Iteration 1652/8505 Training loss: 1.4082 9.3228 sec/batch\n",
      "Epoch 3/15  Iteration 1653/8505 Training loss: 1.4080 9.2712 sec/batch\n",
      "Epoch 3/15  Iteration 1654/8505 Training loss: 1.4079 9.3099 sec/batch\n",
      "Epoch 3/15  Iteration 1655/8505 Training loss: 1.4077 9.3215 sec/batch\n",
      "Epoch 3/15  Iteration 1656/8505 Training loss: 1.4075 9.2992 sec/batch\n",
      "Epoch 3/15  Iteration 1657/8505 Training loss: 1.4074 9.2751 sec/batch\n",
      "Epoch 3/15  Iteration 1658/8505 Training loss: 1.4072 9.2466 sec/batch\n",
      "Epoch 3/15  Iteration 1659/8505 Training loss: 1.4071 9.2381 sec/batch\n",
      "Epoch 3/15  Iteration 1660/8505 Training loss: 1.4069 9.2924 sec/batch\n",
      "Epoch 3/15  Iteration 1661/8505 Training loss: 1.4067 9.4991 sec/batch\n",
      "Epoch 3/15  Iteration 1662/8505 Training loss: 1.4065 9.3154 sec/batch\n",
      "Epoch 3/15  Iteration 1663/8505 Training loss: 1.4064 9.2150 sec/batch\n",
      "Epoch 3/15  Iteration 1664/8505 Training loss: 1.4063 9.3498 sec/batch\n",
      "Epoch 3/15  Iteration 1665/8505 Training loss: 1.4062 9.3000 sec/batch\n",
      "Epoch 3/15  Iteration 1666/8505 Training loss: 1.4061 9.3400 sec/batch\n",
      "Epoch 3/15  Iteration 1667/8505 Training loss: 1.4059 9.3031 sec/batch\n",
      "Epoch 3/15  Iteration 1668/8505 Training loss: 1.4057 9.2736 sec/batch\n",
      "Epoch 3/15  Iteration 1669/8505 Training loss: 1.4056 9.2731 sec/batch\n",
      "Epoch 3/15  Iteration 1670/8505 Training loss: 1.4055 9.2740 sec/batch\n",
      "Epoch 3/15  Iteration 1671/8505 Training loss: 1.4054 9.3422 sec/batch\n",
      "Epoch 3/15  Iteration 1672/8505 Training loss: 1.4052 9.1844 sec/batch\n",
      "Epoch 3/15  Iteration 1673/8505 Training loss: 1.4051 9.2531 sec/batch\n",
      "Epoch 3/15  Iteration 1674/8505 Training loss: 1.4050 9.3452 sec/batch\n",
      "Epoch 3/15  Iteration 1675/8505 Training loss: 1.4049 9.3056 sec/batch\n",
      "Epoch 3/15  Iteration 1676/8505 Training loss: 1.4048 9.3332 sec/batch\n",
      "Epoch 3/15  Iteration 1677/8505 Training loss: 1.4047 9.2739 sec/batch\n",
      "Epoch 3/15  Iteration 1678/8505 Training loss: 1.4046 9.3733 sec/batch\n",
      "Epoch 3/15  Iteration 1679/8505 Training loss: 1.4045 9.2671 sec/batch\n",
      "Epoch 3/15  Iteration 1680/8505 Training loss: 1.4044 9.2982 sec/batch\n",
      "Epoch 3/15  Iteration 1681/8505 Training loss: 1.4043 9.4435 sec/batch\n",
      "Epoch 3/15  Iteration 1682/8505 Training loss: 1.4043 9.2250 sec/batch\n",
      "Epoch 3/15  Iteration 1683/8505 Training loss: 1.4041 9.3227 sec/batch\n",
      "Epoch 3/15  Iteration 1684/8505 Training loss: 1.4039 9.3532 sec/batch\n",
      "Epoch 3/15  Iteration 1685/8505 Training loss: 1.4039 9.4725 sec/batch\n",
      "Epoch 3/15  Iteration 1686/8505 Training loss: 1.4038 9.2089 sec/batch\n",
      "Epoch 3/15  Iteration 1687/8505 Training loss: 1.4037 9.2262 sec/batch\n",
      "Epoch 3/15  Iteration 1688/8505 Training loss: 1.4037 9.2374 sec/batch\n",
      "Epoch 3/15  Iteration 1689/8505 Training loss: 1.4036 9.2550 sec/batch\n",
      "Epoch 3/15  Iteration 1690/8505 Training loss: 1.4035 9.3135 sec/batch\n",
      "Epoch 3/15  Iteration 1691/8505 Training loss: 1.4034 9.3084 sec/batch\n",
      "Epoch 3/15  Iteration 1692/8505 Training loss: 1.4034 9.2670 sec/batch\n",
      "Epoch 3/15  Iteration 1693/8505 Training loss: 1.4034 9.2785 sec/batch\n",
      "Epoch 3/15  Iteration 1694/8505 Training loss: 1.4033 9.4934 sec/batch\n",
      "Epoch 3/15  Iteration 1695/8505 Training loss: 1.4032 9.3178 sec/batch\n",
      "Epoch 3/15  Iteration 1696/8505 Training loss: 1.4031 9.3539 sec/batch\n",
      "Epoch 3/15  Iteration 1697/8505 Training loss: 1.4030 9.2725 sec/batch\n",
      "Epoch 3/15  Iteration 1698/8505 Training loss: 1.4028 9.3771 sec/batch\n",
      "Epoch 3/15  Iteration 1699/8505 Training loss: 1.4027 9.3262 sec/batch\n",
      "Epoch 3/15  Iteration 1700/8505 Training loss: 1.4025 9.2900 sec/batch\n",
      "Epoch 3/15  Iteration 1701/8505 Training loss: 1.4024 9.3253 sec/batch\n",
      "Epoch 4/15  Iteration 1702/8505 Training loss: 1.4981 9.2565 sec/batch\n",
      "Epoch 4/15  Iteration 1703/8505 Training loss: 1.4315 9.2446 sec/batch\n",
      "Epoch 4/15  Iteration 1704/8505 Training loss: 1.4138 9.2999 sec/batch\n",
      "Epoch 4/15  Iteration 1705/8505 Training loss: 1.4067 9.3355 sec/batch\n",
      "Epoch 4/15  Iteration 1706/8505 Training loss: 1.4042 9.2907 sec/batch\n",
      "Epoch 4/15  Iteration 1707/8505 Training loss: 1.3962 9.2671 sec/batch\n",
      "Epoch 4/15  Iteration 1708/8505 Training loss: 1.3885 9.4500 sec/batch\n",
      "Epoch 4/15  Iteration 1709/8505 Training loss: 1.3829 9.4246 sec/batch\n",
      "Epoch 4/15  Iteration 1710/8505 Training loss: 1.3808 9.3542 sec/batch\n",
      "Epoch 4/15  Iteration 1711/8505 Training loss: 1.3697 9.3683 sec/batch\n",
      "Epoch 4/15  Iteration 1712/8505 Training loss: 1.3722 9.3925 sec/batch\n",
      "Epoch 4/15  Iteration 1713/8505 Training loss: 1.3721 9.3467 sec/batch\n",
      "Epoch 4/15  Iteration 1714/8505 Training loss: 1.3704 9.3206 sec/batch\n",
      "Epoch 4/15  Iteration 1715/8505 Training loss: 1.3714 9.3101 sec/batch\n",
      "Epoch 4/15  Iteration 1716/8505 Training loss: 1.3701 9.3556 sec/batch\n",
      "Epoch 4/15  Iteration 1717/8505 Training loss: 1.3683 9.2911 sec/batch\n",
      "Epoch 4/15  Iteration 1718/8505 Training loss: 1.3669 9.2509 sec/batch\n",
      "Epoch 4/15  Iteration 1719/8505 Training loss: 1.3669 9.2168 sec/batch\n",
      "Epoch 4/15  Iteration 1720/8505 Training loss: 1.3656 9.3827 sec/batch\n",
      "Epoch 4/15  Iteration 1721/8505 Training loss: 1.3652 9.2923 sec/batch\n",
      "Epoch 4/15  Iteration 1722/8505 Training loss: 1.3652 9.2755 sec/batch\n",
      "Epoch 4/15  Iteration 1723/8505 Training loss: 1.3636 9.2285 sec/batch\n",
      "Epoch 4/15  Iteration 1724/8505 Training loss: 1.3623 9.2389 sec/batch\n",
      "Epoch 4/15  Iteration 1725/8505 Training loss: 1.3623 9.4216 sec/batch\n",
      "Epoch 4/15  Iteration 1726/8505 Training loss: 1.3626 9.3414 sec/batch\n",
      "Epoch 4/15  Iteration 1727/8505 Training loss: 1.3616 9.3272 sec/batch\n",
      "Epoch 4/15  Iteration 1728/8505 Training loss: 1.3600 9.3142 sec/batch\n",
      "Epoch 4/15  Iteration 1729/8505 Training loss: 1.3582 9.3362 sec/batch\n",
      "Epoch 4/15  Iteration 1730/8505 Training loss: 1.3582 9.2575 sec/batch\n",
      "Epoch 4/15  Iteration 1731/8505 Training loss: 1.3576 9.3931 sec/batch\n",
      "Epoch 4/15  Iteration 1732/8505 Training loss: 1.3561 9.3110 sec/batch\n",
      "Epoch 4/15  Iteration 1733/8505 Training loss: 1.3549 9.3778 sec/batch\n",
      "Epoch 4/15  Iteration 1734/8505 Training loss: 1.3549 9.2487 sec/batch\n",
      "Epoch 4/15  Iteration 1735/8505 Training loss: 1.3551 9.2378 sec/batch\n",
      "Epoch 4/15  Iteration 1736/8505 Training loss: 1.3552 9.2778 sec/batch\n",
      "Epoch 4/15  Iteration 1737/8505 Training loss: 1.3546 9.6025 sec/batch\n",
      "Epoch 4/15  Iteration 1738/8505 Training loss: 1.3549 9.3206 sec/batch\n",
      "Epoch 4/15  Iteration 1739/8505 Training loss: 1.3547 9.2924 sec/batch\n",
      "Epoch 4/15  Iteration 1740/8505 Training loss: 1.3543 9.2563 sec/batch\n",
      "Epoch 4/15  Iteration 1741/8505 Training loss: 1.3533 9.2702 sec/batch\n",
      "Epoch 4/15  Iteration 1742/8505 Training loss: 1.3527 9.2509 sec/batch\n",
      "Epoch 4/15  Iteration 1743/8505 Training loss: 1.3525 9.3165 sec/batch\n",
      "Epoch 4/15  Iteration 1744/8505 Training loss: 1.3528 9.2836 sec/batch\n",
      "Epoch 4/15  Iteration 1745/8505 Training loss: 1.3520 9.3334 sec/batch\n",
      "Epoch 4/15  Iteration 1746/8505 Training loss: 1.3519 9.2543 sec/batch\n",
      "Epoch 4/15  Iteration 1747/8505 Training loss: 1.3521 9.5098 sec/batch\n",
      "Epoch 4/15  Iteration 1748/8505 Training loss: 1.3522 9.3613 sec/batch\n",
      "Epoch 4/15  Iteration 1749/8505 Training loss: 1.3521 9.2840 sec/batch\n",
      "Epoch 4/15  Iteration 1750/8505 Training loss: 1.3513 9.3704 sec/batch\n",
      "Epoch 4/15  Iteration 1751/8505 Training loss: 1.3503 9.4634 sec/batch\n",
      "Epoch 4/15  Iteration 1752/8505 Training loss: 1.3499 9.3551 sec/batch\n",
      "Epoch 4/15  Iteration 1753/8505 Training loss: 1.3502 9.2420 sec/batch\n",
      "Epoch 4/15  Iteration 1754/8505 Training loss: 1.3496 9.2985 sec/batch\n",
      "Epoch 4/15  Iteration 1755/8505 Training loss: 1.3497 9.2077 sec/batch\n",
      "Epoch 4/15  Iteration 1756/8505 Training loss: 1.3497 9.3351 sec/batch\n",
      "Epoch 4/15  Iteration 1757/8505 Training loss: 1.3493 9.2428 sec/batch\n",
      "Epoch 4/15  Iteration 1758/8505 Training loss: 1.3489 9.2557 sec/batch\n",
      "Epoch 4/15  Iteration 1759/8505 Training loss: 1.3489 9.4028 sec/batch\n",
      "Epoch 4/15  Iteration 1760/8505 Training loss: 1.3500 9.3736 sec/batch\n",
      "Epoch 4/15  Iteration 1761/8505 Training loss: 1.3504 9.3106 sec/batch\n",
      "Epoch 4/15  Iteration 1762/8505 Training loss: 1.3504 9.2506 sec/batch\n",
      "Epoch 4/15  Iteration 1763/8505 Training loss: 1.3504 9.3742 sec/batch\n",
      "Epoch 4/15  Iteration 1764/8505 Training loss: 1.3498 9.2220 sec/batch\n",
      "Epoch 4/15  Iteration 1765/8505 Training loss: 1.3495 9.3968 sec/batch\n",
      "Epoch 4/15  Iteration 1766/8505 Training loss: 1.3491 9.3557 sec/batch\n",
      "Epoch 4/15  Iteration 1767/8505 Training loss: 1.3489 9.3614 sec/batch\n",
      "Epoch 4/15  Iteration 1768/8505 Training loss: 1.3490 9.3691 sec/batch\n",
      "Epoch 4/15  Iteration 1769/8505 Training loss: 1.3490 9.3783 sec/batch\n",
      "Epoch 4/15  Iteration 1770/8505 Training loss: 1.3487 9.3822 sec/batch\n",
      "Epoch 4/15  Iteration 1771/8505 Training loss: 1.3485 9.3662 sec/batch\n",
      "Epoch 4/15  Iteration 1772/8505 Training loss: 1.3482 9.2342 sec/batch\n",
      "Epoch 4/15  Iteration 1773/8505 Training loss: 1.3481 9.1845 sec/batch\n",
      "Epoch 4/15  Iteration 1774/8505 Training loss: 1.3480 9.2703 sec/batch\n",
      "Epoch 4/15  Iteration 1775/8505 Training loss: 1.3483 9.3367 sec/batch\n",
      "Epoch 4/15  Iteration 1776/8505 Training loss: 1.3483 9.3196 sec/batch\n",
      "Epoch 4/15  Iteration 1777/8505 Training loss: 1.3482 9.2010 sec/batch\n",
      "Epoch 4/15  Iteration 1778/8505 Training loss: 1.3481 9.3237 sec/batch\n",
      "Epoch 4/15  Iteration 1779/8505 Training loss: 1.3479 9.2405 sec/batch\n",
      "Epoch 4/15  Iteration 1780/8505 Training loss: 1.3479 9.2751 sec/batch\n",
      "Epoch 4/15  Iteration 1781/8505 Training loss: 1.3473 9.2787 sec/batch\n",
      "Epoch 4/15  Iteration 1782/8505 Training loss: 1.3473 9.3854 sec/batch\n",
      "Epoch 4/15  Iteration 1783/8505 Training loss: 1.3467 9.2931 sec/batch\n",
      "Epoch 4/15  Iteration 1784/8505 Training loss: 1.3465 9.3269 sec/batch\n",
      "Epoch 4/15  Iteration 1785/8505 Training loss: 1.3461 9.2696 sec/batch\n",
      "Epoch 4/15  Iteration 1786/8505 Training loss: 1.3461 9.2621 sec/batch\n",
      "Epoch 4/15  Iteration 1787/8505 Training loss: 1.3460 9.2369 sec/batch\n",
      "Epoch 4/15  Iteration 1788/8505 Training loss: 1.3461 9.2508 sec/batch\n",
      "Epoch 4/15  Iteration 1789/8505 Training loss: 1.3460 9.2550 sec/batch\n",
      "Epoch 4/15  Iteration 1790/8505 Training loss: 1.3458 9.2884 sec/batch\n",
      "Epoch 4/15  Iteration 1791/8505 Training loss: 1.3456 9.4025 sec/batch\n",
      "Epoch 4/15  Iteration 1792/8505 Training loss: 1.3453 9.3217 sec/batch\n",
      "Epoch 4/15  Iteration 1793/8505 Training loss: 1.3452 9.3419 sec/batch\n",
      "Epoch 4/15  Iteration 1794/8505 Training loss: 1.3457 9.2516 sec/batch\n",
      "Epoch 4/15  Iteration 1795/8505 Training loss: 1.3459 9.3133 sec/batch\n",
      "Epoch 4/15  Iteration 1796/8505 Training loss: 1.3459 9.3385 sec/batch\n",
      "Epoch 4/15  Iteration 1797/8505 Training loss: 1.3463 10.2831 sec/batch\n",
      "Epoch 4/15  Iteration 1798/8505 Training loss: 1.3461 9.4160 sec/batch\n",
      "Epoch 4/15  Iteration 1799/8505 Training loss: 1.3458 9.2334 sec/batch\n",
      "Epoch 4/15  Iteration 1800/8505 Training loss: 1.3457 9.3293 sec/batch\n",
      "Validation loss: 1.28752 Saving checkpoint!\n",
      "Epoch 4/15  Iteration 1801/8505 Training loss: 1.3467 9.2659 sec/batch\n",
      "Epoch 4/15  Iteration 1802/8505 Training loss: 1.3465 9.2664 sec/batch\n",
      "Epoch 4/15  Iteration 1803/8505 Training loss: 1.3465 9.2401 sec/batch\n",
      "Epoch 4/15  Iteration 1804/8505 Training loss: 1.3466 9.3663 sec/batch\n",
      "Epoch 4/15  Iteration 1805/8505 Training loss: 1.3464 9.2293 sec/batch\n",
      "Epoch 4/15  Iteration 1806/8505 Training loss: 1.3467 9.2420 sec/batch\n",
      "Epoch 4/15  Iteration 1807/8505 Training loss: 1.3462 9.2999 sec/batch\n",
      "Epoch 4/15  Iteration 1808/8505 Training loss: 1.3461 9.3303 sec/batch\n",
      "Epoch 4/15  Iteration 1809/8505 Training loss: 1.3460 9.2957 sec/batch\n",
      "Epoch 4/15  Iteration 1810/8505 Training loss: 1.3456 9.3009 sec/batch\n",
      "Epoch 4/15  Iteration 1811/8505 Training loss: 1.3455 9.3689 sec/batch\n",
      "Epoch 4/15  Iteration 1812/8505 Training loss: 1.3455 9.3476 sec/batch\n",
      "Epoch 4/15  Iteration 1813/8505 Training loss: 1.3454 9.2615 sec/batch\n",
      "Epoch 4/15  Iteration 1814/8505 Training loss: 1.3452 9.3448 sec/batch\n",
      "Epoch 4/15  Iteration 1815/8505 Training loss: 1.3447 9.2459 sec/batch\n",
      "Epoch 4/15  Iteration 1816/8505 Training loss: 1.3445 9.3212 sec/batch\n",
      "Epoch 4/15  Iteration 1817/8505 Training loss: 1.3440 9.2627 sec/batch\n",
      "Epoch 4/15  Iteration 1818/8505 Training loss: 1.3438 9.3277 sec/batch\n",
      "Epoch 4/15  Iteration 1819/8505 Training loss: 1.3439 9.4904 sec/batch\n",
      "Epoch 4/15  Iteration 1820/8505 Training loss: 1.3438 9.3755 sec/batch\n",
      "Epoch 4/15  Iteration 1821/8505 Training loss: 1.3439 9.3792 sec/batch\n",
      "Epoch 4/15  Iteration 1822/8505 Training loss: 1.3441 9.2772 sec/batch\n",
      "Epoch 4/15  Iteration 1823/8505 Training loss: 1.3441 9.2593 sec/batch\n",
      "Epoch 4/15  Iteration 1824/8505 Training loss: 1.3442 9.2562 sec/batch\n",
      "Epoch 4/15  Iteration 1825/8505 Training loss: 1.3442 9.3141 sec/batch\n",
      "Epoch 4/15  Iteration 1826/8505 Training loss: 1.3441 9.1709 sec/batch\n",
      "Epoch 4/15  Iteration 1827/8505 Training loss: 1.3440 9.2847 sec/batch\n",
      "Epoch 4/15  Iteration 1828/8505 Training loss: 1.3438 9.2530 sec/batch\n",
      "Epoch 4/15  Iteration 1829/8505 Training loss: 1.3438 9.3262 sec/batch\n",
      "Epoch 4/15  Iteration 1830/8505 Training loss: 1.3439 9.3117 sec/batch\n",
      "Epoch 4/15  Iteration 1831/8505 Training loss: 1.3437 9.2421 sec/batch\n",
      "Epoch 4/15  Iteration 1832/8505 Training loss: 1.3436 9.3484 sec/batch\n",
      "Epoch 4/15  Iteration 1833/8505 Training loss: 1.3436 9.2721 sec/batch\n",
      "Epoch 4/15  Iteration 1834/8505 Training loss: 1.3439 9.5749 sec/batch\n",
      "Epoch 4/15  Iteration 1835/8505 Training loss: 1.3443 9.3965 sec/batch\n",
      "Epoch 4/15  Iteration 1836/8505 Training loss: 1.3446 9.3833 sec/batch\n",
      "Epoch 4/15  Iteration 1837/8505 Training loss: 1.3448 9.2610 sec/batch\n",
      "Epoch 4/15  Iteration 1838/8505 Training loss: 1.3447 9.3361 sec/batch\n",
      "Epoch 4/15  Iteration 1839/8505 Training loss: 1.3446 9.2680 sec/batch\n",
      "Epoch 4/15  Iteration 1840/8505 Training loss: 1.3443 9.3137 sec/batch\n",
      "Epoch 4/15  Iteration 1841/8505 Training loss: 1.3443 9.2431 sec/batch\n",
      "Epoch 4/15  Iteration 1842/8505 Training loss: 1.3441 9.2721 sec/batch\n",
      "Epoch 4/15  Iteration 1843/8505 Training loss: 1.3437 9.2578 sec/batch\n",
      "Epoch 4/15  Iteration 1844/8505 Training loss: 1.3435 9.2692 sec/batch\n",
      "Epoch 4/15  Iteration 1845/8505 Training loss: 1.3434 9.4178 sec/batch\n",
      "Epoch 4/15  Iteration 1846/8505 Training loss: 1.3432 9.3125 sec/batch\n",
      "Epoch 4/15  Iteration 1847/8505 Training loss: 1.3432 9.4425 sec/batch\n",
      "Epoch 4/15  Iteration 1848/8505 Training loss: 1.3430 9.2577 sec/batch\n",
      "Epoch 4/15  Iteration 1849/8505 Training loss: 1.3428 9.3198 sec/batch\n",
      "Epoch 4/15  Iteration 1850/8505 Training loss: 1.3426 9.2484 sec/batch\n",
      "Epoch 4/15  Iteration 1851/8505 Training loss: 1.3424 9.3147 sec/batch\n",
      "Epoch 4/15  Iteration 1852/8505 Training loss: 1.3422 9.3284 sec/batch\n",
      "Epoch 4/15  Iteration 1853/8505 Training loss: 1.3418 9.3553 sec/batch\n",
      "Epoch 4/15  Iteration 1854/8505 Training loss: 1.3415 9.2672 sec/batch\n",
      "Epoch 4/15  Iteration 1855/8505 Training loss: 1.3415 9.2435 sec/batch\n",
      "Epoch 4/15  Iteration 1856/8505 Training loss: 1.3412 9.1882 sec/batch\n",
      "Epoch 4/15  Iteration 1857/8505 Training loss: 1.3409 9.3689 sec/batch\n",
      "Epoch 4/15  Iteration 1858/8505 Training loss: 1.3412 9.3491 sec/batch\n",
      "Epoch 4/15  Iteration 1859/8505 Training loss: 1.3411 9.2520 sec/batch\n",
      "Epoch 4/15  Iteration 1860/8505 Training loss: 1.3413 9.2541 sec/batch\n",
      "Epoch 4/15  Iteration 1861/8505 Training loss: 1.3410 9.2572 sec/batch\n",
      "Epoch 4/15  Iteration 1862/8505 Training loss: 1.3410 9.3420 sec/batch\n",
      "Epoch 4/15  Iteration 1863/8505 Training loss: 1.3407 9.2308 sec/batch\n",
      "Epoch 4/15  Iteration 1864/8505 Training loss: 1.3407 9.2880 sec/batch\n",
      "Epoch 4/15  Iteration 1865/8505 Training loss: 1.3406 9.3240 sec/batch\n",
      "Epoch 4/15  Iteration 1866/8505 Training loss: 1.3406 9.2108 sec/batch\n",
      "Epoch 4/15  Iteration 1867/8505 Training loss: 1.3405 9.2578 sec/batch\n",
      "Epoch 4/15  Iteration 1868/8505 Training loss: 1.3404 9.2728 sec/batch\n",
      "Epoch 4/15  Iteration 1869/8505 Training loss: 1.3401 9.2587 sec/batch\n",
      "Epoch 4/15  Iteration 1870/8505 Training loss: 1.3402 9.3488 sec/batch\n",
      "Epoch 4/15  Iteration 1871/8505 Training loss: 1.3402 9.2957 sec/batch\n",
      "Epoch 4/15  Iteration 1872/8505 Training loss: 1.3405 9.2979 sec/batch\n",
      "Epoch 4/15  Iteration 1873/8505 Training loss: 1.3404 9.2787 sec/batch\n",
      "Epoch 4/15  Iteration 1874/8505 Training loss: 1.3402 9.3306 sec/batch\n",
      "Epoch 4/15  Iteration 1875/8505 Training loss: 1.3399 9.5239 sec/batch\n",
      "Epoch 4/15  Iteration 1876/8505 Training loss: 1.3399 9.3015 sec/batch\n",
      "Epoch 4/15  Iteration 1877/8505 Training loss: 1.3399 9.3190 sec/batch\n",
      "Epoch 4/15  Iteration 1878/8505 Training loss: 1.3399 9.2791 sec/batch\n",
      "Epoch 4/15  Iteration 1879/8505 Training loss: 1.3398 9.3835 sec/batch\n",
      "Epoch 4/15  Iteration 1880/8505 Training loss: 1.3397 9.3602 sec/batch\n",
      "Epoch 4/15  Iteration 1881/8505 Training loss: 1.3398 9.3854 sec/batch\n",
      "Epoch 4/15  Iteration 1882/8505 Training loss: 1.3396 9.2643 sec/batch\n",
      "Epoch 4/15  Iteration 1883/8505 Training loss: 1.3395 9.3265 sec/batch\n",
      "Epoch 4/15  Iteration 1884/8505 Training loss: 1.3395 9.2638 sec/batch\n",
      "Epoch 4/15  Iteration 1885/8505 Training loss: 1.3395 9.3750 sec/batch\n",
      "Epoch 4/15  Iteration 1886/8505 Training loss: 1.3395 9.2667 sec/batch\n",
      "Epoch 4/15  Iteration 1887/8505 Training loss: 1.3393 9.2628 sec/batch\n",
      "Epoch 4/15  Iteration 1888/8505 Training loss: 1.3392 9.2583 sec/batch\n",
      "Epoch 4/15  Iteration 1889/8505 Training loss: 1.3391 9.2342 sec/batch\n",
      "Epoch 4/15  Iteration 1890/8505 Training loss: 1.3392 9.2601 sec/batch\n",
      "Epoch 4/15  Iteration 1891/8505 Training loss: 1.3389 9.2428 sec/batch\n",
      "Epoch 4/15  Iteration 1892/8505 Training loss: 1.3389 9.2822 sec/batch\n",
      "Epoch 4/15  Iteration 1893/8505 Training loss: 1.3387 9.3089 sec/batch\n",
      "Epoch 4/15  Iteration 1894/8505 Training loss: 1.3389 9.3170 sec/batch\n",
      "Epoch 4/15  Iteration 1895/8505 Training loss: 1.3388 9.3138 sec/batch\n",
      "Epoch 4/15  Iteration 1896/8505 Training loss: 1.3387 9.2450 sec/batch\n",
      "Epoch 4/15  Iteration 1897/8505 Training loss: 1.3387 9.3069 sec/batch\n",
      "Epoch 4/15  Iteration 1898/8505 Training loss: 1.3385 9.2652 sec/batch\n",
      "Epoch 4/15  Iteration 1899/8505 Training loss: 1.3386 9.1924 sec/batch\n",
      "Epoch 4/15  Iteration 1900/8505 Training loss: 1.3387 9.3069 sec/batch\n",
      "Epoch 4/15  Iteration 1901/8505 Training loss: 1.3386 9.2752 sec/batch\n",
      "Epoch 4/15  Iteration 1902/8505 Training loss: 1.3386 10.6581 sec/batch\n",
      "Epoch 4/15  Iteration 1903/8505 Training loss: 1.3386 9.3523 sec/batch\n",
      "Epoch 4/15  Iteration 1904/8505 Training loss: 1.3386 9.2415 sec/batch\n",
      "Epoch 4/15  Iteration 1905/8505 Training loss: 1.3384 9.3458 sec/batch\n",
      "Epoch 4/15  Iteration 1906/8505 Training loss: 1.3382 9.3370 sec/batch\n",
      "Epoch 4/15  Iteration 1907/8505 Training loss: 1.3382 9.3650 sec/batch\n",
      "Epoch 4/15  Iteration 1908/8505 Training loss: 1.3381 9.5140 sec/batch\n",
      "Epoch 4/15  Iteration 1909/8505 Training loss: 1.3381 9.2418 sec/batch\n",
      "Epoch 4/15  Iteration 1910/8505 Training loss: 1.3378 9.2372 sec/batch\n",
      "Epoch 4/15  Iteration 1911/8505 Training loss: 1.3375 9.3772 sec/batch\n",
      "Epoch 4/15  Iteration 1912/8505 Training loss: 1.3373 9.2889 sec/batch\n",
      "Epoch 4/15  Iteration 1913/8505 Training loss: 1.3373 9.3025 sec/batch\n",
      "Epoch 4/15  Iteration 1914/8505 Training loss: 1.3371 9.2926 sec/batch\n",
      "Epoch 4/15  Iteration 1915/8505 Training loss: 1.3371 9.2354 sec/batch\n",
      "Epoch 4/15  Iteration 1916/8505 Training loss: 1.3370 9.2712 sec/batch\n",
      "Epoch 4/15  Iteration 1917/8505 Training loss: 1.3370 9.2833 sec/batch\n",
      "Epoch 4/15  Iteration 1918/8505 Training loss: 1.3368 9.3203 sec/batch\n",
      "Epoch 4/15  Iteration 1919/8505 Training loss: 1.3366 9.2566 sec/batch\n",
      "Epoch 4/15  Iteration 1920/8505 Training loss: 1.3364 9.3650 sec/batch\n",
      "Epoch 4/15  Iteration 1921/8505 Training loss: 1.3361 9.2749 sec/batch\n",
      "Epoch 4/15  Iteration 1922/8505 Training loss: 1.3359 9.2863 sec/batch\n",
      "Epoch 4/15  Iteration 1923/8505 Training loss: 1.3358 9.2984 sec/batch\n",
      "Epoch 4/15  Iteration 1924/8505 Training loss: 1.3356 9.2654 sec/batch\n",
      "Epoch 4/15  Iteration 1925/8505 Training loss: 1.3354 9.2552 sec/batch\n",
      "Epoch 4/15  Iteration 1926/8505 Training loss: 1.3353 9.3471 sec/batch\n",
      "Epoch 4/15  Iteration 1927/8505 Training loss: 1.3352 9.2187 sec/batch\n",
      "Epoch 4/15  Iteration 1928/8505 Training loss: 1.3350 9.3400 sec/batch\n",
      "Epoch 4/15  Iteration 1929/8505 Training loss: 1.3347 9.3086 sec/batch\n",
      "Epoch 4/15  Iteration 1930/8505 Training loss: 1.3347 9.2776 sec/batch\n",
      "Epoch 4/15  Iteration 1931/8505 Training loss: 1.3345 9.2180 sec/batch\n",
      "Epoch 4/15  Iteration 1932/8505 Training loss: 1.3343 9.2025 sec/batch\n",
      "Epoch 4/15  Iteration 1933/8505 Training loss: 1.3343 9.2852 sec/batch\n",
      "Epoch 4/15  Iteration 1934/8505 Training loss: 1.3342 9.2223 sec/batch\n",
      "Epoch 4/15  Iteration 1935/8505 Training loss: 1.3339 9.3478 sec/batch\n",
      "Epoch 4/15  Iteration 1936/8505 Training loss: 1.3338 9.2667 sec/batch\n",
      "Epoch 4/15  Iteration 1937/8505 Training loss: 1.3336 9.3198 sec/batch\n",
      "Epoch 4/15  Iteration 1938/8505 Training loss: 1.3336 9.2841 sec/batch\n",
      "Epoch 4/15  Iteration 1939/8505 Training loss: 1.3335 9.2454 sec/batch\n",
      "Epoch 4/15  Iteration 1940/8505 Training loss: 1.3332 9.2438 sec/batch\n",
      "Epoch 4/15  Iteration 1941/8505 Training loss: 1.3330 9.2240 sec/batch\n",
      "Epoch 4/15  Iteration 1942/8505 Training loss: 1.3328 9.2742 sec/batch\n",
      "Epoch 4/15  Iteration 1943/8505 Training loss: 1.3330 9.3141 sec/batch\n",
      "Epoch 4/15  Iteration 1944/8505 Training loss: 1.3328 9.2582 sec/batch\n",
      "Epoch 4/15  Iteration 1945/8505 Training loss: 1.3328 9.2230 sec/batch\n",
      "Epoch 4/15  Iteration 1946/8505 Training loss: 1.3328 9.2980 sec/batch\n",
      "Epoch 4/15  Iteration 1947/8505 Training loss: 1.3326 9.2642 sec/batch\n",
      "Epoch 4/15  Iteration 1948/8505 Training loss: 1.3326 9.2381 sec/batch\n",
      "Epoch 4/15  Iteration 1949/8505 Training loss: 1.3327 9.2312 sec/batch\n",
      "Epoch 4/15  Iteration 1950/8505 Training loss: 1.3327 9.3244 sec/batch\n",
      "Epoch 4/15  Iteration 1951/8505 Training loss: 1.3326 9.3054 sec/batch\n",
      "Epoch 4/15  Iteration 1952/8505 Training loss: 1.3326 9.3278 sec/batch\n",
      "Epoch 4/15  Iteration 1953/8505 Training loss: 1.3324 9.4006 sec/batch\n",
      "Epoch 4/15  Iteration 1954/8505 Training loss: 1.3322 9.2085 sec/batch\n",
      "Epoch 4/15  Iteration 1955/8505 Training loss: 1.3321 9.3947 sec/batch\n",
      "Epoch 4/15  Iteration 1956/8505 Training loss: 1.3321 9.2986 sec/batch\n",
      "Epoch 4/15  Iteration 1957/8505 Training loss: 1.3321 9.2645 sec/batch\n",
      "Epoch 4/15  Iteration 1958/8505 Training loss: 1.3322 9.3552 sec/batch\n",
      "Epoch 4/15  Iteration 1959/8505 Training loss: 1.3321 9.9548 sec/batch\n",
      "Epoch 4/15  Iteration 1960/8505 Training loss: 1.3319 9.2913 sec/batch\n",
      "Epoch 4/15  Iteration 1961/8505 Training loss: 1.3318 9.2146 sec/batch\n",
      "Epoch 4/15  Iteration 1962/8505 Training loss: 1.3317 9.3742 sec/batch\n",
      "Epoch 4/15  Iteration 1963/8505 Training loss: 1.3317 9.3691 sec/batch\n",
      "Epoch 4/15  Iteration 1964/8505 Training loss: 1.3317 9.2032 sec/batch\n",
      "Epoch 4/15  Iteration 1965/8505 Training loss: 1.3316 9.2964 sec/batch\n",
      "Epoch 4/15  Iteration 1966/8505 Training loss: 1.3315 9.2785 sec/batch\n",
      "Epoch 4/15  Iteration 1967/8505 Training loss: 1.3314 9.2945 sec/batch\n",
      "Epoch 4/15  Iteration 1968/8505 Training loss: 1.3313 9.2450 sec/batch\n",
      "Epoch 4/15  Iteration 1969/8505 Training loss: 1.3313 9.2796 sec/batch\n",
      "Epoch 4/15  Iteration 1970/8505 Training loss: 1.3312 9.2962 sec/batch\n",
      "Epoch 4/15  Iteration 1971/8505 Training loss: 1.3312 9.1858 sec/batch\n",
      "Epoch 4/15  Iteration 1972/8505 Training loss: 1.3311 9.2128 sec/batch\n",
      "Epoch 4/15  Iteration 1973/8505 Training loss: 1.3309 9.2236 sec/batch\n",
      "Epoch 4/15  Iteration 1974/8505 Training loss: 1.3309 9.4771 sec/batch\n",
      "Epoch 4/15  Iteration 1975/8505 Training loss: 1.3310 9.2490 sec/batch\n",
      "Epoch 4/15  Iteration 1976/8505 Training loss: 1.3309 9.3776 sec/batch\n",
      "Epoch 4/15  Iteration 1977/8505 Training loss: 1.3310 9.2461 sec/batch\n",
      "Epoch 4/15  Iteration 1978/8505 Training loss: 1.3309 9.3399 sec/batch\n",
      "Epoch 4/15  Iteration 1979/8505 Training loss: 1.3309 9.2346 sec/batch\n",
      "Epoch 4/15  Iteration 1980/8505 Training loss: 1.3308 9.3420 sec/batch\n",
      "Epoch 4/15  Iteration 1981/8505 Training loss: 1.3308 9.2425 sec/batch\n",
      "Epoch 4/15  Iteration 1982/8505 Training loss: 1.3305 9.2408 sec/batch\n",
      "Epoch 4/15  Iteration 1983/8505 Training loss: 1.3305 9.9911 sec/batch\n",
      "Epoch 4/15  Iteration 1984/8505 Training loss: 1.3304 9.3298 sec/batch\n",
      "Epoch 4/15  Iteration 1985/8505 Training loss: 1.3302 9.2996 sec/batch\n",
      "Epoch 4/15  Iteration 1986/8505 Training loss: 1.3301 9.3481 sec/batch\n",
      "Epoch 4/15  Iteration 1987/8505 Training loss: 1.3301 9.3954 sec/batch\n",
      "Epoch 4/15  Iteration 1988/8505 Training loss: 1.3301 9.2800 sec/batch\n",
      "Epoch 4/15  Iteration 1989/8505 Training loss: 1.3302 9.2196 sec/batch\n",
      "Epoch 4/15  Iteration 1990/8505 Training loss: 1.3301 9.3076 sec/batch\n",
      "Epoch 4/15  Iteration 1991/8505 Training loss: 1.3300 9.2736 sec/batch\n",
      "Epoch 4/15  Iteration 1992/8505 Training loss: 1.3299 9.3340 sec/batch\n",
      "Epoch 4/15  Iteration 1993/8505 Training loss: 1.3297 9.2294 sec/batch\n",
      "Epoch 4/15  Iteration 1994/8505 Training loss: 1.3296 9.3615 sec/batch\n",
      "Epoch 4/15  Iteration 1995/8505 Training loss: 1.3295 9.2526 sec/batch\n",
      "Epoch 4/15  Iteration 1996/8505 Training loss: 1.3296 9.2826 sec/batch\n",
      "Epoch 4/15  Iteration 1997/8505 Training loss: 1.3294 9.1729 sec/batch\n",
      "Epoch 4/15  Iteration 1998/8505 Training loss: 1.3293 9.2993 sec/batch\n",
      "Epoch 4/15  Iteration 1999/8505 Training loss: 1.3292 9.1945 sec/batch\n",
      "Epoch 4/15  Iteration 2000/8505 Training loss: 1.3292 9.3264 sec/batch\n",
      "Validation loss: 1.2676 Saving checkpoint!\n",
      "Epoch 4/15  Iteration 2001/8505 Training loss: 1.3295 9.3889 sec/batch\n",
      "Epoch 4/15  Iteration 2002/8505 Training loss: 1.3294 9.1753 sec/batch\n",
      "Epoch 4/15  Iteration 2003/8505 Training loss: 1.3293 9.3241 sec/batch\n",
      "Epoch 4/15  Iteration 2004/8505 Training loss: 1.3292 9.3122 sec/batch\n",
      "Epoch 4/15  Iteration 2005/8505 Training loss: 1.3291 9.2628 sec/batch\n",
      "Epoch 4/15  Iteration 2006/8505 Training loss: 1.3291 9.4927 sec/batch\n",
      "Epoch 4/15  Iteration 2007/8505 Training loss: 1.3290 9.3314 sec/batch\n",
      "Epoch 4/15  Iteration 2008/8505 Training loss: 1.3289 9.3309 sec/batch\n",
      "Epoch 4/15  Iteration 2009/8505 Training loss: 1.3288 9.2843 sec/batch\n",
      "Epoch 4/15  Iteration 2010/8505 Training loss: 1.3289 9.3110 sec/batch\n",
      "Epoch 4/15  Iteration 2011/8505 Training loss: 1.3288 9.3232 sec/batch\n",
      "Epoch 4/15  Iteration 2012/8505 Training loss: 1.3288 9.2081 sec/batch\n",
      "Epoch 4/15  Iteration 2013/8505 Training loss: 1.3287 9.2896 sec/batch\n",
      "Epoch 4/15  Iteration 2014/8505 Training loss: 1.3286 9.3599 sec/batch\n",
      "Epoch 4/15  Iteration 2015/8505 Training loss: 1.3286 9.2995 sec/batch\n",
      "Epoch 4/15  Iteration 2016/8505 Training loss: 1.3285 9.3391 sec/batch\n",
      "Epoch 4/15  Iteration 2017/8505 Training loss: 1.3284 9.3384 sec/batch\n",
      "Epoch 4/15  Iteration 2018/8505 Training loss: 1.3283 9.3124 sec/batch\n",
      "Epoch 4/15  Iteration 2019/8505 Training loss: 1.3283 9.2361 sec/batch\n",
      "Epoch 4/15  Iteration 2020/8505 Training loss: 1.3282 9.3479 sec/batch\n",
      "Epoch 4/15  Iteration 2021/8505 Training loss: 1.3282 9.2188 sec/batch\n",
      "Epoch 4/15  Iteration 2022/8505 Training loss: 1.3282 9.2303 sec/batch\n",
      "Epoch 4/15  Iteration 2023/8505 Training loss: 1.3281 9.3178 sec/batch\n",
      "Epoch 4/15  Iteration 2024/8505 Training loss: 1.3281 9.2481 sec/batch\n",
      "Epoch 4/15  Iteration 2025/8505 Training loss: 1.3281 9.4077 sec/batch\n",
      "Epoch 4/15  Iteration 2026/8505 Training loss: 1.3282 9.2665 sec/batch\n",
      "Epoch 4/15  Iteration 2027/8505 Training loss: 1.3281 9.2742 sec/batch\n",
      "Epoch 4/15  Iteration 2028/8505 Training loss: 1.3282 9.2485 sec/batch\n",
      "Epoch 4/15  Iteration 2029/8505 Training loss: 1.3281 9.2969 sec/batch\n",
      "Epoch 4/15  Iteration 2030/8505 Training loss: 1.3280 9.3520 sec/batch\n",
      "Epoch 4/15  Iteration 2031/8505 Training loss: 1.3280 9.2164 sec/batch\n",
      "Epoch 4/15  Iteration 2032/8505 Training loss: 1.3279 9.4059 sec/batch\n",
      "Epoch 4/15  Iteration 2033/8505 Training loss: 1.3279 9.3933 sec/batch\n",
      "Epoch 4/15  Iteration 2034/8505 Training loss: 1.3278 9.3380 sec/batch\n",
      "Epoch 4/15  Iteration 2035/8505 Training loss: 1.3277 9.3015 sec/batch\n",
      "Epoch 4/15  Iteration 2036/8505 Training loss: 1.3277 9.2449 sec/batch\n",
      "Epoch 4/15  Iteration 2037/8505 Training loss: 1.3276 9.2072 sec/batch\n",
      "Epoch 4/15  Iteration 2038/8505 Training loss: 1.3277 9.3145 sec/batch\n",
      "Epoch 4/15  Iteration 2039/8505 Training loss: 1.3278 9.3044 sec/batch\n",
      "Epoch 4/15  Iteration 2040/8505 Training loss: 1.3278 9.3450 sec/batch\n",
      "Epoch 4/15  Iteration 2041/8505 Training loss: 1.3277 9.2625 sec/batch\n",
      "Epoch 4/15  Iteration 2042/8505 Training loss: 1.3276 9.3209 sec/batch\n",
      "Epoch 4/15  Iteration 2043/8505 Training loss: 1.3275 9.2625 sec/batch\n",
      "Epoch 4/15  Iteration 2044/8505 Training loss: 1.3274 9.3955 sec/batch\n",
      "Epoch 4/15  Iteration 2045/8505 Training loss: 1.3274 9.3612 sec/batch\n",
      "Epoch 4/15  Iteration 2046/8505 Training loss: 1.3273 9.4593 sec/batch\n",
      "Epoch 4/15  Iteration 2047/8505 Training loss: 1.3273 9.4613 sec/batch\n",
      "Epoch 4/15  Iteration 2048/8505 Training loss: 1.3272 9.2583 sec/batch\n",
      "Epoch 4/15  Iteration 2049/8505 Training loss: 1.3272 9.2688 sec/batch\n",
      "Epoch 4/15  Iteration 2050/8505 Training loss: 1.3271 9.5357 sec/batch\n",
      "Epoch 4/15  Iteration 2051/8505 Training loss: 1.3271 9.5436 sec/batch\n",
      "Epoch 4/15  Iteration 2052/8505 Training loss: 1.3271 9.4530 sec/batch\n",
      "Epoch 4/15  Iteration 2053/8505 Training loss: 1.3270 10.1377 sec/batch\n",
      "Epoch 4/15  Iteration 2054/8505 Training loss: 1.3269 9.4198 sec/batch\n",
      "Epoch 4/15  Iteration 2055/8505 Training loss: 1.3268 9.5195 sec/batch\n",
      "Epoch 4/15  Iteration 2056/8505 Training loss: 1.3269 9.4033 sec/batch\n",
      "Epoch 4/15  Iteration 2057/8505 Training loss: 1.3268 10.0218 sec/batch\n",
      "Epoch 4/15  Iteration 2058/8505 Training loss: 1.3268 9.3501 sec/batch\n",
      "Epoch 4/15  Iteration 2059/8505 Training loss: 1.3267 9.2640 sec/batch\n",
      "Epoch 4/15  Iteration 2060/8505 Training loss: 1.3265 9.2645 sec/batch\n",
      "Epoch 4/15  Iteration 2061/8505 Training loss: 1.3264 9.3363 sec/batch\n",
      "Epoch 4/15  Iteration 2062/8505 Training loss: 1.3263 9.2882 sec/batch\n",
      "Epoch 4/15  Iteration 2063/8505 Training loss: 1.3263 9.2160 sec/batch\n",
      "Epoch 4/15  Iteration 2064/8505 Training loss: 1.3262 9.2578 sec/batch\n",
      "Epoch 4/15  Iteration 2065/8505 Training loss: 1.3261 9.3522 sec/batch\n",
      "Epoch 4/15  Iteration 2066/8505 Training loss: 1.3262 9.2559 sec/batch\n",
      "Epoch 4/15  Iteration 2067/8505 Training loss: 1.3261 9.2026 sec/batch\n",
      "Epoch 4/15  Iteration 2068/8505 Training loss: 1.3262 9.3296 sec/batch\n",
      "Epoch 4/15  Iteration 2069/8505 Training loss: 1.3261 9.2274 sec/batch\n",
      "Epoch 4/15  Iteration 2070/8505 Training loss: 1.3260 9.2896 sec/batch\n",
      "Epoch 4/15  Iteration 2071/8505 Training loss: 1.3259 9.1685 sec/batch\n",
      "Epoch 4/15  Iteration 2072/8505 Training loss: 1.3259 9.2571 sec/batch\n",
      "Epoch 4/15  Iteration 2073/8505 Training loss: 1.3258 9.3865 sec/batch\n",
      "Epoch 4/15  Iteration 2074/8505 Training loss: 1.3258 9.3442 sec/batch\n",
      "Epoch 4/15  Iteration 2075/8505 Training loss: 1.3258 9.3571 sec/batch\n",
      "Epoch 4/15  Iteration 2076/8505 Training loss: 1.3258 9.1823 sec/batch\n",
      "Epoch 4/15  Iteration 2077/8505 Training loss: 1.3258 10.5122 sec/batch\n",
      "Epoch 4/15  Iteration 2078/8505 Training loss: 1.3256 9.2257 sec/batch\n",
      "Epoch 4/15  Iteration 2079/8505 Training loss: 1.3256 9.3942 sec/batch\n",
      "Epoch 4/15  Iteration 2080/8505 Training loss: 1.3257 9.4288 sec/batch\n",
      "Epoch 4/15  Iteration 2081/8505 Training loss: 1.3257 9.4992 sec/batch\n",
      "Epoch 4/15  Iteration 2082/8505 Training loss: 1.3257 9.2827 sec/batch\n",
      "Epoch 4/15  Iteration 2083/8505 Training loss: 1.3256 9.3725 sec/batch\n",
      "Epoch 4/15  Iteration 2084/8505 Training loss: 1.3255 9.2674 sec/batch\n",
      "Epoch 4/15  Iteration 2085/8505 Training loss: 1.3254 9.3318 sec/batch\n",
      "Epoch 4/15  Iteration 2086/8505 Training loss: 1.3253 9.2358 sec/batch\n",
      "Epoch 4/15  Iteration 2087/8505 Training loss: 1.3251 9.2436 sec/batch\n",
      "Epoch 4/15  Iteration 2088/8505 Training loss: 1.3250 9.3021 sec/batch\n",
      "Epoch 4/15  Iteration 2089/8505 Training loss: 1.3248 9.3505 sec/batch\n",
      "Epoch 4/15  Iteration 2090/8505 Training loss: 1.3247 9.3175 sec/batch\n",
      "Epoch 4/15  Iteration 2091/8505 Training loss: 1.3246 9.2837 sec/batch\n",
      "Epoch 4/15  Iteration 2092/8505 Training loss: 1.3245 9.3348 sec/batch\n",
      "Epoch 4/15  Iteration 2093/8505 Training loss: 1.3243 9.1971 sec/batch\n",
      "Epoch 4/15  Iteration 2094/8505 Training loss: 1.3243 9.3296 sec/batch\n",
      "Epoch 4/15  Iteration 2095/8505 Training loss: 1.3242 9.5589 sec/batch\n",
      "Epoch 4/15  Iteration 2096/8505 Training loss: 1.3242 9.3617 sec/batch\n",
      "Epoch 4/15  Iteration 2097/8505 Training loss: 1.3241 9.3253 sec/batch\n",
      "Epoch 4/15  Iteration 2098/8505 Training loss: 1.3241 9.2451 sec/batch\n",
      "Epoch 4/15  Iteration 2099/8505 Training loss: 1.3241 9.4120 sec/batch\n",
      "Epoch 4/15  Iteration 2100/8505 Training loss: 1.3241 9.3389 sec/batch\n",
      "Epoch 4/15  Iteration 2101/8505 Training loss: 1.3240 9.2342 sec/batch\n",
      "Epoch 4/15  Iteration 2102/8505 Training loss: 1.3239 9.2528 sec/batch\n",
      "Epoch 4/15  Iteration 2103/8505 Training loss: 1.3237 9.3187 sec/batch\n",
      "Epoch 4/15  Iteration 2104/8505 Training loss: 1.3236 9.3035 sec/batch\n",
      "Epoch 4/15  Iteration 2105/8505 Training loss: 1.3235 9.2781 sec/batch\n",
      "Epoch 4/15  Iteration 2106/8505 Training loss: 1.3234 9.3200 sec/batch\n",
      "Epoch 4/15  Iteration 2107/8505 Training loss: 1.3233 9.3061 sec/batch\n",
      "Epoch 4/15  Iteration 2108/8505 Training loss: 1.3233 9.3195 sec/batch\n",
      "Epoch 4/15  Iteration 2109/8505 Training loss: 1.3232 9.5469 sec/batch\n",
      "Epoch 4/15  Iteration 2110/8505 Training loss: 1.3231 9.3223 sec/batch\n",
      "Epoch 4/15  Iteration 2111/8505 Training loss: 1.3230 9.3061 sec/batch\n",
      "Epoch 4/15  Iteration 2112/8505 Training loss: 1.3230 9.2316 sec/batch\n",
      "Epoch 4/15  Iteration 2113/8505 Training loss: 1.3230 9.3209 sec/batch\n",
      "Epoch 4/15  Iteration 2114/8505 Training loss: 1.3230 9.3245 sec/batch\n",
      "Epoch 4/15  Iteration 2115/8505 Training loss: 1.3229 9.3282 sec/batch\n",
      "Epoch 4/15  Iteration 2116/8505 Training loss: 1.3227 9.2310 sec/batch\n",
      "Epoch 4/15  Iteration 2117/8505 Training loss: 1.3225 9.2696 sec/batch\n",
      "Epoch 4/15  Iteration 2118/8505 Training loss: 1.3225 9.3110 sec/batch\n",
      "Epoch 4/15  Iteration 2119/8505 Training loss: 1.3224 9.2288 sec/batch\n",
      "Epoch 4/15  Iteration 2120/8505 Training loss: 1.3224 9.2464 sec/batch\n",
      "Epoch 4/15  Iteration 2121/8505 Training loss: 1.3223 9.2354 sec/batch\n",
      "Epoch 4/15  Iteration 2122/8505 Training loss: 1.3223 9.4066 sec/batch\n",
      "Epoch 4/15  Iteration 2123/8505 Training loss: 1.3222 9.2613 sec/batch\n",
      "Epoch 4/15  Iteration 2124/8505 Training loss: 1.3221 9.2340 sec/batch\n",
      "Epoch 4/15  Iteration 2125/8505 Training loss: 1.3221 9.3517 sec/batch\n",
      "Epoch 4/15  Iteration 2126/8505 Training loss: 1.3220 9.2495 sec/batch\n",
      "Epoch 4/15  Iteration 2127/8505 Training loss: 1.3220 9.2731 sec/batch\n",
      "Epoch 4/15  Iteration 2128/8505 Training loss: 1.3219 9.2583 sec/batch\n",
      "Epoch 4/15  Iteration 2129/8505 Training loss: 1.3219 9.2754 sec/batch\n",
      "Epoch 4/15  Iteration 2130/8505 Training loss: 1.3219 9.2393 sec/batch\n",
      "Epoch 4/15  Iteration 2131/8505 Training loss: 1.3218 9.3252 sec/batch\n",
      "Epoch 4/15  Iteration 2132/8505 Training loss: 1.3218 9.2871 sec/batch\n",
      "Epoch 4/15  Iteration 2133/8505 Training loss: 1.3216 9.2614 sec/batch\n",
      "Epoch 4/15  Iteration 2134/8505 Training loss: 1.3216 9.2665 sec/batch\n",
      "Epoch 4/15  Iteration 2135/8505 Training loss: 1.3215 9.2378 sec/batch\n",
      "Epoch 4/15  Iteration 2136/8505 Training loss: 1.3214 9.2709 sec/batch\n",
      "Epoch 4/15  Iteration 2137/8505 Training loss: 1.3213 9.4056 sec/batch\n",
      "Epoch 4/15  Iteration 2138/8505 Training loss: 1.3212 9.4774 sec/batch\n",
      "Epoch 4/15  Iteration 2139/8505 Training loss: 1.3210 9.4639 sec/batch\n",
      "Epoch 4/15  Iteration 2140/8505 Training loss: 1.3209 9.3539 sec/batch\n",
      "Epoch 4/15  Iteration 2141/8505 Training loss: 1.3210 9.2875 sec/batch\n",
      "Epoch 4/15  Iteration 2142/8505 Training loss: 1.3209 9.3487 sec/batch\n",
      "Epoch 4/15  Iteration 2143/8505 Training loss: 1.3208 9.2963 sec/batch\n",
      "Epoch 4/15  Iteration 2144/8505 Training loss: 1.3207 9.1946 sec/batch\n",
      "Epoch 4/15  Iteration 2145/8505 Training loss: 1.3206 9.2894 sec/batch\n",
      "Epoch 4/15  Iteration 2146/8505 Training loss: 1.3206 9.3018 sec/batch\n",
      "Epoch 4/15  Iteration 2147/8505 Training loss: 1.3205 9.3014 sec/batch\n",
      "Epoch 4/15  Iteration 2148/8505 Training loss: 1.3204 9.2468 sec/batch\n",
      "Epoch 4/15  Iteration 2149/8505 Training loss: 1.3203 9.2644 sec/batch\n",
      "Epoch 4/15  Iteration 2150/8505 Training loss: 1.3202 9.2424 sec/batch\n",
      "Epoch 4/15  Iteration 2151/8505 Training loss: 1.3202 9.2407 sec/batch\n",
      "Epoch 4/15  Iteration 2152/8505 Training loss: 1.3201 9.2788 sec/batch\n",
      "Epoch 4/15  Iteration 2153/8505 Training loss: 1.3200 9.2209 sec/batch\n",
      "Epoch 4/15  Iteration 2154/8505 Training loss: 1.3200 9.3282 sec/batch\n",
      "Epoch 4/15  Iteration 2155/8505 Training loss: 1.3199 9.2675 sec/batch\n",
      "Epoch 4/15  Iteration 2156/8505 Training loss: 1.3198 9.2858 sec/batch\n",
      "Epoch 4/15  Iteration 2157/8505 Training loss: 1.3198 9.2878 sec/batch\n",
      "Epoch 4/15  Iteration 2158/8505 Training loss: 1.3197 9.2644 sec/batch\n",
      "Epoch 4/15  Iteration 2159/8505 Training loss: 1.3196 9.3154 sec/batch\n",
      "Epoch 4/15  Iteration 2160/8505 Training loss: 1.3195 9.2616 sec/batch\n",
      "Epoch 4/15  Iteration 2161/8505 Training loss: 1.3194 9.2900 sec/batch\n",
      "Epoch 4/15  Iteration 2162/8505 Training loss: 1.3194 9.2795 sec/batch\n",
      "Epoch 4/15  Iteration 2163/8505 Training loss: 1.3192 9.2792 sec/batch\n",
      "Epoch 4/15  Iteration 2164/8505 Training loss: 1.3192 9.3166 sec/batch\n",
      "Epoch 4/15  Iteration 2165/8505 Training loss: 1.3191 9.3417 sec/batch\n",
      "Epoch 4/15  Iteration 2166/8505 Training loss: 1.3192 9.2747 sec/batch\n",
      "Epoch 4/15  Iteration 2167/8505 Training loss: 1.3191 9.2843 sec/batch\n",
      "Epoch 4/15  Iteration 2168/8505 Training loss: 1.3190 9.2242 sec/batch\n",
      "Epoch 4/15  Iteration 2169/8505 Training loss: 1.3190 9.3986 sec/batch\n",
      "Epoch 4/15  Iteration 2170/8505 Training loss: 1.3189 9.2880 sec/batch\n",
      "Epoch 4/15  Iteration 2171/8505 Training loss: 1.3188 9.2943 sec/batch\n",
      "Epoch 4/15  Iteration 2172/8505 Training loss: 1.3187 9.4242 sec/batch\n",
      "Epoch 4/15  Iteration 2173/8505 Training loss: 1.3186 9.2863 sec/batch\n",
      "Epoch 4/15  Iteration 2174/8505 Training loss: 1.3185 9.2892 sec/batch\n",
      "Epoch 4/15  Iteration 2175/8505 Training loss: 1.3185 9.2917 sec/batch\n",
      "Epoch 4/15  Iteration 2176/8505 Training loss: 1.3184 9.3143 sec/batch\n",
      "Epoch 4/15  Iteration 2177/8505 Training loss: 1.3184 9.2385 sec/batch\n",
      "Epoch 4/15  Iteration 2178/8505 Training loss: 1.3183 9.2256 sec/batch\n",
      "Epoch 4/15  Iteration 2179/8505 Training loss: 1.3183 9.2412 sec/batch\n",
      "Epoch 4/15  Iteration 2180/8505 Training loss: 1.3181 9.3105 sec/batch\n",
      "Epoch 4/15  Iteration 2181/8505 Training loss: 1.3180 9.2078 sec/batch\n",
      "Epoch 4/15  Iteration 2182/8505 Training loss: 1.3179 9.2641 sec/batch\n",
      "Epoch 4/15  Iteration 2183/8505 Training loss: 1.3178 9.2915 sec/batch\n",
      "Epoch 4/15  Iteration 2184/8505 Training loss: 1.3177 9.1950 sec/batch\n",
      "Epoch 4/15  Iteration 2185/8505 Training loss: 1.3177 9.2350 sec/batch\n",
      "Epoch 4/15  Iteration 2186/8505 Training loss: 1.3176 9.2350 sec/batch\n",
      "Epoch 4/15  Iteration 2187/8505 Training loss: 1.3176 9.2634 sec/batch\n",
      "Epoch 4/15  Iteration 2188/8505 Training loss: 1.3175 9.2840 sec/batch\n",
      "Epoch 4/15  Iteration 2189/8505 Training loss: 1.3174 9.3240 sec/batch\n",
      "Epoch 4/15  Iteration 2190/8505 Training loss: 1.3174 9.2517 sec/batch\n",
      "Epoch 4/15  Iteration 2191/8505 Training loss: 1.3173 9.3500 sec/batch\n",
      "Epoch 4/15  Iteration 2192/8505 Training loss: 1.3173 9.1516 sec/batch\n",
      "Epoch 4/15  Iteration 2193/8505 Training loss: 1.3172 9.3146 sec/batch\n",
      "Epoch 4/15  Iteration 2194/8505 Training loss: 1.3171 9.2539 sec/batch\n",
      "Epoch 4/15  Iteration 2195/8505 Training loss: 1.3171 9.3848 sec/batch\n",
      "Epoch 4/15  Iteration 2196/8505 Training loss: 1.3170 9.2177 sec/batch\n",
      "Epoch 4/15  Iteration 2197/8505 Training loss: 1.3169 9.2416 sec/batch\n",
      "Epoch 4/15  Iteration 2198/8505 Training loss: 1.3169 9.2612 sec/batch\n",
      "Epoch 4/15  Iteration 2199/8505 Training loss: 1.3169 9.2423 sec/batch\n",
      "Epoch 4/15  Iteration 2200/8505 Training loss: 1.3167 9.2562 sec/batch\n",
      "Validation loss: 1.25073 Saving checkpoint!\n",
      "Epoch 4/15  Iteration 2201/8505 Training loss: 1.3168 11.2643 sec/batch\n",
      "Epoch 4/15  Iteration 2202/8505 Training loss: 1.3166 9.2796 sec/batch\n",
      "Epoch 4/15  Iteration 2203/8505 Training loss: 1.3165 9.2633 sec/batch\n",
      "Epoch 4/15  Iteration 2204/8505 Training loss: 1.3164 9.4401 sec/batch\n",
      "Epoch 4/15  Iteration 2205/8505 Training loss: 1.3163 9.2812 sec/batch\n",
      "Epoch 4/15  Iteration 2206/8505 Training loss: 1.3162 9.2748 sec/batch\n",
      "Epoch 4/15  Iteration 2207/8505 Training loss: 1.3161 9.2700 sec/batch\n",
      "Epoch 4/15  Iteration 2208/8505 Training loss: 1.3161 9.2426 sec/batch\n",
      "Epoch 4/15  Iteration 2209/8505 Training loss: 1.3160 9.2862 sec/batch\n",
      "Epoch 4/15  Iteration 2210/8505 Training loss: 1.3158 9.2625 sec/batch\n",
      "Epoch 4/15  Iteration 2211/8505 Training loss: 1.3156 9.2674 sec/batch\n",
      "Epoch 4/15  Iteration 2212/8505 Training loss: 1.3155 9.2552 sec/batch\n",
      "Epoch 4/15  Iteration 2213/8505 Training loss: 1.3153 9.2372 sec/batch\n",
      "Epoch 4/15  Iteration 2214/8505 Training loss: 1.3151 9.3429 sec/batch\n",
      "Epoch 4/15  Iteration 2215/8505 Training loss: 1.3151 9.2967 sec/batch\n",
      "Epoch 4/15  Iteration 2216/8505 Training loss: 1.3150 9.2390 sec/batch\n",
      "Epoch 4/15  Iteration 2217/8505 Training loss: 1.3149 9.3732 sec/batch\n",
      "Epoch 4/15  Iteration 2218/8505 Training loss: 1.3148 9.2691 sec/batch\n",
      "Epoch 4/15  Iteration 2219/8505 Training loss: 1.3147 9.2567 sec/batch\n",
      "Epoch 4/15  Iteration 2220/8505 Training loss: 1.3145 9.2991 sec/batch\n",
      "Epoch 4/15  Iteration 2221/8505 Training loss: 1.3144 9.2516 sec/batch\n",
      "Epoch 4/15  Iteration 2222/8505 Training loss: 1.3143 9.2976 sec/batch\n",
      "Epoch 4/15  Iteration 2223/8505 Training loss: 1.3142 9.3735 sec/batch\n",
      "Epoch 4/15  Iteration 2224/8505 Training loss: 1.3141 9.2825 sec/batch\n",
      "Epoch 4/15  Iteration 2225/8505 Training loss: 1.3139 9.2767 sec/batch\n",
      "Epoch 4/15  Iteration 2226/8505 Training loss: 1.3138 9.3389 sec/batch\n",
      "Epoch 4/15  Iteration 2227/8505 Training loss: 1.3137 9.3375 sec/batch\n",
      "Epoch 4/15  Iteration 2228/8505 Training loss: 1.3135 9.2923 sec/batch\n",
      "Epoch 4/15  Iteration 2229/8505 Training loss: 1.3133 9.3180 sec/batch\n",
      "Epoch 4/15  Iteration 2230/8505 Training loss: 1.3133 9.2759 sec/batch\n",
      "Epoch 4/15  Iteration 2231/8505 Training loss: 1.3132 9.2148 sec/batch\n",
      "Epoch 4/15  Iteration 2232/8505 Training loss: 1.3131 9.3450 sec/batch\n",
      "Epoch 4/15  Iteration 2233/8505 Training loss: 1.3131 9.3184 sec/batch\n",
      "Epoch 4/15  Iteration 2234/8505 Training loss: 1.3130 9.2161 sec/batch\n",
      "Epoch 4/15  Iteration 2235/8505 Training loss: 1.3128 9.3358 sec/batch\n",
      "Epoch 4/15  Iteration 2236/8505 Training loss: 1.3128 9.2664 sec/batch\n",
      "Epoch 4/15  Iteration 2237/8505 Training loss: 1.3127 9.2506 sec/batch\n",
      "Epoch 4/15  Iteration 2238/8505 Training loss: 1.3126 9.2667 sec/batch\n",
      "Epoch 4/15  Iteration 2239/8505 Training loss: 1.3125 9.5158 sec/batch\n",
      "Epoch 4/15  Iteration 2240/8505 Training loss: 1.3124 9.4520 sec/batch\n",
      "Epoch 4/15  Iteration 2241/8505 Training loss: 1.3123 9.2089 sec/batch\n",
      "Epoch 4/15  Iteration 2242/8505 Training loss: 1.3123 9.2409 sec/batch\n",
      "Epoch 4/15  Iteration 2243/8505 Training loss: 1.3122 9.3207 sec/batch\n",
      "Epoch 4/15  Iteration 2244/8505 Training loss: 1.3121 9.2475 sec/batch\n",
      "Epoch 4/15  Iteration 2245/8505 Training loss: 1.3121 9.2872 sec/batch\n",
      "Epoch 4/15  Iteration 2246/8505 Training loss: 1.3120 9.2350 sec/batch\n",
      "Epoch 4/15  Iteration 2247/8505 Training loss: 1.3120 9.3755 sec/batch\n",
      "Epoch 4/15  Iteration 2248/8505 Training loss: 1.3119 9.2612 sec/batch\n",
      "Epoch 4/15  Iteration 2249/8505 Training loss: 1.3119 9.2821 sec/batch\n",
      "Epoch 4/15  Iteration 2250/8505 Training loss: 1.3118 9.3332 sec/batch\n",
      "Epoch 4/15  Iteration 2251/8505 Training loss: 1.3117 9.2661 sec/batch\n",
      "Epoch 4/15  Iteration 2252/8505 Training loss: 1.3117 9.2770 sec/batch\n",
      "Epoch 4/15  Iteration 2253/8505 Training loss: 1.3116 9.2156 sec/batch\n",
      "Epoch 4/15  Iteration 2254/8505 Training loss: 1.3115 9.2876 sec/batch\n",
      "Epoch 4/15  Iteration 2255/8505 Training loss: 1.3116 9.2735 sec/batch\n",
      "Epoch 4/15  Iteration 2256/8505 Training loss: 1.3115 9.2357 sec/batch\n",
      "Epoch 4/15  Iteration 2257/8505 Training loss: 1.3114 9.2807 sec/batch\n",
      "Epoch 4/15  Iteration 2258/8505 Training loss: 1.3114 9.3097 sec/batch\n",
      "Epoch 4/15  Iteration 2259/8505 Training loss: 1.3114 9.3491 sec/batch\n",
      "Epoch 4/15  Iteration 2260/8505 Training loss: 1.3115 9.3918 sec/batch\n",
      "Epoch 4/15  Iteration 2261/8505 Training loss: 1.3114 9.3277 sec/batch\n",
      "Epoch 4/15  Iteration 2262/8505 Training loss: 1.3114 9.2725 sec/batch\n",
      "Epoch 4/15  Iteration 2263/8505 Training loss: 1.3113 9.2831 sec/batch\n",
      "Epoch 4/15  Iteration 2264/8505 Training loss: 1.3112 9.3234 sec/batch\n",
      "Epoch 4/15  Iteration 2265/8505 Training loss: 1.3111 9.2719 sec/batch\n",
      "Epoch 4/15  Iteration 2266/8505 Training loss: 1.3109 9.2353 sec/batch\n",
      "Epoch 4/15  Iteration 2267/8505 Training loss: 1.3109 9.2687 sec/batch\n",
      "Epoch 4/15  Iteration 2268/8505 Training loss: 1.3108 9.2668 sec/batch\n",
      "Epoch 5/15  Iteration 2269/8505 Training loss: 1.4127 9.2888 sec/batch\n",
      "Epoch 5/15  Iteration 2270/8505 Training loss: 1.3496 9.1942 sec/batch\n",
      "Epoch 5/15  Iteration 2271/8505 Training loss: 1.3331 9.2785 sec/batch\n",
      "Epoch 5/15  Iteration 2272/8505 Training loss: 1.3294 9.2207 sec/batch\n",
      "Epoch 5/15  Iteration 2273/8505 Training loss: 1.3274 9.3494 sec/batch\n",
      "Epoch 5/15  Iteration 2274/8505 Training loss: 1.3197 9.2447 sec/batch\n",
      "Epoch 5/15  Iteration 2275/8505 Training loss: 1.3116 9.3018 sec/batch\n",
      "Epoch 5/15  Iteration 2276/8505 Training loss: 1.3057 9.3290 sec/batch\n",
      "Epoch 5/15  Iteration 2277/8505 Training loss: 1.3055 9.2112 sec/batch\n",
      "Epoch 5/15  Iteration 2278/8505 Training loss: 1.2947 9.3152 sec/batch\n",
      "Epoch 5/15  Iteration 2279/8505 Training loss: 1.2972 9.3428 sec/batch\n",
      "Epoch 5/15  Iteration 2280/8505 Training loss: 1.2979 9.3025 sec/batch\n",
      "Epoch 5/15  Iteration 2281/8505 Training loss: 1.2968 9.2159 sec/batch\n",
      "Epoch 5/15  Iteration 2282/8505 Training loss: 1.2981 9.2401 sec/batch\n",
      "Epoch 5/15  Iteration 2283/8505 Training loss: 1.2969 9.2812 sec/batch\n",
      "Epoch 5/15  Iteration 2284/8505 Training loss: 1.2959 9.3191 sec/batch\n",
      "Epoch 5/15  Iteration 2285/8505 Training loss: 1.2950 9.2870 sec/batch\n",
      "Epoch 5/15  Iteration 2286/8505 Training loss: 1.2953 9.3283 sec/batch\n",
      "Epoch 5/15  Iteration 2287/8505 Training loss: 1.2934 9.3334 sec/batch\n",
      "Epoch 5/15  Iteration 2288/8505 Training loss: 1.2931 9.2467 sec/batch\n",
      "Epoch 5/15  Iteration 2289/8505 Training loss: 1.2930 9.2456 sec/batch\n",
      "Epoch 5/15  Iteration 2290/8505 Training loss: 1.2914 9.3504 sec/batch\n",
      "Epoch 5/15  Iteration 2291/8505 Training loss: 1.2899 9.3068 sec/batch\n",
      "Epoch 5/15  Iteration 2292/8505 Training loss: 1.2902 9.2587 sec/batch\n",
      "Epoch 5/15  Iteration 2293/8505 Training loss: 1.2909 9.3068 sec/batch\n",
      "Epoch 5/15  Iteration 2294/8505 Training loss: 1.2901 9.2469 sec/batch\n",
      "Epoch 5/15  Iteration 2295/8505 Training loss: 1.2885 9.4207 sec/batch\n",
      "Epoch 5/15  Iteration 2296/8505 Training loss: 1.2864 9.2703 sec/batch\n",
      "Epoch 5/15  Iteration 2297/8505 Training loss: 1.2866 9.2606 sec/batch\n",
      "Epoch 5/15  Iteration 2298/8505 Training loss: 1.2862 9.2550 sec/batch\n",
      "Epoch 5/15  Iteration 2299/8505 Training loss: 1.2850 9.2656 sec/batch\n",
      "Epoch 5/15  Iteration 2300/8505 Training loss: 1.2839 9.3067 sec/batch\n",
      "Epoch 5/15  Iteration 2301/8505 Training loss: 1.2839 9.1950 sec/batch\n",
      "Epoch 5/15  Iteration 2302/8505 Training loss: 1.2845 9.2800 sec/batch\n",
      "Epoch 5/15  Iteration 2303/8505 Training loss: 1.2841 9.2979 sec/batch\n",
      "Epoch 5/15  Iteration 2304/8505 Training loss: 1.2835 9.3654 sec/batch\n",
      "Epoch 5/15  Iteration 2305/8505 Training loss: 1.2838 9.3040 sec/batch\n",
      "Epoch 5/15  Iteration 2306/8505 Training loss: 1.2839 9.2713 sec/batch\n",
      "Epoch 5/15  Iteration 2307/8505 Training loss: 1.2835 9.4017 sec/batch\n",
      "Epoch 5/15  Iteration 2308/8505 Training loss: 1.2825 9.3956 sec/batch\n",
      "Epoch 5/15  Iteration 2309/8505 Training loss: 1.2820 9.3372 sec/batch\n",
      "Epoch 5/15  Iteration 2310/8505 Training loss: 1.2820 9.3619 sec/batch\n",
      "Epoch 5/15  Iteration 2311/8505 Training loss: 1.2825 9.2646 sec/batch\n",
      "Epoch 5/15  Iteration 2312/8505 Training loss: 1.2818 9.2617 sec/batch\n",
      "Epoch 5/15  Iteration 2313/8505 Training loss: 1.2818 9.2069 sec/batch\n",
      "Epoch 5/15  Iteration 2314/8505 Training loss: 1.2819 9.5877 sec/batch\n",
      "Epoch 5/15  Iteration 2315/8505 Training loss: 1.2822 9.2660 sec/batch\n",
      "Epoch 5/15  Iteration 2316/8505 Training loss: 1.2820 9.2021 sec/batch\n",
      "Epoch 5/15  Iteration 2317/8505 Training loss: 1.2815 9.2967 sec/batch\n",
      "Epoch 5/15  Iteration 2318/8505 Training loss: 1.2805 9.2391 sec/batch\n",
      "Epoch 5/15  Iteration 2319/8505 Training loss: 1.2800 9.2321 sec/batch\n",
      "Epoch 5/15  Iteration 2320/8505 Training loss: 1.2802 9.2076 sec/batch\n",
      "Epoch 5/15  Iteration 2321/8505 Training loss: 1.2797 9.3315 sec/batch\n",
      "Epoch 5/15  Iteration 2322/8505 Training loss: 1.2798 9.2770 sec/batch\n",
      "Epoch 5/15  Iteration 2323/8505 Training loss: 1.2800 9.2384 sec/batch\n",
      "Epoch 5/15  Iteration 2324/8505 Training loss: 1.2795 9.2551 sec/batch\n",
      "Epoch 5/15  Iteration 2325/8505 Training loss: 1.2793 9.3542 sec/batch\n",
      "Epoch 5/15  Iteration 2326/8505 Training loss: 1.2794 9.3585 sec/batch\n",
      "Epoch 5/15  Iteration 2327/8505 Training loss: 1.2804 9.2698 sec/batch\n",
      "Epoch 5/15  Iteration 2328/8505 Training loss: 1.2810 9.2413 sec/batch\n",
      "Epoch 5/15  Iteration 2329/8505 Training loss: 1.2810 9.2788 sec/batch\n",
      "Epoch 5/15  Iteration 2330/8505 Training loss: 1.2810 9.3827 sec/batch\n",
      "Epoch 5/15  Iteration 2331/8505 Training loss: 1.2806 9.3855 sec/batch\n",
      "Epoch 5/15  Iteration 2332/8505 Training loss: 1.2804 9.3379 sec/batch\n",
      "Epoch 5/15  Iteration 2333/8505 Training loss: 1.2800 9.3230 sec/batch\n",
      "Epoch 5/15  Iteration 2334/8505 Training loss: 1.2798 9.2171 sec/batch\n",
      "Epoch 5/15  Iteration 2335/8505 Training loss: 1.2800 9.2811 sec/batch\n",
      "Epoch 5/15  Iteration 2336/8505 Training loss: 1.2800 9.2392 sec/batch\n",
      "Epoch 5/15  Iteration 2337/8505 Training loss: 1.2800 9.2492 sec/batch\n",
      "Epoch 5/15  Iteration 2338/8505 Training loss: 1.2798 9.3309 sec/batch\n",
      "Epoch 5/15  Iteration 2339/8505 Training loss: 1.2796 9.2520 sec/batch\n",
      "Epoch 5/15  Iteration 2340/8505 Training loss: 1.2793 9.3032 sec/batch\n",
      "Epoch 5/15  Iteration 2341/8505 Training loss: 1.2794 9.2699 sec/batch\n",
      "Epoch 5/15  Iteration 2342/8505 Training loss: 1.2797 9.2907 sec/batch\n",
      "Epoch 5/15  Iteration 2343/8505 Training loss: 1.2796 9.3671 sec/batch\n",
      "Epoch 5/15  Iteration 2344/8505 Training loss: 1.2796 9.4945 sec/batch\n",
      "Epoch 5/15  Iteration 2345/8505 Training loss: 1.2795 9.3395 sec/batch\n",
      "Epoch 5/15  Iteration 2346/8505 Training loss: 1.2794 9.2603 sec/batch\n",
      "Epoch 5/15  Iteration 2347/8505 Training loss: 1.2794 9.2586 sec/batch\n",
      "Epoch 5/15  Iteration 2348/8505 Training loss: 1.2788 9.3302 sec/batch\n",
      "Epoch 5/15  Iteration 2349/8505 Training loss: 1.2788 9.3520 sec/batch\n",
      "Epoch 5/15  Iteration 2350/8505 Training loss: 1.2782 9.2605 sec/batch\n",
      "Epoch 5/15  Iteration 2351/8505 Training loss: 1.2779 9.3021 sec/batch\n",
      "Epoch 5/15  Iteration 2352/8505 Training loss: 1.2775 9.3564 sec/batch\n",
      "Epoch 5/15  Iteration 2353/8505 Training loss: 1.2777 9.2394 sec/batch\n",
      "Epoch 5/15  Iteration 2354/8505 Training loss: 1.2777 9.3369 sec/batch\n",
      "Epoch 5/15  Iteration 2355/8505 Training loss: 1.2778 9.3013 sec/batch\n",
      "Epoch 5/15  Iteration 2356/8505 Training loss: 1.2778 9.2962 sec/batch\n",
      "Epoch 5/15  Iteration 2357/8505 Training loss: 1.2775 9.3048 sec/batch\n",
      "Epoch 5/15  Iteration 2358/8505 Training loss: 1.2773 9.3155 sec/batch\n",
      "Epoch 5/15  Iteration 2359/8505 Training loss: 1.2771 9.2144 sec/batch\n",
      "Epoch 5/15  Iteration 2360/8505 Training loss: 1.2771 9.2272 sec/batch\n",
      "Epoch 5/15  Iteration 2361/8505 Training loss: 1.2776 9.2855 sec/batch\n",
      "Epoch 5/15  Iteration 2362/8505 Training loss: 1.2778 9.2669 sec/batch\n",
      "Epoch 5/15  Iteration 2363/8505 Training loss: 1.2779 9.2265 sec/batch\n",
      "Epoch 5/15  Iteration 2364/8505 Training loss: 1.2783 9.2486 sec/batch\n",
      "Epoch 5/15  Iteration 2365/8505 Training loss: 1.2781 9.2548 sec/batch\n",
      "Epoch 5/15  Iteration 2366/8505 Training loss: 1.2777 9.2839 sec/batch\n",
      "Epoch 5/15  Iteration 2367/8505 Training loss: 1.2775 9.2827 sec/batch\n",
      "Epoch 5/15  Iteration 2368/8505 Training loss: 1.2776 9.2867 sec/batch\n",
      "Epoch 5/15  Iteration 2369/8505 Training loss: 1.2773 9.3554 sec/batch\n",
      "Epoch 5/15  Iteration 2370/8505 Training loss: 1.2773 9.3991 sec/batch\n",
      "Epoch 5/15  Iteration 2371/8505 Training loss: 1.2775 9.4527 sec/batch\n",
      "Epoch 5/15  Iteration 2372/8505 Training loss: 1.2773 9.2819 sec/batch\n",
      "Epoch 5/15  Iteration 2373/8505 Training loss: 1.2776 9.3817 sec/batch\n",
      "Epoch 5/15  Iteration 2374/8505 Training loss: 1.2770 9.2382 sec/batch\n",
      "Epoch 5/15  Iteration 2375/8505 Training loss: 1.2769 9.3143 sec/batch\n",
      "Epoch 5/15  Iteration 2376/8505 Training loss: 1.2767 9.2016 sec/batch\n",
      "Epoch 5/15  Iteration 2377/8505 Training loss: 1.2764 9.2421 sec/batch\n",
      "Epoch 5/15  Iteration 2378/8505 Training loss: 1.2763 9.3688 sec/batch\n",
      "Epoch 5/15  Iteration 2379/8505 Training loss: 1.2763 9.2156 sec/batch\n",
      "Epoch 5/15  Iteration 2380/8505 Training loss: 1.2763 9.3200 sec/batch\n",
      "Epoch 5/15  Iteration 2381/8505 Training loss: 1.2761 9.3509 sec/batch\n",
      "Epoch 5/15  Iteration 2382/8505 Training loss: 1.2756 9.3105 sec/batch\n",
      "Epoch 5/15  Iteration 2383/8505 Training loss: 1.2755 9.3256 sec/batch\n",
      "Epoch 5/15  Iteration 2384/8505 Training loss: 1.2751 9.2782 sec/batch\n",
      "Epoch 5/15  Iteration 2385/8505 Training loss: 1.2749 9.2882 sec/batch\n",
      "Epoch 5/15  Iteration 2386/8505 Training loss: 1.2750 9.2918 sec/batch\n",
      "Epoch 5/15  Iteration 2387/8505 Training loss: 1.2750 9.3083 sec/batch\n",
      "Epoch 5/15  Iteration 2388/8505 Training loss: 1.2752 9.3500 sec/batch\n",
      "Epoch 5/15  Iteration 2389/8505 Training loss: 1.2754 9.2400 sec/batch\n",
      "Epoch 5/15  Iteration 2390/8505 Training loss: 1.2754 9.2142 sec/batch\n",
      "Epoch 5/15  Iteration 2391/8505 Training loss: 1.2756 9.2702 sec/batch\n",
      "Epoch 5/15  Iteration 2392/8505 Training loss: 1.2757 9.2848 sec/batch\n",
      "Epoch 5/15  Iteration 2393/8505 Training loss: 1.2756 9.2711 sec/batch\n",
      "Epoch 5/15  Iteration 2394/8505 Training loss: 1.2754 9.3862 sec/batch\n",
      "Epoch 5/15  Iteration 2395/8505 Training loss: 1.2753 9.3426 sec/batch\n",
      "Epoch 5/15  Iteration 2396/8505 Training loss: 1.2753 9.2441 sec/batch\n",
      "Epoch 5/15  Iteration 2397/8505 Training loss: 1.2754 9.2662 sec/batch\n",
      "Epoch 5/15  Iteration 2398/8505 Training loss: 1.2753 9.2861 sec/batch\n",
      "Epoch 5/15  Iteration 2399/8505 Training loss: 1.2752 9.3040 sec/batch\n",
      "Epoch 5/15  Iteration 2400/8505 Training loss: 1.2753 9.2992 sec/batch\n",
      "Validation loss: 1.23312 Saving checkpoint!\n",
      "Epoch 5/15  Iteration 2401/8505 Training loss: 1.2764 9.4818 sec/batch\n",
      "Epoch 5/15  Iteration 2402/8505 Training loss: 1.2767 9.2355 sec/batch\n",
      "Epoch 5/15  Iteration 2403/8505 Training loss: 1.2770 9.3670 sec/batch\n",
      "Epoch 5/15  Iteration 2404/8505 Training loss: 1.2772 9.3535 sec/batch\n",
      "Epoch 5/15  Iteration 2405/8505 Training loss: 1.2772 9.2938 sec/batch\n",
      "Epoch 5/15  Iteration 2406/8505 Training loss: 1.2771 9.4454 sec/batch\n",
      "Epoch 5/15  Iteration 2407/8505 Training loss: 1.2767 9.3098 sec/batch\n",
      "Epoch 5/15  Iteration 2408/8505 Training loss: 1.2768 9.2620 sec/batch\n",
      "Epoch 5/15  Iteration 2409/8505 Training loss: 1.2765 9.4460 sec/batch\n",
      "Epoch 5/15  Iteration 2410/8505 Training loss: 1.2762 9.2827 sec/batch\n",
      "Epoch 5/15  Iteration 2411/8505 Training loss: 1.2761 9.2349 sec/batch\n",
      "Epoch 5/15  Iteration 2412/8505 Training loss: 1.2759 9.2202 sec/batch\n",
      "Epoch 5/15  Iteration 2413/8505 Training loss: 1.2758 9.3270 sec/batch\n",
      "Epoch 5/15  Iteration 2414/8505 Training loss: 1.2758 9.3430 sec/batch\n",
      "Epoch 5/15  Iteration 2415/8505 Training loss: 1.2756 9.3145 sec/batch\n",
      "Epoch 5/15  Iteration 2416/8505 Training loss: 1.2755 9.2700 sec/batch\n",
      "Epoch 5/15  Iteration 2417/8505 Training loss: 1.2753 9.3373 sec/batch\n",
      "Epoch 5/15  Iteration 2418/8505 Training loss: 1.2751 9.5258 sec/batch\n",
      "Epoch 5/15  Iteration 2419/8505 Training loss: 1.2749 9.2575 sec/batch\n",
      "Epoch 5/15  Iteration 2420/8505 Training loss: 1.2745 9.2717 sec/batch\n",
      "Epoch 5/15  Iteration 2421/8505 Training loss: 1.2742 9.2990 sec/batch\n",
      "Epoch 5/15  Iteration 2422/8505 Training loss: 1.2742 9.2531 sec/batch\n",
      "Epoch 5/15  Iteration 2423/8505 Training loss: 1.2739 9.3145 sec/batch\n",
      "Epoch 5/15  Iteration 2424/8505 Training loss: 1.2737 9.6363 sec/batch\n",
      "Epoch 5/15  Iteration 2425/8505 Training loss: 1.2740 9.3627 sec/batch\n",
      "Epoch 5/15  Iteration 2426/8505 Training loss: 1.2740 9.2944 sec/batch\n",
      "Epoch 5/15  Iteration 2427/8505 Training loss: 1.2742 9.2857 sec/batch\n",
      "Epoch 5/15  Iteration 2428/8505 Training loss: 1.2740 9.4287 sec/batch\n",
      "Epoch 5/15  Iteration 2429/8505 Training loss: 1.2741 9.2907 sec/batch\n",
      "Epoch 5/15  Iteration 2430/8505 Training loss: 1.2739 9.2278 sec/batch\n",
      "Epoch 5/15  Iteration 2431/8505 Training loss: 1.2739 9.3524 sec/batch\n",
      "Epoch 5/15  Iteration 2432/8505 Training loss: 1.2737 9.5776 sec/batch\n",
      "Epoch 5/15  Iteration 2433/8505 Training loss: 1.2739 9.3251 sec/batch\n",
      "Epoch 5/15  Iteration 2434/8505 Training loss: 1.2738 9.4214 sec/batch\n",
      "Epoch 5/15  Iteration 2435/8505 Training loss: 1.2737 9.3844 sec/batch\n",
      "Epoch 5/15  Iteration 2436/8505 Training loss: 1.2735 9.3515 sec/batch\n",
      "Epoch 5/15  Iteration 2437/8505 Training loss: 1.2736 9.2579 sec/batch\n",
      "Epoch 5/15  Iteration 2438/8505 Training loss: 1.2737 9.3612 sec/batch\n",
      "Epoch 5/15  Iteration 2439/8505 Training loss: 1.2740 9.3384 sec/batch\n",
      "Epoch 5/15  Iteration 2440/8505 Training loss: 1.2740 9.2696 sec/batch\n",
      "Epoch 5/15  Iteration 2441/8505 Training loss: 1.2738 9.2569 sec/batch\n",
      "Epoch 5/15  Iteration 2442/8505 Training loss: 1.2736 9.2654 sec/batch\n",
      "Epoch 5/15  Iteration 2443/8505 Training loss: 1.2737 9.2734 sec/batch\n",
      "Epoch 5/15  Iteration 2444/8505 Training loss: 1.2737 9.3168 sec/batch\n",
      "Epoch 5/15  Iteration 2445/8505 Training loss: 1.2737 9.2945 sec/batch\n",
      "Epoch 5/15  Iteration 2446/8505 Training loss: 1.2737 9.3393 sec/batch\n",
      "Epoch 5/15  Iteration 2447/8505 Training loss: 1.2737 9.2875 sec/batch\n",
      "Epoch 5/15  Iteration 2448/8505 Training loss: 1.2738 9.2593 sec/batch\n",
      "Epoch 5/15  Iteration 2449/8505 Training loss: 1.2737 9.2907 sec/batch\n",
      "Epoch 5/15  Iteration 2450/8505 Training loss: 1.2736 9.3722 sec/batch\n",
      "Epoch 5/15  Iteration 2451/8505 Training loss: 1.2737 9.3951 sec/batch\n",
      "Epoch 5/15  Iteration 2452/8505 Training loss: 1.2738 9.3411 sec/batch\n",
      "Epoch 5/15  Iteration 2453/8505 Training loss: 1.2738 9.3129 sec/batch\n",
      "Epoch 5/15  Iteration 2454/8505 Training loss: 1.2736 9.3115 sec/batch\n",
      "Epoch 5/15  Iteration 2455/8505 Training loss: 1.2736 9.3765 sec/batch\n",
      "Epoch 5/15  Iteration 2456/8505 Training loss: 1.2735 9.2671 sec/batch\n",
      "Epoch 5/15  Iteration 2457/8505 Training loss: 1.2736 9.3336 sec/batch\n",
      "Epoch 5/15  Iteration 2458/8505 Training loss: 1.2733 9.2629 sec/batch\n",
      "Epoch 5/15  Iteration 2459/8505 Training loss: 1.2733 9.3312 sec/batch\n",
      "Epoch 5/15  Iteration 2460/8505 Training loss: 1.2732 9.3440 sec/batch\n",
      "Epoch 5/15  Iteration 2461/8505 Training loss: 1.2734 9.2834 sec/batch\n",
      "Epoch 5/15  Iteration 2462/8505 Training loss: 1.2733 9.3061 sec/batch\n",
      "Epoch 5/15  Iteration 2463/8505 Training loss: 1.2732 9.2848 sec/batch\n",
      "Epoch 5/15  Iteration 2464/8505 Training loss: 1.2731 9.3911 sec/batch\n",
      "Epoch 5/15  Iteration 2465/8505 Training loss: 1.2730 9.5060 sec/batch\n",
      "Epoch 5/15  Iteration 2466/8505 Training loss: 1.2731 9.2634 sec/batch\n",
      "Epoch 5/15  Iteration 2467/8505 Training loss: 1.2732 9.2800 sec/batch\n",
      "Epoch 5/15  Iteration 2468/8505 Training loss: 1.2731 9.2921 sec/batch\n",
      "Epoch 5/15  Iteration 2469/8505 Training loss: 1.2731 9.3370 sec/batch\n",
      "Epoch 5/15  Iteration 2470/8505 Training loss: 1.2731 9.2757 sec/batch\n",
      "Epoch 5/15  Iteration 2471/8505 Training loss: 1.2731 9.3648 sec/batch\n",
      "Epoch 5/15  Iteration 2472/8505 Training loss: 1.2729 9.3294 sec/batch\n",
      "Epoch 5/15  Iteration 2473/8505 Training loss: 1.2727 9.2849 sec/batch\n",
      "Epoch 5/15  Iteration 2474/8505 Training loss: 1.2727 9.2884 sec/batch\n",
      "Epoch 5/15  Iteration 2475/8505 Training loss: 1.2726 9.5154 sec/batch\n",
      "Epoch 5/15  Iteration 2476/8505 Training loss: 1.2726 9.2649 sec/batch\n",
      "Epoch 5/15  Iteration 2477/8505 Training loss: 1.2724 9.3081 sec/batch\n",
      "Epoch 5/15  Iteration 2478/8505 Training loss: 1.2721 9.3639 sec/batch\n",
      "Epoch 5/15  Iteration 2479/8505 Training loss: 1.2719 9.3109 sec/batch\n",
      "Epoch 5/15  Iteration 2480/8505 Training loss: 1.2720 9.2912 sec/batch\n",
      "Epoch 5/15  Iteration 2481/8505 Training loss: 1.2718 9.4429 sec/batch\n",
      "Epoch 5/15  Iteration 2482/8505 Training loss: 1.2718 9.4285 sec/batch\n",
      "Epoch 5/15  Iteration 2483/8505 Training loss: 1.2717 9.2891 sec/batch\n",
      "Epoch 5/15  Iteration 2484/8505 Training loss: 1.2717 9.2649 sec/batch\n",
      "Epoch 5/15  Iteration 2485/8505 Training loss: 1.2715 9.1936 sec/batch\n",
      "Epoch 5/15  Iteration 2486/8505 Training loss: 1.2715 9.3589 sec/batch\n",
      "Epoch 5/15  Iteration 2487/8505 Training loss: 1.2712 9.2505 sec/batch\n",
      "Epoch 5/15  Iteration 2488/8505 Training loss: 1.2709 9.2964 sec/batch\n",
      "Epoch 5/15  Iteration 2489/8505 Training loss: 1.2708 9.2502 sec/batch\n",
      "Epoch 5/15  Iteration 2490/8505 Training loss: 1.2707 9.3202 sec/batch\n",
      "Epoch 5/15  Iteration 2491/8505 Training loss: 1.2705 9.2463 sec/batch\n",
      "Epoch 5/15  Iteration 2492/8505 Training loss: 1.2704 9.2203 sec/batch\n",
      "Epoch 5/15  Iteration 2493/8505 Training loss: 1.2702 9.2291 sec/batch\n",
      "Epoch 5/15  Iteration 2494/8505 Training loss: 1.2702 9.3048 sec/batch\n",
      "Epoch 5/15  Iteration 2495/8505 Training loss: 1.2700 9.2466 sec/batch\n",
      "Epoch 5/15  Iteration 2496/8505 Training loss: 1.2698 9.2664 sec/batch\n",
      "Epoch 5/15  Iteration 2497/8505 Training loss: 1.2697 9.2815 sec/batch\n",
      "Epoch 5/15  Iteration 2498/8505 Training loss: 1.2696 9.3123 sec/batch\n",
      "Epoch 5/15  Iteration 2499/8505 Training loss: 1.2695 9.2473 sec/batch\n",
      "Epoch 5/15  Iteration 2500/8505 Training loss: 1.2695 9.2062 sec/batch\n",
      "Epoch 5/15  Iteration 2501/8505 Training loss: 1.2694 9.2555 sec/batch\n",
      "Epoch 5/15  Iteration 2502/8505 Training loss: 1.2692 9.1617 sec/batch\n",
      "Epoch 5/15  Iteration 2503/8505 Training loss: 1.2691 9.2461 sec/batch\n",
      "Epoch 5/15  Iteration 2504/8505 Training loss: 1.2689 9.2330 sec/batch\n",
      "Epoch 5/15  Iteration 2505/8505 Training loss: 1.2690 9.2698 sec/batch\n",
      "Epoch 5/15  Iteration 2506/8505 Training loss: 1.2689 9.3002 sec/batch\n",
      "Epoch 5/15  Iteration 2507/8505 Training loss: 1.2688 9.3267 sec/batch\n",
      "Epoch 5/15  Iteration 2508/8505 Training loss: 1.2686 9.2788 sec/batch\n",
      "Epoch 5/15  Iteration 2509/8505 Training loss: 1.2684 9.2928 sec/batch\n",
      "Epoch 5/15  Iteration 2510/8505 Training loss: 1.2686 9.2890 sec/batch\n",
      "Epoch 5/15  Iteration 2511/8505 Training loss: 1.2685 9.3671 sec/batch\n",
      "Epoch 5/15  Iteration 2512/8505 Training loss: 1.2685 9.3758 sec/batch\n",
      "Epoch 5/15  Iteration 2513/8505 Training loss: 1.2685 9.3212 sec/batch\n",
      "Epoch 5/15  Iteration 2514/8505 Training loss: 1.2684 9.3342 sec/batch\n",
      "Epoch 5/15  Iteration 2515/8505 Training loss: 1.2684 9.3492 sec/batch\n",
      "Epoch 5/15  Iteration 2516/8505 Training loss: 1.2685 9.3311 sec/batch\n",
      "Epoch 5/15  Iteration 2517/8505 Training loss: 1.2685 9.3049 sec/batch\n",
      "Epoch 5/15  Iteration 2518/8505 Training loss: 1.2684 9.2641 sec/batch\n",
      "Epoch 5/15  Iteration 2519/8505 Training loss: 1.2684 9.2733 sec/batch\n",
      "Epoch 5/15  Iteration 2520/8505 Training loss: 1.2683 9.2401 sec/batch\n",
      "Epoch 5/15  Iteration 2521/8505 Training loss: 1.2681 9.2944 sec/batch\n",
      "Epoch 5/15  Iteration 2522/8505 Training loss: 1.2681 9.2953 sec/batch\n",
      "Epoch 5/15  Iteration 2523/8505 Training loss: 1.2682 9.2111 sec/batch\n",
      "Epoch 5/15  Iteration 2524/8505 Training loss: 1.2682 9.2757 sec/batch\n",
      "Epoch 5/15  Iteration 2525/8505 Training loss: 1.2683 9.3222 sec/batch\n",
      "Epoch 5/15  Iteration 2526/8505 Training loss: 1.2682 9.3094 sec/batch\n",
      "Epoch 5/15  Iteration 2527/8505 Training loss: 1.2681 9.3638 sec/batch\n",
      "Epoch 5/15  Iteration 2528/8505 Training loss: 1.2680 9.3430 sec/batch\n",
      "Epoch 5/15  Iteration 2529/8505 Training loss: 1.2680 9.2976 sec/batch\n",
      "Epoch 5/15  Iteration 2530/8505 Training loss: 1.2680 9.3240 sec/batch\n",
      "Epoch 5/15  Iteration 2531/8505 Training loss: 1.2680 9.3009 sec/batch\n",
      "Epoch 5/15  Iteration 2532/8505 Training loss: 1.2679 9.3952 sec/batch\n",
      "Epoch 5/15  Iteration 2533/8505 Training loss: 1.2679 9.3096 sec/batch\n",
      "Epoch 5/15  Iteration 2534/8505 Training loss: 1.2678 9.1885 sec/batch\n",
      "Epoch 5/15  Iteration 2535/8505 Training loss: 1.2677 9.2980 sec/batch\n",
      "Epoch 5/15  Iteration 2536/8505 Training loss: 1.2677 9.3402 sec/batch\n",
      "Epoch 5/15  Iteration 2537/8505 Training loss: 1.2675 9.2956 sec/batch\n",
      "Epoch 5/15  Iteration 2538/8505 Training loss: 1.2675 9.2841 sec/batch\n",
      "Epoch 5/15  Iteration 2539/8505 Training loss: 1.2675 9.2940 sec/batch\n",
      "Epoch 5/15  Iteration 2540/8505 Training loss: 1.2673 10.7311 sec/batch\n",
      "Epoch 5/15  Iteration 2541/8505 Training loss: 1.2674 9.4769 sec/batch\n",
      "Epoch 5/15  Iteration 2542/8505 Training loss: 1.2674 9.3254 sec/batch\n",
      "Epoch 5/15  Iteration 2543/8505 Training loss: 1.2673 9.2738 sec/batch\n",
      "Epoch 5/15  Iteration 2544/8505 Training loss: 1.2674 9.2232 sec/batch\n",
      "Epoch 5/15  Iteration 2545/8505 Training loss: 1.2673 9.2681 sec/batch\n",
      "Epoch 5/15  Iteration 2546/8505 Training loss: 1.2673 9.3460 sec/batch\n",
      "Epoch 5/15  Iteration 2547/8505 Training loss: 1.2673 9.2657 sec/batch\n",
      "Epoch 5/15  Iteration 2548/8505 Training loss: 1.2673 9.2961 sec/batch\n",
      "Epoch 5/15  Iteration 2549/8505 Training loss: 1.2670 9.2134 sec/batch\n",
      "Epoch 5/15  Iteration 2550/8505 Training loss: 1.2670 9.3358 sec/batch\n",
      "Epoch 5/15  Iteration 2551/8505 Training loss: 1.2669 9.3276 sec/batch\n",
      "Epoch 5/15  Iteration 2552/8505 Training loss: 1.2668 9.2744 sec/batch\n",
      "Epoch 5/15  Iteration 2553/8505 Training loss: 1.2667 9.3398 sec/batch\n",
      "Epoch 5/15  Iteration 2554/8505 Training loss: 1.2667 9.3575 sec/batch\n",
      "Epoch 5/15  Iteration 2555/8505 Training loss: 1.2668 9.4569 sec/batch\n",
      "Epoch 5/15  Iteration 2556/8505 Training loss: 1.2669 9.4473 sec/batch\n",
      "Epoch 5/15  Iteration 2557/8505 Training loss: 1.2668 9.3161 sec/batch\n",
      "Epoch 5/15  Iteration 2558/8505 Training loss: 1.2668 9.4165 sec/batch\n",
      "Epoch 5/15  Iteration 2559/8505 Training loss: 1.2666 9.3885 sec/batch\n",
      "Epoch 5/15  Iteration 2560/8505 Training loss: 1.2665 9.3021 sec/batch\n",
      "Epoch 5/15  Iteration 2561/8505 Training loss: 1.2664 9.4066 sec/batch\n",
      "Epoch 5/15  Iteration 2562/8505 Training loss: 1.2664 9.3250 sec/batch\n",
      "Epoch 5/15  Iteration 2563/8505 Training loss: 1.2664 9.3973 sec/batch\n",
      "Epoch 5/15  Iteration 2564/8505 Training loss: 1.2663 9.2230 sec/batch\n",
      "Epoch 5/15  Iteration 2565/8505 Training loss: 1.2662 9.3307 sec/batch\n",
      "Epoch 5/15  Iteration 2566/8505 Training loss: 1.2661 9.2541 sec/batch\n",
      "Epoch 5/15  Iteration 2567/8505 Training loss: 1.2661 9.2526 sec/batch\n",
      "Epoch 5/15  Iteration 2568/8505 Training loss: 1.2661 9.3044 sec/batch\n",
      "Epoch 5/15  Iteration 2569/8505 Training loss: 1.2660 9.2845 sec/batch\n",
      "Epoch 5/15  Iteration 2570/8505 Training loss: 1.2659 9.2846 sec/batch\n",
      "Epoch 5/15  Iteration 2571/8505 Training loss: 1.2659 9.2121 sec/batch\n",
      "Epoch 5/15  Iteration 2572/8505 Training loss: 1.2658 9.2432 sec/batch\n",
      "Epoch 5/15  Iteration 2573/8505 Training loss: 1.2658 9.2391 sec/batch\n",
      "Epoch 5/15  Iteration 2574/8505 Training loss: 1.2657 9.3513 sec/batch\n",
      "Epoch 5/15  Iteration 2575/8505 Training loss: 1.2656 9.2828 sec/batch\n",
      "Epoch 5/15  Iteration 2576/8505 Training loss: 1.2656 9.3158 sec/batch\n",
      "Epoch 5/15  Iteration 2577/8505 Training loss: 1.2657 9.3345 sec/batch\n",
      "Epoch 5/15  Iteration 2578/8505 Training loss: 1.2656 9.3073 sec/batch\n",
      "Epoch 5/15  Iteration 2579/8505 Training loss: 1.2656 9.2899 sec/batch\n",
      "Epoch 5/15  Iteration 2580/8505 Training loss: 1.2655 9.2661 sec/batch\n",
      "Epoch 5/15  Iteration 2581/8505 Training loss: 1.2654 9.3043 sec/batch\n",
      "Epoch 5/15  Iteration 2582/8505 Training loss: 1.2654 9.2092 sec/batch\n",
      "Epoch 5/15  Iteration 2583/8505 Training loss: 1.2653 9.3348 sec/batch\n",
      "Epoch 5/15  Iteration 2584/8505 Training loss: 1.2652 9.4084 sec/batch\n",
      "Epoch 5/15  Iteration 2585/8505 Training loss: 1.2652 9.3835 sec/batch\n",
      "Epoch 5/15  Iteration 2586/8505 Training loss: 1.2652 9.3831 sec/batch\n",
      "Epoch 5/15  Iteration 2587/8505 Training loss: 1.2652 9.4694 sec/batch\n",
      "Epoch 5/15  Iteration 2588/8505 Training loss: 1.2651 9.3305 sec/batch\n",
      "Epoch 5/15  Iteration 2589/8505 Training loss: 1.2651 9.2851 sec/batch\n",
      "Epoch 5/15  Iteration 2590/8505 Training loss: 1.2651 9.2894 sec/batch\n",
      "Epoch 5/15  Iteration 2591/8505 Training loss: 1.2651 9.3607 sec/batch\n",
      "Epoch 5/15  Iteration 2592/8505 Training loss: 1.2651 9.3447 sec/batch\n",
      "Epoch 5/15  Iteration 2593/8505 Training loss: 1.2652 9.2647 sec/batch\n",
      "Epoch 5/15  Iteration 2594/8505 Training loss: 1.2652 9.2654 sec/batch\n",
      "Epoch 5/15  Iteration 2595/8505 Training loss: 1.2652 9.2012 sec/batch\n",
      "Epoch 5/15  Iteration 2596/8505 Training loss: 1.2652 9.3375 sec/batch\n",
      "Epoch 5/15  Iteration 2597/8505 Training loss: 1.2651 9.3408 sec/batch\n",
      "Epoch 5/15  Iteration 2598/8505 Training loss: 1.2651 9.2169 sec/batch\n",
      "Epoch 5/15  Iteration 2599/8505 Training loss: 1.2650 9.2139 sec/batch\n",
      "Epoch 5/15  Iteration 2600/8505 Training loss: 1.2651 9.2918 sec/batch\n",
      "Validation loss: 1.22169 Saving checkpoint!\n",
      "Epoch 5/15  Iteration 2601/8505 Training loss: 1.2654 10.8163 sec/batch\n",
      "Epoch 5/15  Iteration 2602/8505 Training loss: 1.2654 9.4152 sec/batch\n",
      "Epoch 5/15  Iteration 2603/8505 Training loss: 1.2653 9.2940 sec/batch\n",
      "Epoch 5/15  Iteration 2604/8505 Training loss: 1.2653 9.3451 sec/batch\n",
      "Epoch 5/15  Iteration 2605/8505 Training loss: 1.2654 9.4033 sec/batch\n",
      "Epoch 5/15  Iteration 2606/8505 Training loss: 1.2655 9.3674 sec/batch\n",
      "Epoch 5/15  Iteration 2607/8505 Training loss: 1.2655 9.3051 sec/batch\n",
      "Epoch 5/15  Iteration 2608/8505 Training loss: 1.2654 9.3202 sec/batch\n",
      "Epoch 5/15  Iteration 2609/8505 Training loss: 1.2654 9.2438 sec/batch\n",
      "Epoch 5/15  Iteration 2610/8505 Training loss: 1.2653 9.3245 sec/batch\n",
      "Epoch 5/15  Iteration 2611/8505 Training loss: 1.2653 9.5118 sec/batch\n",
      "Epoch 5/15  Iteration 2612/8505 Training loss: 1.2653 9.3511 sec/batch\n",
      "Epoch 5/15  Iteration 2613/8505 Training loss: 1.2652 9.2150 sec/batch\n",
      "Epoch 5/15  Iteration 2614/8505 Training loss: 1.2652 9.2336 sec/batch\n",
      "Epoch 5/15  Iteration 2615/8505 Training loss: 1.2652 9.2526 sec/batch\n",
      "Epoch 5/15  Iteration 2616/8505 Training loss: 1.2651 9.2735 sec/batch\n",
      "Epoch 5/15  Iteration 2617/8505 Training loss: 1.2651 9.2894 sec/batch\n",
      "Epoch 5/15  Iteration 2618/8505 Training loss: 1.2651 9.2484 sec/batch\n",
      "Epoch 5/15  Iteration 2619/8505 Training loss: 1.2651 9.2615 sec/batch\n",
      "Epoch 5/15  Iteration 2620/8505 Training loss: 1.2651 9.3531 sec/batch\n",
      "Epoch 5/15  Iteration 2621/8505 Training loss: 1.2650 9.4393 sec/batch\n",
      "Epoch 5/15  Iteration 2622/8505 Training loss: 1.2649 9.2962 sec/batch\n",
      "Epoch 5/15  Iteration 2623/8505 Training loss: 1.2650 9.3254 sec/batch\n",
      "Epoch 5/15  Iteration 2624/8505 Training loss: 1.2650 9.2927 sec/batch\n",
      "Epoch 5/15  Iteration 2625/8505 Training loss: 1.2649 9.2846 sec/batch\n",
      "Epoch 5/15  Iteration 2626/8505 Training loss: 1.2649 9.3466 sec/batch\n",
      "Epoch 5/15  Iteration 2627/8505 Training loss: 1.2647 9.3700 sec/batch\n",
      "Epoch 5/15  Iteration 2628/8505 Training loss: 1.2646 9.3069 sec/batch\n",
      "Epoch 5/15  Iteration 2629/8505 Training loss: 1.2646 9.3605 sec/batch\n",
      "Epoch 5/15  Iteration 2630/8505 Training loss: 1.2646 9.3400 sec/batch\n",
      "Epoch 5/15  Iteration 2631/8505 Training loss: 1.2645 9.3104 sec/batch\n",
      "Epoch 5/15  Iteration 2632/8505 Training loss: 1.2644 9.3331 sec/batch\n",
      "Epoch 5/15  Iteration 2633/8505 Training loss: 1.2645 9.3117 sec/batch\n",
      "Epoch 5/15  Iteration 2634/8505 Training loss: 1.2645 9.3146 sec/batch\n",
      "Epoch 5/15  Iteration 2635/8505 Training loss: 1.2646 9.3659 sec/batch\n",
      "Epoch 5/15  Iteration 2636/8505 Training loss: 1.2644 9.3659 sec/batch\n",
      "Epoch 5/15  Iteration 2637/8505 Training loss: 1.2644 9.2998 sec/batch\n",
      "Epoch 5/15  Iteration 2638/8505 Training loss: 1.2643 9.2854 sec/batch\n",
      "Epoch 5/15  Iteration 2639/8505 Training loss: 1.2643 9.2958 sec/batch\n",
      "Epoch 5/15  Iteration 2640/8505 Training loss: 1.2643 9.2849 sec/batch\n",
      "Epoch 5/15  Iteration 2641/8505 Training loss: 1.2643 9.2680 sec/batch\n",
      "Epoch 5/15  Iteration 2642/8505 Training loss: 1.2643 9.3125 sec/batch\n",
      "Epoch 5/15  Iteration 2643/8505 Training loss: 1.2643 9.2182 sec/batch\n",
      "Epoch 5/15  Iteration 2644/8505 Training loss: 1.2643 9.2461 sec/batch\n",
      "Epoch 5/15  Iteration 2645/8505 Training loss: 1.2642 9.2494 sec/batch\n",
      "Epoch 5/15  Iteration 2646/8505 Training loss: 1.2642 9.3385 sec/batch\n",
      "Epoch 5/15  Iteration 2647/8505 Training loss: 1.2643 9.3501 sec/batch\n",
      "Epoch 5/15  Iteration 2648/8505 Training loss: 1.2643 9.2381 sec/batch\n",
      "Epoch 5/15  Iteration 2649/8505 Training loss: 1.2643 9.2173 sec/batch\n",
      "Epoch 5/15  Iteration 2650/8505 Training loss: 1.2642 9.2290 sec/batch\n",
      "Epoch 5/15  Iteration 2651/8505 Training loss: 1.2642 9.2933 sec/batch\n",
      "Epoch 5/15  Iteration 2652/8505 Training loss: 1.2641 9.2731 sec/batch\n",
      "Epoch 5/15  Iteration 2653/8505 Training loss: 1.2640 9.3566 sec/batch\n",
      "Epoch 5/15  Iteration 2654/8505 Training loss: 1.2639 9.2749 sec/batch\n",
      "Epoch 5/15  Iteration 2655/8505 Training loss: 1.2637 9.5709 sec/batch\n",
      "Epoch 5/15  Iteration 2656/8505 Training loss: 1.2636 9.4289 sec/batch\n",
      "Epoch 5/15  Iteration 2657/8505 Training loss: 1.2636 9.3842 sec/batch\n",
      "Epoch 5/15  Iteration 2658/8505 Training loss: 1.2634 9.2223 sec/batch\n",
      "Epoch 5/15  Iteration 2659/8505 Training loss: 1.2633 9.3110 sec/batch\n",
      "Epoch 5/15  Iteration 2660/8505 Training loss: 1.2632 9.4097 sec/batch\n",
      "Epoch 5/15  Iteration 2661/8505 Training loss: 1.2632 9.2658 sec/batch\n",
      "Epoch 5/15  Iteration 2662/8505 Training loss: 1.2632 9.2446 sec/batch\n",
      "Epoch 5/15  Iteration 2663/8505 Training loss: 1.2632 9.2633 sec/batch\n",
      "Epoch 5/15  Iteration 2664/8505 Training loss: 1.2631 9.2642 sec/batch\n",
      "Epoch 5/15  Iteration 2665/8505 Training loss: 1.2631 9.2417 sec/batch\n",
      "Epoch 5/15  Iteration 2666/8505 Training loss: 1.2632 9.2553 sec/batch\n",
      "Epoch 5/15  Iteration 2667/8505 Training loss: 1.2632 9.1879 sec/batch\n",
      "Epoch 5/15  Iteration 2668/8505 Training loss: 1.2631 9.3278 sec/batch\n",
      "Epoch 5/15  Iteration 2669/8505 Training loss: 1.2630 9.5523 sec/batch\n",
      "Epoch 5/15  Iteration 2670/8505 Training loss: 1.2629 9.3993 sec/batch\n",
      "Epoch 5/15  Iteration 2671/8505 Training loss: 1.2627 9.3509 sec/batch\n",
      "Epoch 5/15  Iteration 2672/8505 Training loss: 1.2626 9.2824 sec/batch\n",
      "Epoch 5/15  Iteration 2673/8505 Training loss: 1.2626 9.2209 sec/batch\n",
      "Epoch 5/15  Iteration 2674/8505 Training loss: 1.2625 9.3045 sec/batch\n",
      "Epoch 5/15  Iteration 2675/8505 Training loss: 1.2625 9.3863 sec/batch\n",
      "Epoch 5/15  Iteration 2676/8505 Training loss: 1.2624 9.3454 sec/batch\n",
      "Epoch 5/15  Iteration 2677/8505 Training loss: 1.2624 9.2377 sec/batch\n",
      "Epoch 5/15  Iteration 2678/8505 Training loss: 1.2623 9.2841 sec/batch\n",
      "Epoch 5/15  Iteration 2679/8505 Training loss: 1.2623 9.3524 sec/batch\n",
      "Epoch 5/15  Iteration 2680/8505 Training loss: 1.2623 9.3303 sec/batch\n",
      "Epoch 5/15  Iteration 2681/8505 Training loss: 1.2624 9.3652 sec/batch\n",
      "Epoch 5/15  Iteration 2682/8505 Training loss: 1.2623 9.3523 sec/batch\n",
      "Epoch 5/15  Iteration 2683/8505 Training loss: 1.2621 9.3374 sec/batch\n",
      "Epoch 5/15  Iteration 2684/8505 Training loss: 1.2620 9.3685 sec/batch\n",
      "Epoch 5/15  Iteration 2685/8505 Training loss: 1.2620 9.2853 sec/batch\n",
      "Epoch 5/15  Iteration 2686/8505 Training loss: 1.2619 9.3275 sec/batch\n",
      "Epoch 5/15  Iteration 2687/8505 Training loss: 1.2619 9.2752 sec/batch\n",
      "Epoch 5/15  Iteration 2688/8505 Training loss: 1.2618 9.4581 sec/batch\n",
      "Epoch 5/15  Iteration 2689/8505 Training loss: 1.2618 9.2821 sec/batch\n",
      "Epoch 5/15  Iteration 2690/8505 Training loss: 1.2618 9.3038 sec/batch\n",
      "Epoch 5/15  Iteration 2691/8505 Training loss: 1.2617 9.2897 sec/batch\n",
      "Epoch 5/15  Iteration 2692/8505 Training loss: 1.2617 9.2727 sec/batch\n",
      "Epoch 5/15  Iteration 2693/8505 Training loss: 1.2616 9.2760 sec/batch\n",
      "Epoch 5/15  Iteration 2694/8505 Training loss: 1.2616 9.2477 sec/batch\n",
      "Epoch 5/15  Iteration 2695/8505 Training loss: 1.2616 9.4906 sec/batch\n",
      "Epoch 5/15  Iteration 2696/8505 Training loss: 1.2616 9.3509 sec/batch\n",
      "Epoch 5/15  Iteration 2697/8505 Training loss: 1.2616 9.3707 sec/batch\n",
      "Epoch 5/15  Iteration 2698/8505 Training loss: 1.2615 9.2930 sec/batch\n",
      "Epoch 5/15  Iteration 2699/8505 Training loss: 1.2615 9.3085 sec/batch\n",
      "Epoch 5/15  Iteration 2700/8505 Training loss: 1.2614 9.3131 sec/batch\n",
      "Epoch 5/15  Iteration 2701/8505 Training loss: 1.2613 9.3118 sec/batch\n",
      "Epoch 5/15  Iteration 2702/8505 Training loss: 1.2613 9.3311 sec/batch\n",
      "Epoch 5/15  Iteration 2703/8505 Training loss: 1.2612 9.2664 sec/batch\n",
      "Epoch 5/15  Iteration 2704/8505 Training loss: 1.2611 9.5203 sec/batch\n",
      "Epoch 5/15  Iteration 2705/8505 Training loss: 1.2610 9.4295 sec/batch\n",
      "Epoch 5/15  Iteration 2706/8505 Training loss: 1.2609 9.2148 sec/batch\n",
      "Epoch 5/15  Iteration 2707/8505 Training loss: 1.2608 9.3425 sec/batch\n",
      "Epoch 5/15  Iteration 2708/8505 Training loss: 1.2609 9.4245 sec/batch\n",
      "Epoch 5/15  Iteration 2709/8505 Training loss: 1.2608 9.3857 sec/batch\n",
      "Epoch 5/15  Iteration 2710/8505 Training loss: 1.2607 9.2419 sec/batch\n",
      "Epoch 5/15  Iteration 2711/8505 Training loss: 1.2607 9.2429 sec/batch\n",
      "Epoch 5/15  Iteration 2712/8505 Training loss: 1.2606 9.2517 sec/batch\n",
      "Epoch 5/15  Iteration 2713/8505 Training loss: 1.2606 9.2962 sec/batch\n",
      "Epoch 5/15  Iteration 2714/8505 Training loss: 1.2605 9.2582 sec/batch\n",
      "Epoch 5/15  Iteration 2715/8505 Training loss: 1.2604 9.2916 sec/batch\n",
      "Epoch 5/15  Iteration 2716/8505 Training loss: 1.2603 9.2927 sec/batch\n",
      "Epoch 5/15  Iteration 2717/8505 Training loss: 1.2603 9.2754 sec/batch\n",
      "Epoch 5/15  Iteration 2718/8505 Training loss: 1.2602 9.3504 sec/batch\n",
      "Epoch 5/15  Iteration 2719/8505 Training loss: 1.2601 9.2186 sec/batch\n",
      "Epoch 5/15  Iteration 2720/8505 Training loss: 1.2601 9.3408 sec/batch\n",
      "Epoch 5/15  Iteration 2721/8505 Training loss: 1.2600 9.2334 sec/batch\n",
      "Epoch 5/15  Iteration 2722/8505 Training loss: 1.2600 9.2887 sec/batch\n",
      "Epoch 5/15  Iteration 2723/8505 Training loss: 1.2600 9.2706 sec/batch\n",
      "Epoch 5/15  Iteration 2724/8505 Training loss: 1.2599 9.3206 sec/batch\n",
      "Epoch 5/15  Iteration 2725/8505 Training loss: 1.2599 9.3018 sec/batch\n",
      "Epoch 5/15  Iteration 2726/8505 Training loss: 1.2598 9.3152 sec/batch\n",
      "Epoch 5/15  Iteration 2727/8505 Training loss: 1.2597 9.3453 sec/batch\n",
      "Epoch 5/15  Iteration 2728/8505 Training loss: 1.2597 9.5499 sec/batch\n",
      "Epoch 5/15  Iteration 2729/8505 Training loss: 1.2596 9.3411 sec/batch\n",
      "Epoch 5/15  Iteration 2730/8505 Training loss: 1.2595 9.2178 sec/batch\n",
      "Epoch 5/15  Iteration 2731/8505 Training loss: 1.2595 9.4090 sec/batch\n",
      "Epoch 5/15  Iteration 2732/8505 Training loss: 1.2594 9.4014 sec/batch\n",
      "Epoch 5/15  Iteration 2733/8505 Training loss: 1.2595 9.3935 sec/batch\n",
      "Epoch 5/15  Iteration 2734/8505 Training loss: 1.2595 9.3571 sec/batch\n",
      "Epoch 5/15  Iteration 2735/8505 Training loss: 1.2594 9.4092 sec/batch\n",
      "Epoch 5/15  Iteration 2736/8505 Training loss: 1.2593 9.2895 sec/batch\n",
      "Epoch 5/15  Iteration 2737/8505 Training loss: 1.2593 9.2850 sec/batch\n",
      "Epoch 5/15  Iteration 2738/8505 Training loss: 1.2592 9.2622 sec/batch\n",
      "Epoch 5/15  Iteration 2739/8505 Training loss: 1.2592 9.3469 sec/batch\n",
      "Epoch 5/15  Iteration 2740/8505 Training loss: 1.2590 9.2822 sec/batch\n",
      "Epoch 5/15  Iteration 2741/8505 Training loss: 1.2590 9.3473 sec/batch\n",
      "Epoch 5/15  Iteration 2742/8505 Training loss: 1.2589 9.3678 sec/batch\n",
      "Epoch 5/15  Iteration 2743/8505 Training loss: 1.2589 9.2949 sec/batch\n",
      "Epoch 5/15  Iteration 2744/8505 Training loss: 1.2589 9.5032 sec/batch\n",
      "Epoch 5/15  Iteration 2745/8505 Training loss: 1.2588 9.2630 sec/batch\n",
      "Epoch 5/15  Iteration 2746/8505 Training loss: 1.2588 9.3132 sec/batch\n",
      "Epoch 5/15  Iteration 2747/8505 Training loss: 1.2587 9.3153 sec/batch\n",
      "Epoch 5/15  Iteration 2748/8505 Training loss: 1.2586 9.2360 sec/batch\n",
      "Epoch 5/15  Iteration 2749/8505 Training loss: 1.2586 9.2416 sec/batch\n",
      "Epoch 5/15  Iteration 2750/8505 Training loss: 1.2585 9.3103 sec/batch\n",
      "Epoch 5/15  Iteration 2751/8505 Training loss: 1.2584 9.2845 sec/batch\n",
      "Epoch 5/15  Iteration 2752/8505 Training loss: 1.2584 9.2655 sec/batch\n",
      "Epoch 5/15  Iteration 2753/8505 Training loss: 1.2584 9.2635 sec/batch\n",
      "Epoch 5/15  Iteration 2754/8505 Training loss: 1.2584 9.3069 sec/batch\n",
      "Epoch 5/15  Iteration 2755/8505 Training loss: 1.2583 9.3645 sec/batch\n",
      "Epoch 5/15  Iteration 2756/8505 Training loss: 1.2583 9.3315 sec/batch\n",
      "Epoch 5/15  Iteration 2757/8505 Training loss: 1.2583 9.4095 sec/batch\n",
      "Epoch 5/15  Iteration 2758/8505 Training loss: 1.2582 9.4175 sec/batch\n",
      "Epoch 5/15  Iteration 2759/8505 Training loss: 1.2582 9.3509 sec/batch\n",
      "Epoch 5/15  Iteration 2760/8505 Training loss: 1.2582 9.2404 sec/batch\n",
      "Epoch 5/15  Iteration 2761/8505 Training loss: 1.2581 9.2448 sec/batch\n",
      "Epoch 5/15  Iteration 2762/8505 Training loss: 1.2581 9.2517 sec/batch\n",
      "Epoch 5/15  Iteration 2763/8505 Training loss: 1.2581 9.2036 sec/batch\n",
      "Epoch 5/15  Iteration 2764/8505 Training loss: 1.2580 9.3374 sec/batch\n",
      "Epoch 5/15  Iteration 2765/8505 Training loss: 1.2580 9.3135 sec/batch\n",
      "Epoch 5/15  Iteration 2766/8505 Training loss: 1.2579 9.2904 sec/batch\n",
      "Epoch 5/15  Iteration 2767/8505 Training loss: 1.2578 9.3052 sec/batch\n",
      "Epoch 5/15  Iteration 2768/8505 Training loss: 1.2577 9.6935 sec/batch\n",
      "Epoch 5/15  Iteration 2769/8505 Training loss: 1.2576 9.5896 sec/batch\n",
      "Epoch 5/15  Iteration 2770/8505 Training loss: 1.2575 9.3281 sec/batch\n",
      "Epoch 5/15  Iteration 2771/8505 Training loss: 1.2574 9.3566 sec/batch\n",
      "Epoch 5/15  Iteration 2772/8505 Training loss: 1.2573 9.5551 sec/batch\n",
      "Epoch 5/15  Iteration 2773/8505 Training loss: 1.2572 9.3152 sec/batch\n",
      "Epoch 5/15  Iteration 2774/8505 Training loss: 1.2572 9.2198 sec/batch\n",
      "Epoch 5/15  Iteration 2775/8505 Training loss: 1.2572 9.2191 sec/batch\n",
      "Epoch 5/15  Iteration 2776/8505 Training loss: 1.2571 9.3940 sec/batch\n",
      "Epoch 5/15  Iteration 2777/8505 Training loss: 1.2570 9.2779 sec/batch\n",
      "Epoch 5/15  Iteration 2778/8505 Training loss: 1.2569 9.2899 sec/batch\n",
      "Epoch 5/15  Iteration 2779/8505 Training loss: 1.2567 9.4482 sec/batch\n",
      "Epoch 5/15  Iteration 2780/8505 Training loss: 1.2566 9.2553 sec/batch\n",
      "Epoch 5/15  Iteration 2781/8505 Training loss: 1.2564 9.1911 sec/batch\n",
      "Epoch 5/15  Iteration 2782/8505 Training loss: 1.2564 9.0897 sec/batch\n",
      "Epoch 5/15  Iteration 2783/8505 Training loss: 1.2563 9.4009 sec/batch\n",
      "Epoch 5/15  Iteration 2784/8505 Training loss: 1.2563 9.2663 sec/batch\n",
      "Epoch 5/15  Iteration 2785/8505 Training loss: 1.2562 9.2948 sec/batch\n",
      "Epoch 5/15  Iteration 2786/8505 Training loss: 1.2561 9.1975 sec/batch\n",
      "Epoch 5/15  Iteration 2787/8505 Training loss: 1.2560 9.3432 sec/batch\n",
      "Epoch 5/15  Iteration 2788/8505 Training loss: 1.2559 9.3489 sec/batch\n",
      "Epoch 5/15  Iteration 2789/8505 Training loss: 1.2558 9.2309 sec/batch\n",
      "Epoch 5/15  Iteration 2790/8505 Training loss: 1.2557 9.2775 sec/batch\n",
      "Epoch 5/15  Iteration 2791/8505 Training loss: 1.2556 9.3224 sec/batch\n",
      "Epoch 5/15  Iteration 2792/8505 Training loss: 1.2555 9.2954 sec/batch\n",
      "Epoch 5/15  Iteration 2793/8505 Training loss: 1.2554 9.3225 sec/batch\n",
      "Epoch 5/15  Iteration 2794/8505 Training loss: 1.2553 9.2466 sec/batch\n",
      "Epoch 5/15  Iteration 2795/8505 Training loss: 1.2552 9.4821 sec/batch\n",
      "Epoch 5/15  Iteration 2796/8505 Training loss: 1.2551 9.2559 sec/batch\n",
      "Epoch 5/15  Iteration 2797/8505 Training loss: 1.2550 9.2443 sec/batch\n",
      "Epoch 5/15  Iteration 2798/8505 Training loss: 1.2550 9.3203 sec/batch\n",
      "Epoch 5/15  Iteration 2799/8505 Training loss: 1.2549 9.3140 sec/batch\n",
      "Epoch 5/15  Iteration 2800/8505 Training loss: 1.2548 9.3075 sec/batch\n",
      "Validation loss: 1.20777 Saving checkpoint!\n",
      "Epoch 5/15  Iteration 2801/8505 Training loss: 1.2550 11.2068 sec/batch\n",
      "Epoch 5/15  Iteration 2802/8505 Training loss: 1.2549 9.3572 sec/batch\n",
      "Epoch 5/15  Iteration 2803/8505 Training loss: 1.2548 9.3270 sec/batch\n",
      "Epoch 5/15  Iteration 2804/8505 Training loss: 1.2548 9.3304 sec/batch\n",
      "Epoch 5/15  Iteration 2805/8505 Training loss: 1.2547 9.2600 sec/batch\n",
      "Epoch 5/15  Iteration 2806/8505 Training loss: 1.2546 9.2900 sec/batch\n",
      "Epoch 5/15  Iteration 2807/8505 Training loss: 1.2545 9.2736 sec/batch\n",
      "Epoch 5/15  Iteration 2808/8505 Training loss: 1.2545 9.2381 sec/batch\n",
      "Epoch 5/15  Iteration 2809/8505 Training loss: 1.2544 9.3499 sec/batch\n",
      "Epoch 5/15  Iteration 2810/8505 Training loss: 1.2544 9.2636 sec/batch\n",
      "Epoch 5/15  Iteration 2811/8505 Training loss: 1.2543 9.3124 sec/batch\n",
      "Epoch 5/15  Iteration 2812/8505 Training loss: 1.2543 9.3375 sec/batch\n",
      "Epoch 5/15  Iteration 2813/8505 Training loss: 1.2542 9.2792 sec/batch\n",
      "Epoch 5/15  Iteration 2814/8505 Training loss: 1.2542 9.2835 sec/batch\n",
      "Epoch 5/15  Iteration 2815/8505 Training loss: 1.2541 9.2549 sec/batch\n",
      "Epoch 5/15  Iteration 2816/8505 Training loss: 1.2541 9.2539 sec/batch\n",
      "Epoch 5/15  Iteration 2817/8505 Training loss: 1.2540 9.2201 sec/batch\n",
      "Epoch 5/15  Iteration 2818/8505 Training loss: 1.2539 9.2343 sec/batch\n",
      "Epoch 5/15  Iteration 2819/8505 Training loss: 1.2539 9.2265 sec/batch\n",
      "Epoch 5/15  Iteration 2820/8505 Training loss: 1.2539 9.2967 sec/batch\n",
      "Epoch 5/15  Iteration 2821/8505 Training loss: 1.2538 9.2492 sec/batch\n",
      "Epoch 5/15  Iteration 2822/8505 Training loss: 1.2539 9.4235 sec/batch\n",
      "Epoch 5/15  Iteration 2823/8505 Training loss: 1.2538 9.4959 sec/batch\n",
      "Epoch 5/15  Iteration 2824/8505 Training loss: 1.2538 9.5355 sec/batch\n",
      "Epoch 5/15  Iteration 2825/8505 Training loss: 1.2538 9.2798 sec/batch\n",
      "Epoch 5/15  Iteration 2826/8505 Training loss: 1.2538 9.2258 sec/batch\n",
      "Epoch 5/15  Iteration 2827/8505 Training loss: 1.2539 9.2233 sec/batch\n",
      "Epoch 5/15  Iteration 2828/8505 Training loss: 1.2538 9.4180 sec/batch\n",
      "Epoch 5/15  Iteration 2829/8505 Training loss: 1.2538 9.2482 sec/batch\n",
      "Epoch 5/15  Iteration 2830/8505 Training loss: 1.2538 9.1975 sec/batch\n",
      "Epoch 5/15  Iteration 2831/8505 Training loss: 1.2537 9.3822 sec/batch\n",
      "Epoch 5/15  Iteration 2832/8505 Training loss: 1.2536 9.2978 sec/batch\n",
      "Epoch 5/15  Iteration 2833/8505 Training loss: 1.2535 9.2437 sec/batch\n",
      "Epoch 5/15  Iteration 2834/8505 Training loss: 1.2535 9.2702 sec/batch\n",
      "Epoch 5/15  Iteration 2835/8505 Training loss: 1.2534 9.3631 sec/batch\n",
      "Epoch 6/15  Iteration 2836/8505 Training loss: 1.3484 9.2919 sec/batch\n",
      "Epoch 6/15  Iteration 2837/8505 Training loss: 1.2981 9.2824 sec/batch\n",
      "Epoch 6/15  Iteration 2838/8505 Training loss: 1.2840 9.2130 sec/batch\n",
      "Epoch 6/15  Iteration 2839/8505 Training loss: 1.2782 9.4321 sec/batch\n",
      "Epoch 6/15  Iteration 2840/8505 Training loss: 1.2765 9.4016 sec/batch\n",
      "Epoch 6/15  Iteration 2841/8505 Training loss: 1.2714 9.2324 sec/batch\n",
      "Epoch 6/15  Iteration 2842/8505 Training loss: 1.2640 9.3809 sec/batch\n",
      "Epoch 6/15  Iteration 2843/8505 Training loss: 1.2586 9.3263 sec/batch\n",
      "Epoch 6/15  Iteration 2844/8505 Training loss: 1.2591 9.3782 sec/batch\n",
      "Epoch 6/15  Iteration 2845/8505 Training loss: 1.2494 9.2689 sec/batch\n",
      "Epoch 6/15  Iteration 2846/8505 Training loss: 1.2512 9.2530 sec/batch\n",
      "Epoch 6/15  Iteration 2847/8505 Training loss: 1.2519 9.2657 sec/batch\n",
      "Epoch 6/15  Iteration 2848/8505 Training loss: 1.2507 9.2742 sec/batch\n",
      "Epoch 6/15  Iteration 2849/8505 Training loss: 1.2525 9.3352 sec/batch\n",
      "Epoch 6/15  Iteration 2850/8505 Training loss: 1.2506 9.3429 sec/batch\n",
      "Epoch 6/15  Iteration 2851/8505 Training loss: 1.2491 9.3297 sec/batch\n",
      "Epoch 6/15  Iteration 2852/8505 Training loss: 1.2478 9.2929 sec/batch\n",
      "Epoch 6/15  Iteration 2853/8505 Training loss: 1.2483 9.2563 sec/batch\n",
      "Epoch 6/15  Iteration 2854/8505 Training loss: 1.2469 9.2888 sec/batch\n",
      "Epoch 6/15  Iteration 2855/8505 Training loss: 1.2462 9.3408 sec/batch\n",
      "Epoch 6/15  Iteration 2856/8505 Training loss: 1.2463 9.2727 sec/batch\n",
      "Epoch 6/15  Iteration 2857/8505 Training loss: 1.2448 9.2863 sec/batch\n",
      "Epoch 6/15  Iteration 2858/8505 Training loss: 1.2435 9.2262 sec/batch\n",
      "Epoch 6/15  Iteration 2859/8505 Training loss: 1.2438 9.3253 sec/batch\n",
      "Epoch 6/15  Iteration 2860/8505 Training loss: 1.2440 9.2215 sec/batch\n",
      "Epoch 6/15  Iteration 2861/8505 Training loss: 1.2431 9.2846 sec/batch\n",
      "Epoch 6/15  Iteration 2862/8505 Training loss: 1.2418 9.2249 sec/batch\n",
      "Epoch 6/15  Iteration 2863/8505 Training loss: 1.2399 9.2615 sec/batch\n",
      "Epoch 6/15  Iteration 2864/8505 Training loss: 1.2404 9.2484 sec/batch\n",
      "Epoch 6/15  Iteration 2865/8505 Training loss: 1.2400 9.3465 sec/batch\n",
      "Epoch 6/15  Iteration 2866/8505 Training loss: 1.2389 9.2796 sec/batch\n",
      "Epoch 6/15  Iteration 2867/8505 Training loss: 1.2377 9.4541 sec/batch\n",
      "Epoch 6/15  Iteration 2868/8505 Training loss: 1.2376 9.5014 sec/batch\n",
      "Epoch 6/15  Iteration 2869/8505 Training loss: 1.2381 9.4292 sec/batch\n",
      "Epoch 6/15  Iteration 2870/8505 Training loss: 1.2377 9.3596 sec/batch\n",
      "Epoch 6/15  Iteration 2871/8505 Training loss: 1.2370 9.3924 sec/batch\n",
      "Epoch 6/15  Iteration 2872/8505 Training loss: 1.2374 9.2748 sec/batch\n",
      "Epoch 6/15  Iteration 2873/8505 Training loss: 1.2372 9.2260 sec/batch\n",
      "Epoch 6/15  Iteration 2874/8505 Training loss: 1.2368 9.3134 sec/batch\n",
      "Epoch 6/15  Iteration 2875/8505 Training loss: 1.2360 9.3117 sec/batch\n",
      "Epoch 6/15  Iteration 2876/8505 Training loss: 1.2354 9.3156 sec/batch\n",
      "Epoch 6/15  Iteration 2877/8505 Training loss: 1.2355 9.2519 sec/batch\n",
      "Epoch 6/15  Iteration 2878/8505 Training loss: 1.2358 9.2820 sec/batch\n",
      "Epoch 6/15  Iteration 2879/8505 Training loss: 1.2352 9.2945 sec/batch\n",
      "Epoch 6/15  Iteration 2880/8505 Training loss: 1.2352 9.3008 sec/batch\n",
      "Epoch 6/15  Iteration 2881/8505 Training loss: 1.2352 9.2728 sec/batch\n",
      "Epoch 6/15  Iteration 2882/8505 Training loss: 1.2354 9.4637 sec/batch\n",
      "Epoch 6/15  Iteration 2883/8505 Training loss: 1.2352 9.3983 sec/batch\n",
      "Epoch 6/15  Iteration 2884/8505 Training loss: 1.2345 9.2687 sec/batch\n",
      "Epoch 6/15  Iteration 2885/8505 Training loss: 1.2336 9.2808 sec/batch\n",
      "Epoch 6/15  Iteration 2886/8505 Training loss: 1.2332 9.3026 sec/batch\n",
      "Epoch 6/15  Iteration 2887/8505 Training loss: 1.2335 9.1627 sec/batch\n",
      "Epoch 6/15  Iteration 2888/8505 Training loss: 1.2329 9.2794 sec/batch\n",
      "Epoch 6/15  Iteration 2889/8505 Training loss: 1.2330 9.2997 sec/batch\n",
      "Epoch 6/15  Iteration 2890/8505 Training loss: 1.2332 9.2654 sec/batch\n",
      "Epoch 6/15  Iteration 2891/8505 Training loss: 1.2329 9.2940 sec/batch\n",
      "Epoch 6/15  Iteration 2892/8505 Training loss: 1.2326 9.2492 sec/batch\n",
      "Epoch 6/15  Iteration 2893/8505 Training loss: 1.2327 9.3263 sec/batch\n",
      "Epoch 6/15  Iteration 2894/8505 Training loss: 1.2337 9.3356 sec/batch\n",
      "Epoch 6/15  Iteration 2895/8505 Training loss: 1.2344 9.4328 sec/batch\n",
      "Epoch 6/15  Iteration 2896/8505 Training loss: 1.2345 9.3379 sec/batch\n",
      "Epoch 6/15  Iteration 2897/8505 Training loss: 1.2345 9.2179 sec/batch\n",
      "Epoch 6/15  Iteration 2898/8505 Training loss: 1.2341 9.3217 sec/batch\n",
      "Epoch 6/15  Iteration 2899/8505 Training loss: 1.2339 9.2282 sec/batch\n",
      "Epoch 6/15  Iteration 2900/8505 Training loss: 1.2337 9.2540 sec/batch\n",
      "Epoch 6/15  Iteration 2901/8505 Training loss: 1.2335 9.2511 sec/batch\n",
      "Epoch 6/15  Iteration 2902/8505 Training loss: 1.2336 9.3306 sec/batch\n",
      "Epoch 6/15  Iteration 2903/8505 Training loss: 1.2336 9.2757 sec/batch\n",
      "Epoch 6/15  Iteration 2904/8505 Training loss: 1.2336 9.4227 sec/batch\n",
      "Epoch 6/15  Iteration 2905/8505 Training loss: 1.2332 9.3886 sec/batch\n",
      "Epoch 6/15  Iteration 2906/8505 Training loss: 1.2330 9.2807 sec/batch\n",
      "Epoch 6/15  Iteration 2907/8505 Training loss: 1.2327 9.4188 sec/batch\n",
      "Epoch 6/15  Iteration 2908/8505 Training loss: 1.2329 9.3296 sec/batch\n",
      "Epoch 6/15  Iteration 2909/8505 Training loss: 1.2333 9.2322 sec/batch\n",
      "Epoch 6/15  Iteration 2910/8505 Training loss: 1.2333 9.3521 sec/batch\n",
      "Epoch 6/15  Iteration 2911/8505 Training loss: 1.2332 9.2615 sec/batch\n",
      "Epoch 6/15  Iteration 2912/8505 Training loss: 1.2331 9.2330 sec/batch\n",
      "Epoch 6/15  Iteration 2913/8505 Training loss: 1.2330 9.2322 sec/batch\n",
      "Epoch 6/15  Iteration 2914/8505 Training loss: 1.2331 9.2279 sec/batch\n",
      "Epoch 6/15  Iteration 2915/8505 Training loss: 1.2326 9.2632 sec/batch\n",
      "Epoch 6/15  Iteration 2916/8505 Training loss: 1.2325 9.2786 sec/batch\n",
      "Epoch 6/15  Iteration 2917/8505 Training loss: 1.2320 9.2938 sec/batch\n",
      "Epoch 6/15  Iteration 2918/8505 Training loss: 1.2318 9.2902 sec/batch\n",
      "Epoch 6/15  Iteration 2919/8505 Training loss: 1.2315 9.2597 sec/batch\n",
      "Epoch 6/15  Iteration 2920/8505 Training loss: 1.2317 9.2373 sec/batch\n",
      "Epoch 6/15  Iteration 2921/8505 Training loss: 1.2319 9.2878 sec/batch\n",
      "Epoch 6/15  Iteration 2922/8505 Training loss: 1.2320 9.3545 sec/batch\n",
      "Epoch 6/15  Iteration 2923/8505 Training loss: 1.2321 9.2563 sec/batch\n",
      "Epoch 6/15  Iteration 2924/8505 Training loss: 1.2319 9.2404 sec/batch\n",
      "Epoch 6/15  Iteration 2925/8505 Training loss: 1.2317 9.2493 sec/batch\n",
      "Epoch 6/15  Iteration 2926/8505 Training loss: 1.2315 9.3412 sec/batch\n",
      "Epoch 6/15  Iteration 2927/8505 Training loss: 1.2315 9.2835 sec/batch\n",
      "Epoch 6/15  Iteration 2928/8505 Training loss: 1.2318 9.3265 sec/batch\n",
      "Epoch 6/15  Iteration 2929/8505 Training loss: 1.2321 9.3000 sec/batch\n",
      "Epoch 6/15  Iteration 2930/8505 Training loss: 1.2321 9.3122 sec/batch\n",
      "Epoch 6/15  Iteration 2931/8505 Training loss: 1.2326 9.2561 sec/batch\n",
      "Epoch 6/15  Iteration 2932/8505 Training loss: 1.2324 9.3033 sec/batch\n",
      "Epoch 6/15  Iteration 2933/8505 Training loss: 1.2321 9.2724 sec/batch\n",
      "Epoch 6/15  Iteration 2934/8505 Training loss: 1.2319 9.3086 sec/batch\n",
      "Epoch 6/15  Iteration 2935/8505 Training loss: 1.2321 9.3176 sec/batch\n",
      "Epoch 6/15  Iteration 2936/8505 Training loss: 1.2319 9.3032 sec/batch\n",
      "Epoch 6/15  Iteration 2937/8505 Training loss: 1.2319 9.2437 sec/batch\n",
      "Epoch 6/15  Iteration 2938/8505 Training loss: 1.2320 9.2936 sec/batch\n",
      "Epoch 6/15  Iteration 2939/8505 Training loss: 1.2319 9.2501 sec/batch\n",
      "Epoch 6/15  Iteration 2940/8505 Training loss: 1.2321 9.3301 sec/batch\n",
      "Epoch 6/15  Iteration 2941/8505 Training loss: 1.2318 9.2770 sec/batch\n",
      "Epoch 6/15  Iteration 2942/8505 Training loss: 1.2317 9.3205 sec/batch\n",
      "Epoch 6/15  Iteration 2943/8505 Training loss: 1.2315 9.1798 sec/batch\n",
      "Epoch 6/15  Iteration 2944/8505 Training loss: 1.2312 9.3050 sec/batch\n",
      "Epoch 6/15  Iteration 2945/8505 Training loss: 1.2311 9.2513 sec/batch\n",
      "Epoch 6/15  Iteration 2946/8505 Training loss: 1.2312 9.3514 sec/batch\n",
      "Epoch 6/15  Iteration 2947/8505 Training loss: 1.2311 9.2528 sec/batch\n",
      "Epoch 6/15  Iteration 2948/8505 Training loss: 1.2309 9.2706 sec/batch\n",
      "Epoch 6/15  Iteration 2949/8505 Training loss: 1.2305 9.2955 sec/batch\n",
      "Epoch 6/15  Iteration 2950/8505 Training loss: 1.2304 9.2819 sec/batch\n",
      "Epoch 6/15  Iteration 2951/8505 Training loss: 1.2301 9.3139 sec/batch\n",
      "Epoch 6/15  Iteration 2952/8505 Training loss: 1.2299 9.3657 sec/batch\n",
      "Epoch 6/15  Iteration 2953/8505 Training loss: 1.2299 9.2983 sec/batch\n",
      "Epoch 6/15  Iteration 2954/8505 Training loss: 1.2300 9.4692 sec/batch\n",
      "Epoch 6/15  Iteration 2955/8505 Training loss: 1.2300 9.3047 sec/batch\n",
      "Epoch 6/15  Iteration 2956/8505 Training loss: 1.2303 9.2469 sec/batch\n",
      "Epoch 6/15  Iteration 2957/8505 Training loss: 1.2304 9.3364 sec/batch\n",
      "Epoch 6/15  Iteration 2958/8505 Training loss: 1.2305 9.3745 sec/batch\n",
      "Epoch 6/15  Iteration 2959/8505 Training loss: 1.2305 9.3333 sec/batch\n",
      "Epoch 6/15  Iteration 2960/8505 Training loss: 1.2305 9.2409 sec/batch\n",
      "Epoch 6/15  Iteration 2961/8505 Training loss: 1.2304 9.2732 sec/batch\n",
      "Epoch 6/15  Iteration 2962/8505 Training loss: 1.2303 9.3129 sec/batch\n",
      "Epoch 6/15  Iteration 2963/8505 Training loss: 1.2303 9.2607 sec/batch\n",
      "Epoch 6/15  Iteration 2964/8505 Training loss: 1.2303 9.2252 sec/batch\n",
      "Epoch 6/15  Iteration 2965/8505 Training loss: 1.2302 9.4079 sec/batch\n",
      "Epoch 6/15  Iteration 2966/8505 Training loss: 1.2301 9.2897 sec/batch\n",
      "Epoch 6/15  Iteration 2967/8505 Training loss: 1.2302 9.2954 sec/batch\n",
      "Epoch 6/15  Iteration 2968/8505 Training loss: 1.2305 9.5203 sec/batch\n",
      "Epoch 6/15  Iteration 2969/8505 Training loss: 1.2308 9.3593 sec/batch\n",
      "Epoch 6/15  Iteration 2970/8505 Training loss: 1.2312 9.3253 sec/batch\n",
      "Epoch 6/15  Iteration 2971/8505 Training loss: 1.2314 9.3006 sec/batch\n",
      "Epoch 6/15  Iteration 2972/8505 Training loss: 1.2314 9.3256 sec/batch\n",
      "Epoch 6/15  Iteration 2973/8505 Training loss: 1.2313 9.2726 sec/batch\n",
      "Epoch 6/15  Iteration 2974/8505 Training loss: 1.2310 9.3757 sec/batch\n",
      "Epoch 6/15  Iteration 2975/8505 Training loss: 1.2310 9.2946 sec/batch\n",
      "Epoch 6/15  Iteration 2976/8505 Training loss: 1.2308 9.2583 sec/batch\n",
      "Epoch 6/15  Iteration 2977/8505 Training loss: 1.2305 9.3300 sec/batch\n",
      "Epoch 6/15  Iteration 2978/8505 Training loss: 1.2304 9.2454 sec/batch\n",
      "Epoch 6/15  Iteration 2979/8505 Training loss: 1.2304 9.3431 sec/batch\n",
      "Epoch 6/15  Iteration 2980/8505 Training loss: 1.2302 9.2375 sec/batch\n",
      "Epoch 6/15  Iteration 2981/8505 Training loss: 1.2303 9.3418 sec/batch\n",
      "Epoch 6/15  Iteration 2982/8505 Training loss: 1.2301 9.3276 sec/batch\n",
      "Epoch 6/15  Iteration 2983/8505 Training loss: 1.2300 9.3218 sec/batch\n",
      "Epoch 6/15  Iteration 2984/8505 Training loss: 1.2298 9.2667 sec/batch\n",
      "Epoch 6/15  Iteration 2985/8505 Training loss: 1.2297 9.3440 sec/batch\n",
      "Epoch 6/15  Iteration 2986/8505 Training loss: 1.2295 9.3022 sec/batch\n",
      "Epoch 6/15  Iteration 2987/8505 Training loss: 1.2291 9.1902 sec/batch\n",
      "Epoch 6/15  Iteration 2988/8505 Training loss: 1.2289 9.2862 sec/batch\n",
      "Epoch 6/15  Iteration 2989/8505 Training loss: 1.2289 9.3649 sec/batch\n",
      "Epoch 6/15  Iteration 2990/8505 Training loss: 1.2287 9.2291 sec/batch\n",
      "Epoch 6/15  Iteration 2991/8505 Training loss: 1.2286 9.2483 sec/batch\n",
      "Epoch 6/15  Iteration 2992/8505 Training loss: 1.2288 9.3006 sec/batch\n",
      "Epoch 6/15  Iteration 2993/8505 Training loss: 1.2288 9.2559 sec/batch\n",
      "Epoch 6/15  Iteration 2994/8505 Training loss: 1.2291 9.2932 sec/batch\n",
      "Epoch 6/15  Iteration 2995/8505 Training loss: 1.2289 9.3623 sec/batch\n",
      "Epoch 6/15  Iteration 2996/8505 Training loss: 1.2290 9.2648 sec/batch\n",
      "Epoch 6/15  Iteration 2997/8505 Training loss: 1.2289 9.3630 sec/batch\n",
      "Epoch 6/15  Iteration 2998/8505 Training loss: 1.2289 9.3121 sec/batch\n",
      "Epoch 6/15  Iteration 2999/8505 Training loss: 1.2288 9.2647 sec/batch\n",
      "Epoch 6/15  Iteration 3000/8505 Training loss: 1.2290 9.2975 sec/batch\n",
      "Validation loss: 1.2024 Saving checkpoint!\n",
      "Epoch 6/15  Iteration 3001/8505 Training loss: 1.2296 9.3104 sec/batch\n",
      "Epoch 6/15  Iteration 3002/8505 Training loss: 1.2295 9.3811 sec/batch\n",
      "Epoch 6/15  Iteration 3003/8505 Training loss: 1.2293 9.3302 sec/batch\n",
      "Epoch 6/15  Iteration 3004/8505 Training loss: 1.2294 9.3864 sec/batch\n",
      "Epoch 6/15  Iteration 3005/8505 Training loss: 1.2295 9.2538 sec/batch\n",
      "Epoch 6/15  Iteration 3006/8505 Training loss: 1.2298 9.3289 sec/batch\n",
      "Epoch 6/15  Iteration 3007/8505 Training loss: 1.2298 9.3627 sec/batch\n",
      "Epoch 6/15  Iteration 3008/8505 Training loss: 1.2296 9.2240 sec/batch\n",
      "Epoch 6/15  Iteration 3009/8505 Training loss: 1.2294 9.2504 sec/batch\n",
      "Epoch 6/15  Iteration 3010/8505 Training loss: 1.2294 9.3067 sec/batch\n",
      "Epoch 6/15  Iteration 3011/8505 Training loss: 1.2295 9.2884 sec/batch\n",
      "Epoch 6/15  Iteration 3012/8505 Training loss: 1.2296 9.3177 sec/batch\n",
      "Epoch 6/15  Iteration 3013/8505 Training loss: 1.2296 9.2532 sec/batch\n",
      "Epoch 6/15  Iteration 3014/8505 Training loss: 1.2296 9.3146 sec/batch\n",
      "Epoch 6/15  Iteration 3015/8505 Training loss: 1.2297 9.2957 sec/batch\n",
      "Epoch 6/15  Iteration 3016/8505 Training loss: 1.2296 9.4099 sec/batch\n",
      "Epoch 6/15  Iteration 3017/8505 Training loss: 1.2295 9.3085 sec/batch\n",
      "Epoch 6/15  Iteration 3018/8505 Training loss: 1.2295 9.1835 sec/batch\n",
      "Epoch 6/15  Iteration 3019/8505 Training loss: 1.2296 9.2887 sec/batch\n",
      "Epoch 6/15  Iteration 3020/8505 Training loss: 1.2296 9.3562 sec/batch\n",
      "Epoch 6/15  Iteration 3021/8505 Training loss: 1.2294 9.3618 sec/batch\n",
      "Epoch 6/15  Iteration 3022/8505 Training loss: 1.2294 9.2597 sec/batch\n",
      "Epoch 6/15  Iteration 3023/8505 Training loss: 1.2292 9.3211 sec/batch\n",
      "Epoch 6/15  Iteration 3024/8505 Training loss: 1.2294 9.2690 sec/batch\n",
      "Epoch 6/15  Iteration 3025/8505 Training loss: 1.2291 9.2094 sec/batch\n",
      "Epoch 6/15  Iteration 3026/8505 Training loss: 1.2291 9.3221 sec/batch\n",
      "Epoch 6/15  Iteration 3027/8505 Training loss: 1.2290 9.3794 sec/batch\n",
      "Epoch 6/15  Iteration 3028/8505 Training loss: 1.2292 9.6426 sec/batch\n",
      "Epoch 6/15  Iteration 3029/8505 Training loss: 1.2292 9.3642 sec/batch\n",
      "Epoch 6/15  Iteration 3030/8505 Training loss: 1.2291 9.3331 sec/batch\n",
      "Epoch 6/15  Iteration 3031/8505 Training loss: 1.2291 9.2854 sec/batch\n",
      "Epoch 6/15  Iteration 3032/8505 Training loss: 1.2290 9.2979 sec/batch\n",
      "Epoch 6/15  Iteration 3033/8505 Training loss: 1.2291 9.2915 sec/batch\n",
      "Epoch 6/15  Iteration 3034/8505 Training loss: 1.2292 9.2856 sec/batch\n",
      "Epoch 6/15  Iteration 3035/8505 Training loss: 1.2291 9.2729 sec/batch\n",
      "Epoch 6/15  Iteration 3036/8505 Training loss: 1.2291 9.2524 sec/batch\n",
      "Epoch 6/15  Iteration 3037/8505 Training loss: 1.2291 9.2650 sec/batch\n",
      "Epoch 6/15  Iteration 3038/8505 Training loss: 1.2291 9.3044 sec/batch\n",
      "Epoch 6/15  Iteration 3039/8505 Training loss: 1.2290 9.4070 sec/batch\n",
      "Epoch 6/15  Iteration 3040/8505 Training loss: 1.2287 9.3239 sec/batch\n",
      "Epoch 6/15  Iteration 3041/8505 Training loss: 1.2288 9.3594 sec/batch\n",
      "Epoch 6/15  Iteration 3042/8505 Training loss: 1.2288 9.3288 sec/batch\n",
      "Epoch 6/15  Iteration 3043/8505 Training loss: 1.2287 9.2748 sec/batch\n",
      "Epoch 6/15  Iteration 3044/8505 Training loss: 1.2285 9.2441 sec/batch\n",
      "Epoch 6/15  Iteration 3045/8505 Training loss: 1.2283 9.2501 sec/batch\n",
      "Epoch 6/15  Iteration 3046/8505 Training loss: 1.2281 9.2551 sec/batch\n",
      "Epoch 6/15  Iteration 3047/8505 Training loss: 1.2281 9.3122 sec/batch\n",
      "Epoch 6/15  Iteration 3048/8505 Training loss: 1.2280 9.2930 sec/batch\n",
      "Epoch 6/15  Iteration 3049/8505 Training loss: 1.2280 9.2957 sec/batch\n",
      "Epoch 6/15  Iteration 3050/8505 Training loss: 1.2279 9.3257 sec/batch\n",
      "Epoch 6/15  Iteration 3051/8505 Training loss: 1.2279 9.2323 sec/batch\n",
      "Epoch 6/15  Iteration 3052/8505 Training loss: 1.2278 9.3363 sec/batch\n",
      "Epoch 6/15  Iteration 3053/8505 Training loss: 1.2277 9.2999 sec/batch\n",
      "Epoch 6/15  Iteration 3054/8505 Training loss: 1.2275 9.4580 sec/batch\n",
      "Epoch 6/15  Iteration 3055/8505 Training loss: 1.2272 9.2474 sec/batch\n",
      "Epoch 6/15  Iteration 3056/8505 Training loss: 1.2271 9.2710 sec/batch\n",
      "Epoch 6/15  Iteration 3057/8505 Training loss: 1.2270 9.2807 sec/batch\n",
      "Epoch 6/15  Iteration 3058/8505 Training loss: 1.2268 9.2741 sec/batch\n",
      "Epoch 6/15  Iteration 3059/8505 Training loss: 1.2267 9.3296 sec/batch\n",
      "Epoch 6/15  Iteration 3060/8505 Training loss: 1.2266 9.3106 sec/batch\n",
      "Epoch 6/15  Iteration 3061/8505 Training loss: 1.2265 9.3093 sec/batch\n",
      "Epoch 6/15  Iteration 3062/8505 Training loss: 1.2264 9.3266 sec/batch\n",
      "Epoch 6/15  Iteration 3063/8505 Training loss: 1.2262 9.2376 sec/batch\n",
      "Epoch 6/15  Iteration 3064/8505 Training loss: 1.2261 9.2824 sec/batch\n",
      "Epoch 6/15  Iteration 3065/8505 Training loss: 1.2260 9.2785 sec/batch\n",
      "Epoch 6/15  Iteration 3066/8505 Training loss: 1.2259 9.3274 sec/batch\n",
      "Epoch 6/15  Iteration 3067/8505 Training loss: 1.2258 9.4266 sec/batch\n",
      "Epoch 6/15  Iteration 3068/8505 Training loss: 1.2258 9.3010 sec/batch\n",
      "Epoch 6/15  Iteration 3069/8505 Training loss: 1.2255 9.2788 sec/batch\n",
      "Epoch 6/15  Iteration 3070/8505 Training loss: 1.2255 9.2181 sec/batch\n",
      "Epoch 6/15  Iteration 3071/8505 Training loss: 1.2253 9.2173 sec/batch\n",
      "Epoch 6/15  Iteration 3072/8505 Training loss: 1.2253 9.3361 sec/batch\n",
      "Epoch 6/15  Iteration 3073/8505 Training loss: 1.2253 9.3977 sec/batch\n",
      "Epoch 6/15  Iteration 3074/8505 Training loss: 1.2251 9.2818 sec/batch\n",
      "Epoch 6/15  Iteration 3075/8505 Training loss: 1.2250 9.2765 sec/batch\n",
      "Epoch 6/15  Iteration 3076/8505 Training loss: 1.2249 9.3206 sec/batch\n",
      "Epoch 6/15  Iteration 3077/8505 Training loss: 1.2250 9.2417 sec/batch\n",
      "Epoch 6/15  Iteration 3078/8505 Training loss: 1.2249 9.4665 sec/batch\n",
      "Epoch 6/15  Iteration 3079/8505 Training loss: 1.2249 9.3132 sec/batch\n",
      "Epoch 6/15  Iteration 3080/8505 Training loss: 1.2249 9.4066 sec/batch\n",
      "Epoch 6/15  Iteration 3081/8505 Training loss: 1.2248 9.5670 sec/batch\n",
      "Epoch 6/15  Iteration 3082/8505 Training loss: 1.2248 9.3110 sec/batch\n",
      "Epoch 6/15  Iteration 3083/8505 Training loss: 1.2249 9.2503 sec/batch\n",
      "Epoch 6/15  Iteration 3084/8505 Training loss: 1.2249 9.3353 sec/batch\n",
      "Epoch 6/15  Iteration 3085/8505 Training loss: 1.2249 9.3020 sec/batch\n",
      "Epoch 6/15  Iteration 3086/8505 Training loss: 1.2249 9.2936 sec/batch\n",
      "Epoch 6/15  Iteration 3087/8505 Training loss: 1.2247 9.2953 sec/batch\n",
      "Epoch 6/15  Iteration 3088/8505 Training loss: 1.2246 9.2970 sec/batch\n",
      "Epoch 6/15  Iteration 3089/8505 Training loss: 1.2245 9.2326 sec/batch\n",
      "Epoch 6/15  Iteration 3090/8505 Training loss: 1.2246 9.2491 sec/batch\n",
      "Epoch 6/15  Iteration 3091/8505 Training loss: 1.2246 9.3463 sec/batch\n",
      "Epoch 6/15  Iteration 3092/8505 Training loss: 1.2247 9.2789 sec/batch\n",
      "Epoch 6/15  Iteration 3093/8505 Training loss: 1.2246 9.3432 sec/batch\n",
      "Epoch 6/15  Iteration 3094/8505 Training loss: 1.2245 9.2222 sec/batch\n",
      "Epoch 6/15  Iteration 3095/8505 Training loss: 1.2245 9.2322 sec/batch\n",
      "Epoch 6/15  Iteration 3096/8505 Training loss: 1.2244 9.2419 sec/batch\n",
      "Epoch 6/15  Iteration 3097/8505 Training loss: 1.2245 9.2970 sec/batch\n",
      "Epoch 6/15  Iteration 3098/8505 Training loss: 1.2245 9.2277 sec/batch\n",
      "Epoch 6/15  Iteration 3099/8505 Training loss: 1.2245 9.5989 sec/batch\n",
      "Epoch 6/15  Iteration 3100/8505 Training loss: 1.2244 9.3769 sec/batch\n",
      "Epoch 6/15  Iteration 3101/8505 Training loss: 1.2243 9.3261 sec/batch\n",
      "Epoch 6/15  Iteration 3102/8505 Training loss: 1.2242 9.2956 sec/batch\n",
      "Epoch 6/15  Iteration 3103/8505 Training loss: 1.2242 9.2887 sec/batch\n",
      "Epoch 6/15  Iteration 3104/8505 Training loss: 1.2241 9.2810 sec/batch\n",
      "Epoch 6/15  Iteration 3105/8505 Training loss: 1.2241 9.3949 sec/batch\n",
      "Epoch 6/15  Iteration 3106/8505 Training loss: 1.2240 9.3687 sec/batch\n",
      "Epoch 6/15  Iteration 3107/8505 Training loss: 1.2239 9.2588 sec/batch\n",
      "Epoch 6/15  Iteration 3108/8505 Training loss: 1.2239 9.4203 sec/batch\n",
      "Epoch 6/15  Iteration 3109/8505 Training loss: 1.2239 9.2523 sec/batch\n",
      "Epoch 6/15  Iteration 3110/8505 Training loss: 1.2239 9.3210 sec/batch\n",
      "Epoch 6/15  Iteration 3111/8505 Training loss: 1.2240 9.2941 sec/batch\n",
      "Epoch 6/15  Iteration 3112/8505 Training loss: 1.2240 9.2832 sec/batch\n",
      "Epoch 6/15  Iteration 3113/8505 Training loss: 1.2240 9.3245 sec/batch\n",
      "Epoch 6/15  Iteration 3114/8505 Training loss: 1.2240 9.2709 sec/batch\n",
      "Epoch 6/15  Iteration 3115/8505 Training loss: 1.2240 9.6616 sec/batch\n",
      "Epoch 6/15  Iteration 3116/8505 Training loss: 1.2238 9.4667 sec/batch\n",
      "Epoch 6/15  Iteration 3117/8505 Training loss: 1.2237 9.2992 sec/batch\n",
      "Epoch 6/15  Iteration 3118/8505 Training loss: 1.2237 9.3062 sec/batch\n",
      "Epoch 6/15  Iteration 3119/8505 Training loss: 1.2236 9.3174 sec/batch\n",
      "Epoch 6/15  Iteration 3120/8505 Training loss: 1.2235 9.2899 sec/batch\n",
      "Epoch 6/15  Iteration 3121/8505 Training loss: 1.2235 9.4419 sec/batch\n",
      "Epoch 6/15  Iteration 3122/8505 Training loss: 1.2236 9.3154 sec/batch\n",
      "Epoch 6/15  Iteration 3123/8505 Training loss: 1.2237 9.3093 sec/batch\n",
      "Epoch 6/15  Iteration 3124/8505 Training loss: 1.2236 9.3310 sec/batch\n",
      "Epoch 6/15  Iteration 3125/8505 Training loss: 1.2235 9.3540 sec/batch\n",
      "Epoch 6/15  Iteration 3126/8505 Training loss: 1.2235 9.2852 sec/batch\n",
      "Epoch 6/15  Iteration 3127/8505 Training loss: 1.2233 9.2965 sec/batch\n",
      "Epoch 6/15  Iteration 3128/8505 Training loss: 1.2232 9.2781 sec/batch\n",
      "Epoch 6/15  Iteration 3129/8505 Training loss: 1.2232 9.2558 sec/batch\n",
      "Epoch 6/15  Iteration 3130/8505 Training loss: 1.2233 9.2838 sec/batch\n",
      "Epoch 6/15  Iteration 3131/8505 Training loss: 1.2231 9.2664 sec/batch\n",
      "Epoch 6/15  Iteration 3132/8505 Training loss: 1.2231 9.3029 sec/batch\n",
      "Epoch 6/15  Iteration 3133/8505 Training loss: 1.2230 9.3103 sec/batch\n",
      "Epoch 6/15  Iteration 3134/8505 Training loss: 1.2231 9.2876 sec/batch\n",
      "Epoch 6/15  Iteration 3135/8505 Training loss: 1.2231 9.3029 sec/batch\n",
      "Epoch 6/15  Iteration 3136/8505 Training loss: 1.2231 9.3269 sec/batch\n",
      "Epoch 6/15  Iteration 3137/8505 Training loss: 1.2229 9.2560 sec/batch\n",
      "Epoch 6/15  Iteration 3138/8505 Training loss: 1.2229 9.3738 sec/batch\n",
      "Epoch 6/15  Iteration 3139/8505 Training loss: 1.2228 9.2716 sec/batch\n",
      "Epoch 6/15  Iteration 3140/8505 Training loss: 1.2229 9.3279 sec/batch\n",
      "Epoch 6/15  Iteration 3141/8505 Training loss: 1.2228 9.2538 sec/batch\n",
      "Epoch 6/15  Iteration 3142/8505 Training loss: 1.2227 9.2656 sec/batch\n",
      "Epoch 6/15  Iteration 3143/8505 Training loss: 1.2226 9.2639 sec/batch\n",
      "Epoch 6/15  Iteration 3144/8505 Training loss: 1.2227 9.2882 sec/batch\n",
      "Epoch 6/15  Iteration 3145/8505 Training loss: 1.2227 9.1724 sec/batch\n",
      "Epoch 6/15  Iteration 3146/8505 Training loss: 1.2227 9.4351 sec/batch\n",
      "Epoch 6/15  Iteration 3147/8505 Training loss: 1.2227 9.4161 sec/batch\n",
      "Epoch 6/15  Iteration 3148/8505 Training loss: 1.2226 9.3064 sec/batch\n",
      "Epoch 6/15  Iteration 3149/8505 Training loss: 1.2226 9.3683 sec/batch\n",
      "Epoch 6/15  Iteration 3150/8505 Training loss: 1.2225 9.4471 sec/batch\n",
      "Epoch 6/15  Iteration 3151/8505 Training loss: 1.2224 9.5976 sec/batch\n",
      "Epoch 6/15  Iteration 3152/8505 Training loss: 1.2224 9.6078 sec/batch\n",
      "Epoch 6/15  Iteration 3153/8505 Training loss: 1.2224 9.2913 sec/batch\n",
      "Epoch 6/15  Iteration 3154/8505 Training loss: 1.2224 9.3820 sec/batch\n",
      "Epoch 6/15  Iteration 3155/8505 Training loss: 1.2224 9.4404 sec/batch\n",
      "Epoch 6/15  Iteration 3156/8505 Training loss: 1.2224 9.2863 sec/batch\n",
      "Epoch 6/15  Iteration 3157/8505 Training loss: 1.2223 9.3044 sec/batch\n",
      "Epoch 6/15  Iteration 3158/8505 Training loss: 1.2223 9.3155 sec/batch\n",
      "Epoch 6/15  Iteration 3159/8505 Training loss: 1.2224 9.3105 sec/batch\n",
      "Epoch 6/15  Iteration 3160/8505 Training loss: 1.2225 9.2402 sec/batch\n",
      "Epoch 6/15  Iteration 3161/8505 Training loss: 1.2225 9.2283 sec/batch\n",
      "Epoch 6/15  Iteration 3162/8505 Training loss: 1.2225 9.2512 sec/batch\n",
      "Epoch 6/15  Iteration 3163/8505 Training loss: 1.2225 9.3104 sec/batch\n",
      "Epoch 6/15  Iteration 3164/8505 Training loss: 1.2224 9.2050 sec/batch\n",
      "Epoch 6/15  Iteration 3165/8505 Training loss: 1.2224 9.3000 sec/batch\n",
      "Epoch 6/15  Iteration 3166/8505 Training loss: 1.2224 9.3235 sec/batch\n",
      "Epoch 6/15  Iteration 3167/8505 Training loss: 1.2224 9.2629 sec/batch\n",
      "Epoch 6/15  Iteration 3168/8505 Training loss: 1.2224 9.2752 sec/batch\n",
      "Epoch 6/15  Iteration 3169/8505 Training loss: 1.2223 9.2826 sec/batch\n",
      "Epoch 6/15  Iteration 3170/8505 Training loss: 1.2223 9.2929 sec/batch\n",
      "Epoch 6/15  Iteration 3171/8505 Training loss: 1.2223 9.3338 sec/batch\n",
      "Epoch 6/15  Iteration 3172/8505 Training loss: 1.2223 9.3471 sec/batch\n",
      "Epoch 6/15  Iteration 3173/8505 Training loss: 1.2224 9.2717 sec/batch\n",
      "Epoch 6/15  Iteration 3174/8505 Training loss: 1.2225 9.3237 sec/batch\n",
      "Epoch 6/15  Iteration 3175/8505 Training loss: 1.2224 9.3139 sec/batch\n",
      "Epoch 6/15  Iteration 3176/8505 Training loss: 1.2224 9.2226 sec/batch\n",
      "Epoch 6/15  Iteration 3177/8505 Training loss: 1.2223 9.2881 sec/batch\n",
      "Epoch 6/15  Iteration 3178/8505 Training loss: 1.2222 9.3926 sec/batch\n",
      "Epoch 6/15  Iteration 3179/8505 Training loss: 1.2223 9.2472 sec/batch\n",
      "Epoch 6/15  Iteration 3180/8505 Training loss: 1.2222 9.2224 sec/batch\n",
      "Epoch 6/15  Iteration 3181/8505 Training loss: 1.2222 9.3618 sec/batch\n",
      "Epoch 6/15  Iteration 3182/8505 Training loss: 1.2222 9.2818 sec/batch\n",
      "Epoch 6/15  Iteration 3183/8505 Training loss: 1.2222 9.2221 sec/batch\n",
      "Epoch 6/15  Iteration 3184/8505 Training loss: 1.2221 9.2841 sec/batch\n",
      "Epoch 6/15  Iteration 3185/8505 Training loss: 1.2221 9.3491 sec/batch\n",
      "Epoch 6/15  Iteration 3186/8505 Training loss: 1.2222 9.3015 sec/batch\n",
      "Epoch 6/15  Iteration 3187/8505 Training loss: 1.2221 9.4847 sec/batch\n",
      "Epoch 6/15  Iteration 3188/8505 Training loss: 1.2221 9.4464 sec/batch\n",
      "Epoch 6/15  Iteration 3189/8505 Training loss: 1.2220 9.3615 sec/batch\n",
      "Epoch 6/15  Iteration 3190/8505 Training loss: 1.2221 9.2487 sec/batch\n",
      "Epoch 6/15  Iteration 3191/8505 Training loss: 1.2220 9.3872 sec/batch\n",
      "Epoch 6/15  Iteration 3192/8505 Training loss: 1.2220 9.2910 sec/batch\n",
      "Epoch 6/15  Iteration 3193/8505 Training loss: 1.2220 9.2571 sec/batch\n",
      "Epoch 6/15  Iteration 3194/8505 Training loss: 1.2219 9.2994 sec/batch\n",
      "Epoch 6/15  Iteration 3195/8505 Training loss: 1.2218 9.3532 sec/batch\n",
      "Epoch 6/15  Iteration 3196/8505 Training loss: 1.2218 11.6027 sec/batch\n",
      "Epoch 6/15  Iteration 3197/8505 Training loss: 1.2218 9.3014 sec/batch\n",
      "Epoch 6/15  Iteration 3198/8505 Training loss: 1.2218 9.4558 sec/batch\n",
      "Epoch 6/15  Iteration 3199/8505 Training loss: 1.2217 9.3279 sec/batch\n",
      "Epoch 6/15  Iteration 3200/8505 Training loss: 1.2218 9.2734 sec/batch\n",
      "Validation loss: 1.19309 Saving checkpoint!\n",
      "Epoch 6/15  Iteration 3201/8505 Training loss: 1.2221 9.8354 sec/batch\n",
      "Epoch 6/15  Iteration 3202/8505 Training loss: 1.2222 9.3196 sec/batch\n",
      "Epoch 6/15  Iteration 3203/8505 Training loss: 1.2221 9.2793 sec/batch\n",
      "Epoch 6/15  Iteration 3204/8505 Training loss: 1.2221 9.2739 sec/batch\n",
      "Epoch 6/15  Iteration 3205/8505 Training loss: 1.2220 9.2701 sec/batch\n",
      "Epoch 6/15  Iteration 3206/8505 Training loss: 1.2220 9.3556 sec/batch\n",
      "Epoch 6/15  Iteration 3207/8505 Training loss: 1.2220 9.5679 sec/batch\n",
      "Epoch 6/15  Iteration 3208/8505 Training loss: 1.2221 9.3647 sec/batch\n",
      "Epoch 6/15  Iteration 3209/8505 Training loss: 1.2221 9.3839 sec/batch\n",
      "Epoch 6/15  Iteration 3210/8505 Training loss: 1.2221 9.3792 sec/batch\n",
      "Epoch 6/15  Iteration 3211/8505 Training loss: 1.2221 9.3842 sec/batch\n",
      "Epoch 6/15  Iteration 3212/8505 Training loss: 1.2220 9.3184 sec/batch\n",
      "Epoch 6/15  Iteration 3213/8505 Training loss: 1.2220 9.3003 sec/batch\n",
      "Epoch 6/15  Iteration 3214/8505 Training loss: 1.2221 9.2754 sec/batch\n",
      "Epoch 6/15  Iteration 3215/8505 Training loss: 1.2222 9.2682 sec/batch\n",
      "Epoch 6/15  Iteration 3216/8505 Training loss: 1.2222 9.3141 sec/batch\n",
      "Epoch 6/15  Iteration 3217/8505 Training loss: 1.2221 9.2857 sec/batch\n",
      "Epoch 6/15  Iteration 3218/8505 Training loss: 1.2221 9.3376 sec/batch\n",
      "Epoch 6/15  Iteration 3219/8505 Training loss: 1.2220 9.2990 sec/batch\n",
      "Epoch 6/15  Iteration 3220/8505 Training loss: 1.2219 9.3346 sec/batch\n",
      "Epoch 6/15  Iteration 3221/8505 Training loss: 1.2218 9.3576 sec/batch\n",
      "Epoch 6/15  Iteration 3222/8505 Training loss: 1.2217 9.2556 sec/batch\n",
      "Epoch 6/15  Iteration 3223/8505 Training loss: 1.2216 9.3650 sec/batch\n",
      "Epoch 6/15  Iteration 3224/8505 Training loss: 1.2215 9.3475 sec/batch\n",
      "Epoch 6/15  Iteration 3225/8505 Training loss: 1.2214 9.2518 sec/batch\n",
      "Epoch 6/15  Iteration 3226/8505 Training loss: 1.2213 9.2555 sec/batch\n",
      "Epoch 6/15  Iteration 3227/8505 Training loss: 1.2213 9.3688 sec/batch\n",
      "Epoch 6/15  Iteration 3228/8505 Training loss: 1.2212 9.2563 sec/batch\n",
      "Epoch 6/15  Iteration 3229/8505 Training loss: 1.2212 9.2670 sec/batch\n",
      "Epoch 6/15  Iteration 3230/8505 Training loss: 1.2212 9.3943 sec/batch\n",
      "Epoch 6/15  Iteration 3231/8505 Training loss: 1.2211 9.2046 sec/batch\n",
      "Epoch 6/15  Iteration 3232/8505 Training loss: 1.2212 9.3014 sec/batch\n",
      "Epoch 6/15  Iteration 3233/8505 Training loss: 1.2212 9.3041 sec/batch\n",
      "Epoch 6/15  Iteration 3234/8505 Training loss: 1.2213 9.3132 sec/batch\n",
      "Epoch 6/15  Iteration 3235/8505 Training loss: 1.2212 9.3070 sec/batch\n",
      "Epoch 6/15  Iteration 3236/8505 Training loss: 1.2211 9.3102 sec/batch\n",
      "Epoch 6/15  Iteration 3237/8505 Training loss: 1.2210 9.3175 sec/batch\n",
      "Epoch 6/15  Iteration 3238/8505 Training loss: 1.2208 9.3141 sec/batch\n",
      "Epoch 6/15  Iteration 3239/8505 Training loss: 1.2207 9.4691 sec/batch\n",
      "Epoch 6/15  Iteration 3240/8505 Training loss: 1.2207 9.3753 sec/batch\n",
      "Epoch 6/15  Iteration 3241/8505 Training loss: 1.2207 9.2672 sec/batch\n",
      "Epoch 6/15  Iteration 3242/8505 Training loss: 1.2206 9.3795 sec/batch\n",
      "Epoch 6/15  Iteration 3243/8505 Training loss: 1.2205 9.3520 sec/batch\n",
      "Epoch 6/15  Iteration 3244/8505 Training loss: 1.2205 9.1999 sec/batch\n",
      "Epoch 6/15  Iteration 3245/8505 Training loss: 1.2205 9.3522 sec/batch\n",
      "Epoch 6/15  Iteration 3246/8505 Training loss: 1.2205 9.3647 sec/batch\n",
      "Epoch 6/15  Iteration 3247/8505 Training loss: 1.2205 9.2524 sec/batch\n",
      "Epoch 6/15  Iteration 3248/8505 Training loss: 1.2206 9.2168 sec/batch\n",
      "Epoch 6/15  Iteration 3249/8505 Training loss: 1.2205 9.3435 sec/batch\n",
      "Epoch 6/15  Iteration 3250/8505 Training loss: 1.2203 9.3284 sec/batch\n",
      "Epoch 6/15  Iteration 3251/8505 Training loss: 1.2202 9.2665 sec/batch\n",
      "Epoch 6/15  Iteration 3252/8505 Training loss: 1.2202 9.2374 sec/batch\n",
      "Epoch 6/15  Iteration 3253/8505 Training loss: 1.2201 9.4410 sec/batch\n",
      "Epoch 6/15  Iteration 3254/8505 Training loss: 1.2201 9.2728 sec/batch\n",
      "Epoch 6/15  Iteration 3255/8505 Training loss: 1.2201 9.3173 sec/batch\n",
      "Epoch 6/15  Iteration 3256/8505 Training loss: 1.2201 9.3177 sec/batch\n",
      "Epoch 6/15  Iteration 3257/8505 Training loss: 1.2200 9.3156 sec/batch\n",
      "Epoch 6/15  Iteration 3258/8505 Training loss: 1.2200 9.4851 sec/batch\n",
      "Epoch 6/15  Iteration 3259/8505 Training loss: 1.2200 9.2521 sec/batch\n",
      "Epoch 6/15  Iteration 3260/8505 Training loss: 1.2199 9.4190 sec/batch\n",
      "Epoch 6/15  Iteration 3261/8505 Training loss: 1.2199 9.3956 sec/batch\n",
      "Epoch 6/15  Iteration 3262/8505 Training loss: 1.2199 9.3483 sec/batch\n",
      "Epoch 6/15  Iteration 3263/8505 Training loss: 1.2199 9.3545 sec/batch\n",
      "Epoch 6/15  Iteration 3264/8505 Training loss: 1.2199 9.3366 sec/batch\n",
      "Epoch 6/15  Iteration 3265/8505 Training loss: 1.2199 9.3238 sec/batch\n",
      "Epoch 6/15  Iteration 3266/8505 Training loss: 1.2199 9.2147 sec/batch\n",
      "Epoch 6/15  Iteration 3267/8505 Training loss: 1.2198 9.3078 sec/batch\n",
      "Epoch 6/15  Iteration 3268/8505 Training loss: 1.2197 9.3681 sec/batch\n",
      "Epoch 6/15  Iteration 3269/8505 Training loss: 1.2197 9.3553 sec/batch\n",
      "Epoch 6/15  Iteration 3270/8505 Training loss: 1.2196 9.3688 sec/batch\n",
      "Epoch 6/15  Iteration 3271/8505 Training loss: 1.2195 9.3357 sec/batch\n",
      "Epoch 6/15  Iteration 3272/8505 Training loss: 1.2194 9.3096 sec/batch\n",
      "Epoch 6/15  Iteration 3273/8505 Training loss: 1.2193 9.3099 sec/batch\n",
      "Epoch 6/15  Iteration 3274/8505 Training loss: 1.2193 9.2781 sec/batch\n",
      "Epoch 6/15  Iteration 3275/8505 Training loss: 1.2194 9.3583 sec/batch\n",
      "Epoch 6/15  Iteration 3276/8505 Training loss: 1.2193 9.2430 sec/batch\n",
      "Epoch 6/15  Iteration 3277/8505 Training loss: 1.2192 9.3741 sec/batch\n",
      "Epoch 6/15  Iteration 3278/8505 Training loss: 1.2192 10.4455 sec/batch\n",
      "Epoch 6/15  Iteration 3279/8505 Training loss: 1.2191 9.4483 sec/batch\n",
      "Epoch 6/15  Iteration 3280/8505 Training loss: 1.2191 9.3900 sec/batch\n",
      "Epoch 6/15  Iteration 3281/8505 Training loss: 1.2190 9.3573 sec/batch\n",
      "Epoch 6/15  Iteration 3282/8505 Training loss: 1.2190 9.2931 sec/batch\n",
      "Epoch 6/15  Iteration 3283/8505 Training loss: 1.2189 9.2733 sec/batch\n",
      "Epoch 6/15  Iteration 3284/8505 Training loss: 1.2189 9.3925 sec/batch\n",
      "Epoch 6/15  Iteration 3285/8505 Training loss: 1.2189 9.1876 sec/batch\n",
      "Epoch 6/15  Iteration 3286/8505 Training loss: 1.2187 9.3139 sec/batch\n",
      "Epoch 6/15  Iteration 3287/8505 Training loss: 1.2187 9.2474 sec/batch\n",
      "Epoch 6/15  Iteration 3288/8505 Training loss: 1.2187 9.3248 sec/batch\n",
      "Epoch 6/15  Iteration 3289/8505 Training loss: 1.2186 9.2512 sec/batch\n",
      "Epoch 6/15  Iteration 3290/8505 Training loss: 1.2186 9.2777 sec/batch\n",
      "Epoch 6/15  Iteration 3291/8505 Training loss: 1.2186 9.2820 sec/batch\n",
      "Epoch 6/15  Iteration 3292/8505 Training loss: 1.2186 9.3656 sec/batch\n",
      "Epoch 6/15  Iteration 3293/8505 Training loss: 1.2185 9.2774 sec/batch\n",
      "Epoch 6/15  Iteration 3294/8505 Training loss: 1.2184 9.4201 sec/batch\n",
      "Epoch 6/15  Iteration 3295/8505 Training loss: 1.2184 9.3047 sec/batch\n",
      "Epoch 6/15  Iteration 3296/8505 Training loss: 1.2183 9.2887 sec/batch\n",
      "Epoch 6/15  Iteration 3297/8505 Training loss: 1.2182 9.2414 sec/batch\n",
      "Epoch 6/15  Iteration 3298/8505 Training loss: 1.2182 9.3747 sec/batch\n",
      "Epoch 6/15  Iteration 3299/8505 Training loss: 1.2181 9.3119 sec/batch\n",
      "Epoch 6/15  Iteration 3300/8505 Training loss: 1.2182 9.3151 sec/batch\n",
      "Epoch 6/15  Iteration 3301/8505 Training loss: 1.2182 9.4325 sec/batch\n",
      "Epoch 6/15  Iteration 3302/8505 Training loss: 1.2181 9.4680 sec/batch\n",
      "Epoch 6/15  Iteration 3303/8505 Training loss: 1.2180 9.3506 sec/batch\n",
      "Epoch 6/15  Iteration 3304/8505 Training loss: 1.2180 9.4009 sec/batch\n",
      "Epoch 6/15  Iteration 3305/8505 Training loss: 1.2180 9.3102 sec/batch\n",
      "Epoch 6/15  Iteration 3306/8505 Training loss: 1.2179 9.4012 sec/batch\n",
      "Epoch 6/15  Iteration 3307/8505 Training loss: 1.2178 9.4811 sec/batch\n",
      "Epoch 6/15  Iteration 3308/8505 Training loss: 1.2177 9.9186 sec/batch\n",
      "Epoch 6/15  Iteration 3309/8505 Training loss: 1.2177 9.4236 sec/batch\n",
      "Epoch 6/15  Iteration 3310/8505 Training loss: 1.2177 9.3593 sec/batch\n",
      "Epoch 6/15  Iteration 3311/8505 Training loss: 1.2177 9.6153 sec/batch\n",
      "Epoch 6/15  Iteration 3312/8505 Training loss: 1.2176 9.4027 sec/batch\n",
      "Epoch 6/15  Iteration 3313/8505 Training loss: 1.2176 9.3003 sec/batch\n",
      "Epoch 6/15  Iteration 3314/8505 Training loss: 1.2175 9.3126 sec/batch\n",
      "Epoch 6/15  Iteration 3315/8505 Training loss: 1.2174 9.2617 sec/batch\n",
      "Epoch 6/15  Iteration 3316/8505 Training loss: 1.2174 9.1583 sec/batch\n",
      "Epoch 6/15  Iteration 3317/8505 Training loss: 1.2173 9.2946 sec/batch\n",
      "Epoch 6/15  Iteration 3318/8505 Training loss: 1.2173 9.2232 sec/batch\n",
      "Epoch 6/15  Iteration 3319/8505 Training loss: 1.2173 9.2910 sec/batch\n",
      "Epoch 6/15  Iteration 3320/8505 Training loss: 1.2172 9.3048 sec/batch\n",
      "Epoch 6/15  Iteration 3321/8505 Training loss: 1.2172 9.2978 sec/batch\n",
      "Epoch 6/15  Iteration 3322/8505 Training loss: 1.2172 9.2810 sec/batch\n",
      "Epoch 6/15  Iteration 3323/8505 Training loss: 1.2171 9.2731 sec/batch\n",
      "Epoch 6/15  Iteration 3324/8505 Training loss: 1.2171 9.2575 sec/batch\n",
      "Epoch 6/15  Iteration 3325/8505 Training loss: 1.2171 9.3212 sec/batch\n",
      "Epoch 6/15  Iteration 3326/8505 Training loss: 1.2171 9.3488 sec/batch\n",
      "Epoch 6/15  Iteration 3327/8505 Training loss: 1.2171 10.2150 sec/batch\n",
      "Epoch 6/15  Iteration 3328/8505 Training loss: 1.2170 9.5165 sec/batch\n",
      "Epoch 6/15  Iteration 3329/8505 Training loss: 1.2170 9.2339 sec/batch\n",
      "Epoch 6/15  Iteration 3330/8505 Training loss: 1.2170 9.2719 sec/batch\n",
      "Epoch 6/15  Iteration 3331/8505 Training loss: 1.2169 9.2624 sec/batch\n",
      "Epoch 6/15  Iteration 3332/8505 Training loss: 1.2169 9.3086 sec/batch\n",
      "Epoch 6/15  Iteration 3333/8505 Training loss: 1.2170 9.2645 sec/batch\n",
      "Epoch 6/15  Iteration 3334/8505 Training loss: 1.2168 9.3042 sec/batch\n",
      "Epoch 6/15  Iteration 3335/8505 Training loss: 1.2167 9.3900 sec/batch\n",
      "Epoch 6/15  Iteration 3336/8505 Training loss: 1.2166 9.4018 sec/batch\n",
      "Epoch 6/15  Iteration 3337/8505 Training loss: 1.2165 9.3574 sec/batch\n",
      "Epoch 6/15  Iteration 3338/8505 Training loss: 1.2164 9.3054 sec/batch\n",
      "Epoch 6/15  Iteration 3339/8505 Training loss: 1.2164 9.2906 sec/batch\n",
      "Epoch 6/15  Iteration 3340/8505 Training loss: 1.2163 9.2558 sec/batch\n",
      "Epoch 6/15  Iteration 3341/8505 Training loss: 1.2163 9.2514 sec/batch\n",
      "Epoch 6/15  Iteration 3342/8505 Training loss: 1.2163 9.3432 sec/batch\n",
      "Epoch 6/15  Iteration 3343/8505 Training loss: 1.2162 9.3628 sec/batch\n",
      "Epoch 6/15  Iteration 3344/8505 Training loss: 1.2161 9.2891 sec/batch\n",
      "Epoch 6/15  Iteration 3345/8505 Training loss: 1.2160 9.2723 sec/batch\n",
      "Epoch 6/15  Iteration 3346/8505 Training loss: 1.2159 9.2332 sec/batch\n",
      "Epoch 6/15  Iteration 3347/8505 Training loss: 1.2157 9.2930 sec/batch\n",
      "Epoch 6/15  Iteration 3348/8505 Training loss: 1.2156 9.3817 sec/batch\n",
      "Epoch 6/15  Iteration 3349/8505 Training loss: 1.2156 9.2785 sec/batch\n",
      "Epoch 6/15  Iteration 3350/8505 Training loss: 1.2155 9.2719 sec/batch\n",
      "Epoch 6/15  Iteration 3351/8505 Training loss: 1.2155 9.3478 sec/batch\n",
      "Epoch 6/15  Iteration 3352/8505 Training loss: 1.2154 9.3704 sec/batch\n",
      "Epoch 6/15  Iteration 3353/8505 Training loss: 1.2153 9.2970 sec/batch\n",
      "Epoch 6/15  Iteration 3354/8505 Training loss: 1.2152 9.2563 sec/batch\n",
      "Epoch 6/15  Iteration 3355/8505 Training loss: 1.2151 9.3647 sec/batch\n",
      "Epoch 6/15  Iteration 3356/8505 Training loss: 1.2150 9.4415 sec/batch\n",
      "Epoch 6/15  Iteration 3357/8505 Training loss: 1.2149 9.3516 sec/batch\n",
      "Epoch 6/15  Iteration 3358/8505 Training loss: 1.2149 9.2418 sec/batch\n",
      "Epoch 6/15  Iteration 3359/8505 Training loss: 1.2147 9.2936 sec/batch\n",
      "Epoch 6/15  Iteration 3360/8505 Training loss: 1.2147 9.4101 sec/batch\n",
      "Epoch 6/15  Iteration 3361/8505 Training loss: 1.2146 9.3348 sec/batch\n",
      "Epoch 6/15  Iteration 3362/8505 Training loss: 1.2144 13.9516 sec/batch\n",
      "Epoch 6/15  Iteration 3363/8505 Training loss: 1.2143 10.0000 sec/batch\n",
      "Epoch 6/15  Iteration 3364/8505 Training loss: 1.2142 11.3927 sec/batch\n",
      "Epoch 6/15  Iteration 3365/8505 Training loss: 1.2143 13.0911 sec/batch\n",
      "Epoch 6/15  Iteration 3366/8505 Training loss: 1.2142 14.0608 sec/batch\n",
      "Epoch 6/15  Iteration 3367/8505 Training loss: 1.2141 12.5149 sec/batch\n",
      "Epoch 6/15  Iteration 3368/8505 Training loss: 1.2141 14.1084 sec/batch\n",
      "Epoch 6/15  Iteration 3369/8505 Training loss: 1.2139 11.9463 sec/batch\n",
      "Epoch 6/15  Iteration 3370/8505 Training loss: 1.2139 14.8944 sec/batch\n",
      "Epoch 6/15  Iteration 3371/8505 Training loss: 1.2139 10.8817 sec/batch\n",
      "Epoch 6/15  Iteration 3372/8505 Training loss: 1.2138 12.1186 sec/batch\n",
      "Epoch 6/15  Iteration 3373/8505 Training loss: 1.2137 12.9326 sec/batch\n",
      "Epoch 6/15  Iteration 3374/8505 Training loss: 1.2136 10.7657 sec/batch\n",
      "Epoch 6/15  Iteration 3375/8505 Training loss: 1.2136 11.1191 sec/batch\n",
      "Epoch 6/15  Iteration 3376/8505 Training loss: 1.2135 14.3115 sec/batch\n",
      "Epoch 6/15  Iteration 3377/8505 Training loss: 1.2135 11.4224 sec/batch\n",
      "Epoch 6/15  Iteration 3378/8505 Training loss: 1.2134 11.0510 sec/batch\n",
      "Epoch 6/15  Iteration 3379/8505 Training loss: 1.2134 12.3580 sec/batch\n",
      "Epoch 6/15  Iteration 3380/8505 Training loss: 1.2134 11.6133 sec/batch\n",
      "Epoch 6/15  Iteration 3381/8505 Training loss: 1.2134 12.1935 sec/batch\n",
      "Epoch 6/15  Iteration 3382/8505 Training loss: 1.2133 14.1757 sec/batch\n",
      "Epoch 6/15  Iteration 3383/8505 Training loss: 1.2133 11.1086 sec/batch\n",
      "Epoch 6/15  Iteration 3384/8505 Training loss: 1.2132 12.7430 sec/batch\n",
      "Epoch 6/15  Iteration 3385/8505 Training loss: 1.2131 10.2300 sec/batch\n",
      "Epoch 6/15  Iteration 3386/8505 Training loss: 1.2132 11.4897 sec/batch\n",
      "Epoch 6/15  Iteration 3387/8505 Training loss: 1.2131 12.3238 sec/batch\n",
      "Epoch 6/15  Iteration 3388/8505 Training loss: 1.2131 15.4760 sec/batch\n",
      "Epoch 6/15  Iteration 3389/8505 Training loss: 1.2131 11.7533 sec/batch\n",
      "Epoch 6/15  Iteration 3390/8505 Training loss: 1.2131 10.7565 sec/batch\n",
      "Epoch 6/15  Iteration 3391/8505 Training loss: 1.2130 11.0667 sec/batch\n",
      "Epoch 6/15  Iteration 3392/8505 Training loss: 1.2130 11.7864 sec/batch\n",
      "Epoch 6/15  Iteration 3393/8505 Training loss: 1.2131 12.2108 sec/batch\n",
      "Epoch 6/15  Iteration 3394/8505 Training loss: 1.2131 10.7911 sec/batch\n",
      "Epoch 6/15  Iteration 3395/8505 Training loss: 1.2131 10.9945 sec/batch\n",
      "Epoch 6/15  Iteration 3396/8505 Training loss: 1.2131 14.7350 sec/batch\n",
      "Epoch 6/15  Iteration 3397/8505 Training loss: 1.2131 11.6903 sec/batch\n",
      "Epoch 6/15  Iteration 3398/8505 Training loss: 1.2130 11.4820 sec/batch\n",
      "Epoch 6/15  Iteration 3399/8505 Training loss: 1.2129 12.9155 sec/batch\n",
      "Epoch 6/15  Iteration 3400/8505 Training loss: 1.2128 13.1293 sec/batch\n",
      "Validation loss: 1.18422 Saving checkpoint!\n",
      "Epoch 6/15  Iteration 3401/8505 Training loss: 1.2130 15.6733 sec/batch\n",
      "Epoch 6/15  Iteration 3402/8505 Training loss: 1.2130 11.1660 sec/batch\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-7b8cd91db090>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m                     model.initial_state: new_state}\n\u001b[1;32m     34\u001b[0m             batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer], \n\u001b[0;32m---> 35\u001b[0;31m                                                  feed_dict=feed)\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 6\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "model = build_rnn(len(vocab), \n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  hidden_size=hidden_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_steps)\n",
    "    iterations = n_batches * epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "            iteration = e*n_batches + b\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], num_steps):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    batch_loss, new_state = sess.run([model.cost, model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"Charles_Dickens_i{}_l{}_v{:.3f}.ckpt\".format(iteration, hidden_size, np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saved checkpoints\n",
    "\n",
    "Read up on saving and loading checkpoints here: https://www.tensorflow.org/programmers_guide/variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.train.get_checkpoint_state('checkpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, hidden_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, hidden_size=hidden_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, pass in the path to a checkpoint and sample from the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 200 iterations, loss = 2.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "she looked whith, ang heve beat, I d of the sared an toreding thongent hate fre sor the sald in tom the the he whar sat anger af thing as ar sore toot ing the willly, wirly were fort oferither. \n",
      "I he shen trichers in the wate tore ainthe sorser, whith he wass to then ate an ore toon th thathen, I dound hes iokent too saresting ale fromener to hat hist he the sing tare, singterten toon ane hin wisthe ter ale so toresten ar and ale whas\n",
      "I seatlice fored. \n",
      "I me hire wered the sh me th ar sares ing hat on thand\n",
      "ryas me fecestre war sof his shing, the thet oned on an wathe ther hes in sier andis if the carlich oure and astor, the with hers we ler he shateren then withing hers tore fouthin sher, will and werert iled the frit ate at an intt ous, at her and wher wast onte to my or hire fre sher and an ther hatrong one the were whit werthe wen as the ce poreed th of mestit on hin sint is thar sille wer on waren to tary than wor himengof oo mand buthand that ald hat heand, wasere woun ang the has ing ous in\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"/Users/Dave/Desktop/Programming/Personal Projects/text_generation_great_expectations/great_expectations_i200_l800_v2.220.ckpt\"\n",
    "samp = sample(checkpoint, 1000, hidden_size, len(vocab), prime=\"she looked \")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1600 iterations, less = 1.32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "she looked in\n",
      "the matter-shops arreaded.  He had a good distanted to the woman state of the\n",
      "stutt.  It was sure the passionary tememble shout in the coach-strought\n",
      "appraceing, by the tarder, and a stote coult of him that there, and\n",
      "who was to me. I was that the world same thought at little best of his.\n",
      "\n",
      "'It is not a something?  What sit yer an imprisinescess is an indicture\n",
      "to the seconsire weeps, it's no mother's course, if it is that? Mida's\n",
      "have been could hope out her, she have say no one so the will.\n",
      "\n",
      "The clothes he was along, and had a street travel and collections, somibling,\n",
      "though there was an early day, with myself and streegs to and state of\n",
      "that same commor at tabses in his seat instantly remigning.\n",
      "\n",
      "As ho seemed that she wrutten undershates that she was so face and\n",
      "strate and searable country was, to be an oppression, and then ansole\n",
      "out of the same the close with her stopy tarned which was alwey to tender,\n",
      "the subject the service who had taken his house with him as a short way\n",
      "to her\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"/Users/Dave/Desktop/Programming/Personal Projects/text_generation_great_expectations/great_expectations_i1600_l800_v1.320.ckpt\"\n",
    "samp = sample(checkpoint, 1000, hidden_size, len(vocab), prime=\"she looked \")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3400 iterations, loss = 1.18 (final iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "she looked intimently\n",
      "that the convict on the chair was not out of her face. If he were the\n",
      "part of the street away together, importance of the short shining.\n",
      "He went out to this conversation and staring the true, the man who\n",
      "have been down in his heart.\n",
      "\n",
      "I hide it then. There was nothing or spare howselfoush, and all the\n",
      "children were the thoughts to him that having helted it all that hand\n",
      "of his passagies, and the presence of perish, and his heart to him,\n",
      "they were all thinking, and seemed to take away them with her.\n",
      "\n",
      "'You dare say anything,' said the gate, who was too laten with the\n",
      "place, 'when I do.'\n",
      "\n",
      "'This is my legative,' said the doctor, 'and what he is a power to him;\n",
      "but I was in her own searth--that would yrew the presence of his own\n",
      "frish thing, but it's a gradual flight of him tired. I saw he would\n",
      "have her father hand to be here from being ill thinking that I was to have\n",
      "a history. I have been to say that he might have been. The peor life,\n",
      "an old come on. All that when I saw him seem\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"/Users/Dave/Desktop/Programming/Personal Projects/text_generation_great_expectations/great_expectations_i3400_l800_v1.184.ckpt\"\n",
    "samp = sample(checkpoint, 1000, hidden_size, len(vocab), prime=\"she looked \")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Given the length of time that it takes to train this network on my laptop, ~9 hours, more experimenting could be done to improve results, but I would rather move on to other projects. I have some thoughts about how to improve results:\n",
    "- Increase the num_steps so that each character gains a greater understanding of its relation to the other characters.\n",
    "- Increase the hidden_size. This should help the network to learn better (but it may also fill up your RAM as it did to me).\n",
    "- Use a custom made cell to replace the GRU cell that I used. An example can be found [here](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html).\n",
    "\n",
    "Although I did not make significant changes to the code, my choice of input and parameters helped to improve the results. By using more text, overfitting could be reduced and the size of the testing data could be increased. Increasing the num_steps, hidden_size, and dropout, as well as decreasing the learning rate all helped to improve the performance of the model.\n",
    "\n",
    "Looking at the output, we can really see how much the model has learned. Spelling mistakes are minimal, punctuation is very good, as well as the structure of a paragraph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
